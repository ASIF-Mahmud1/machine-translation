{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NMT:EnglishToHindi(Working).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+BiQs2uIKwZnbfTLwZq3g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### TASK: Create Encder-Decoder LSTM model to convert English sentences to Hindi sentences.  \n","We are going to use word level embedding"],"metadata":{"id":"YkLw2eBCN5Tu"}},{"cell_type":"code","source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'"],"metadata":{"id":"Qxm6DVwmngRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing Data"],"metadata":{"id":"SDmvvJONOW-b"}},{"cell_type":"markdown","source":["### 1 .  Import Libraries"],"metadata":{"id":"bDhGmxabOeih"}},{"cell_type":"code","metadata":{"id":"qK2TWV1nm48Q","executionInfo":{"status":"ok","timestamp":1658247088723,"user_tz":-360,"elapsed":2839,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"source":["\n","#%tensorflow_version 2.x\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","import pandas as pd\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### 2 . Read Data\n"],"metadata":{"id":"kq8_kYcaOwYa"}},{"cell_type":"code","metadata":{"id":"27OzmS-MIymc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658247089449,"user_tz":-360,"elapsed":736,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"c8592093-5558-4411-b83b-0af9b4eed0c5"},"source":["!wget http://www.manythings.org/anki/hin-eng.zip -O hin-eng.zip\n","\n","## IIT dataset : https://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.zip\n","\n","!unzip hin-eng.zip\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-19 16:11:28--  http://www.manythings.org/anki/hin-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 132114 (129K) [application/zip]\n","Saving to: ‘hin-eng.zip’\n","\n","hin-eng.zip         100%[===================>] 129.02K  --.-KB/s    in 0.1s    \n","\n","2022-07-19 16:11:28 (1.25 MB/s) - ‘hin-eng.zip’ saved [132114/132114]\n","\n","Archive:  hin-eng.zip\n","  inflating: hin.txt                 \n","  inflating: _about.txt              \n"]}]},{"cell_type":"code","source":["lines = pd.read_table( 'hin.txt' , names=[ 'eng' , 'hindi' ] )\n","lines.reset_index( level=0 , inplace=True )\n","lines.rename( columns={ 'index' : 'eng' , 'eng' : 'hindi' , 'hindi' : 'c' } , inplace=True )\n","lines = lines.drop( 'c' , 1 )\n","\n","lines.head()\n"],"metadata":{"id":"pKJUzeRvPYm2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dgIdfjIRLDN"},"source":["### 3) Preparing input data for the Encoder ( `encoder_input_data` )\n","The Encoder model will be fed input data which are **preprocessed English sentences**. Following preprocessing is done:\n","\n","\n","1.   Tokenizing the English sentences from `eng_lines`.\n","2.   Determining the maximum length of the English sentence that's `max_input_length`.\n","3.   Padding the `tokenized_eng_lines` to the max_input_length.\n","4.   Determining the vocabulary size ( `num_eng_tokens` ) for English words.\n","\n","\n","\n"]},{"cell_type":"code","source":["eng_lines = list()\n","for line in lines.eng:\n","    eng_lines.append( line ) \n","\n","tokenizer = preprocessing.text.Tokenizer(oov_token=1)\n","tokenizer.fit_on_texts( eng_lines ) \n","tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) \n","\n","length_list = list()\n","for token_seq in tokenized_eng_lines:\n","    length_list.append( len( token_seq ))\n","max_input_length = np.array( length_list ).max()\n","print( 'English max length is {}'.format( max_input_length ))\n","\n","padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\n","encoder_input_data = np.array( padded_eng_lines )\n","print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n","\n","eng_word_dict = tokenizer.word_index\n","num_eng_tokens = len( eng_word_dict )+1\n","print( 'Number of English tokens = {}'.format( num_eng_tokens))\n","\n","# print(\"Dictionary Eng word to tokens\", eng_word_dict)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\"\"\"\n","Oov tokens are out of vocabulary tokens used to replace unknown words.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"UZqA3VG9PdLn","executionInfo":{"status":"ok","timestamp":1658044312602,"user_tz":-360,"elapsed":17,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"5258746f-248c-4307-d767-c62aed7086bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English max length is 22\n","Encoder input data shape -> (2915, 22)\n","Number of English tokens = 2402\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nOov tokens are out of vocabulary tokens used to replace unknown words.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"cRwAd310SPkG"},"source":["### 4) Preparing input data for the Decoder ( `decoder_input_data` )\n","The Decoder model will be fed the preprocessed Hindi lines. Preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\n","\n","\n","*   Append `<START>` tag at the first position in  each Hindi sentence.\n","*   Append `<END>` tag at the last position in  each Hindi sentence.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"deB0oX_0pj8R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658044312603,"user_tz":-360,"elapsed":14,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"9c8fbe63-7689-4f4e-88f5-76b95c124314"},"source":["\n","hindi_lines = list()\n","for line in lines.hindi:\n","    hindi_lines.append( '<START> ' + line + ' <END>' )  \n","\n","tokenizer = preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts( hindi_lines ) \n","tokenized_hindi_lines = tokenizer.texts_to_sequences( hindi_lines ) \n","\n","length_list = list()\n","for token_seq in tokenized_hindi_lines:\n","    length_list.append( len( token_seq ))\n","max_output_length = np.array( length_list ).max()\n","print( 'Hindi max length is {}'.format( max_output_length ))\n","\n","padded_hindi_lines = preprocessing.sequence.pad_sequences( tokenized_hindi_lines , maxlen=max_output_length, padding='post' )\n","decoder_input_data = np.array( padded_hindi_lines  )\n","print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n","\n","hindi_word_dict = tokenizer.word_index\n","num_hindi_tokens = len( hindi_word_dict )+1\n","print( 'Number of Hindi tokens = {}'.format( num_hindi_tokens))\n","\n","# print(\"Dictionary Hindi word to tokens\", hindi_word_dict)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hindi max length is 27\n","Decoder input data shape -> (2915, 27)\n","Number of Hindi tokens = 3026\n"]}]},{"cell_type":"markdown","metadata":{"id":"DJTcSlygTQ_V"},"source":["### 5) Preparing target data for the Decoder ( decoder_target_data ) \n","\n","We take a copy of `tokenized_hindi_lines` and modify it like this.\n","\n","\n","\n","1.  Remove the `<start>` tag which we appended earlier. Hence, the word ( which is `<start>` in this case  ) will be removed.\n","2.   Convert the `padded_hindi_lines` ( ones which do not have `<start>` tag ) to one-hot vectors.\n","\n","For example :\n","\n","```\n"," [ '<start>' , 'hello' , 'world' , '<end>' ]\n","\n","```\n","\n","wil become \n","\n","```\n"," [ 'hello' , 'world' , '<end>' ]\n","\n","```\n"]},{"cell_type":"code","metadata":{"id":"NPCTmeL7qj3T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658044313029,"user_tz":-360,"elapsed":437,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"4f491a56-5eb9-4f60-d154-ede9f78e70f0"},"source":["\n","decoder_target_data = list()\n","for token_seq in tokenized_hindi_lines:\n","    decoder_target_data.append( token_seq[ 1 : ] ) \n","    \n","padded_hindi_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n","onehot_hindi_lines = utils.to_categorical( padded_hindi_lines , num_hindi_tokens )\n","decoder_target_data = np.array( onehot_hindi_lines )\n","print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder target data shape -> (2915, 27, 3026)\n"]}]},{"cell_type":"markdown","metadata":{"id":"M_N71uykUPbe"},"source":["### 1) Defining the Encoder-Decoder model\n","The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n","\n","\n","*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n","*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n","*   LSTM layer : Provide access to Long-Short Term cells.\n","\n","Working : \n","\n","1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n","2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n","3.   These states are set in the LSTM cell of the decoder.\n","4.   The decoder_input_data comes in through the Embedding layer.\n","5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM"],"metadata":{"id":"2ZG3ceruwFXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_inputs = Input(shape=( max_input_length ,  ))\n","encoder_embedding = Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\n","encoder_outputs , state_h , state_c = LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n","encoder_states = [ state_h , state_c ]\n","\n","decoder_inputs = Input(shape=( max_output_length , ))\n","decoder_embedding = Embedding( num_hindi_tokens, 256 , mask_zero=True) (decoder_inputs)\n","decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n","decoder_dense = Dense( num_hindi_tokens , activation=tf.keras.activations.softmax ) \n","output = decoder_dense ( decoder_outputs )\n","\n","model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n","model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n","\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vLS0nc0waKJ","executionInfo":{"status":"ok","timestamp":1658044313848,"user_tz":-360,"elapsed":823,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"f15f9cf4-626a-4dee-e960-8da1ec9d328e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_13\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_15 (InputLayer)          [(None, 22)]         0           []                               \n","                                                                                                  \n"," input_16 (InputLayer)          [(None, 27)]         0           []                               \n","                                                                                                  \n"," embedding_2 (Embedding)        (None, 22, 256)      614912      ['input_15[0][0]']               \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 27, 256)      774656      ['input_16[0][0]']               \n","                                                                                                  \n"," lstm_2 (LSTM)                  [(None, 256),        525312      ['embedding_2[0][0]']            \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_3 (LSTM)                  [(None, 27, 256),    525312      ['embedding_3[0][0]',            \n","                                 (None, 256),                     'lstm_2[0][1]',                 \n","                                 (None, 256)]                     'lstm_2[0][2]']                 \n","                                                                                                  \n"," dense_1 (Dense)                (None, 27, 3026)     777682      ['lstm_3[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,217,874\n","Trainable params: 3,217,874\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"n9g_8sR7WWf3"},"source":["### 2) Training the model\n","We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."]},{"cell_type":"code","source":["mc = tf.keras.callbacks.ModelCheckpoint('my_nmt_model_min_loss.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"],"metadata":{"id":"8HDkBQXA2xdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnd2H27qt4Hy","cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658059497201,"user_tz":-360,"elapsed":2422748,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"012bd25d-bd93-45a1-914c-abd7494260fb"},"source":["\n","history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=100 ,\n","                     validation_split = 0.1,\n","                   callbacks=[mc], verbose=1    ) \n","model.save( 'model.h5' ) \n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2666\n","Epoch 1: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2666 - val_loss: 0.8641\n","Epoch 2/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2603\n","Epoch 2: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2603 - val_loss: 0.8718\n","Epoch 3/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2546\n","Epoch 3: val_loss did not improve from 0.71402\n","11/11 [==============================] - 25s 2s/step - loss: 0.2546 - val_loss: 0.8811\n","Epoch 4/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2490\n","Epoch 4: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2490 - val_loss: 0.8910\n","Epoch 5/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2428\n","Epoch 5: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2428 - val_loss: 0.9001\n","Epoch 6/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2381\n","Epoch 6: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2381 - val_loss: 0.9057\n","Epoch 7/100\n"," 1/11 [=>............................] - ETA: 21s - loss: 0.2348\n","Epoch 7: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2328 - val_loss: 0.9145\n","Epoch 8/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2278\n","Epoch 8: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2278 - val_loss: 0.9286\n","Epoch 9/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2232\n","Epoch 9: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2232 - val_loss: 0.9362\n","Epoch 10/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2178\n","Epoch 10: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2178 - val_loss: 0.9450\n","Epoch 11/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2134\n","Epoch 11: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2134 - val_loss: 0.9507\n","Epoch 12/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2082\n","Epoch 12: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2082 - val_loss: 0.9591\n","Epoch 13/100\n","11/11 [==============================] - ETA: 0s - loss: 0.2034\n","Epoch 13: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.2034 - val_loss: 0.9685\n","Epoch 14/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1990\n","Epoch 14: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1990 - val_loss: 0.9769\n","Epoch 15/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1943\n","Epoch 15: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1943 - val_loss: 0.9823\n","Epoch 16/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1896\n","Epoch 16: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1896 - val_loss: 0.9939\n","Epoch 17/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1855\n","Epoch 17: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1855 - val_loss: 1.0004\n","Epoch 18/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1811\n","Epoch 18: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1811 - val_loss: 1.0084\n","Epoch 19/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1776\n","Epoch 19: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1776 - val_loss: 1.0110\n","Epoch 20/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1740\n","Epoch 20: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1740 - val_loss: 1.0201\n","Epoch 21/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1704\n","Epoch 21: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1704 - val_loss: 1.0290\n","Epoch 22/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1660\n","Epoch 22: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1660 - val_loss: 1.0357\n","Epoch 23/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1631\n","Epoch 23: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1631 - val_loss: 1.0390\n","Epoch 24/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1593\n","Epoch 24: val_loss did not improve from 0.71402\n","11/11 [==============================] - 26s 2s/step - loss: 0.1593 - val_loss: 1.0457\n","Epoch 25/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1562\n","Epoch 25: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1562 - val_loss: 1.0549\n","Epoch 26/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1526\n","Epoch 26: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1526 - val_loss: 1.0631\n","Epoch 27/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1494\n","Epoch 27: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1494 - val_loss: 1.0673\n","Epoch 28/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1459\n","Epoch 28: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1459 - val_loss: 1.0791\n","Epoch 29/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1427\n","Epoch 29: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1427 - val_loss: 1.0811\n","Epoch 30/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1389\n","Epoch 30: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1389 - val_loss: 1.0861\n","Epoch 31/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1367\n","Epoch 31: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1367 - val_loss: 1.0936\n","Epoch 32/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1340\n","Epoch 32: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1340 - val_loss: 1.1004\n","Epoch 33/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1310\n","Epoch 33: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1310 - val_loss: 1.1066\n","Epoch 34/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1283\n","Epoch 34: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1283 - val_loss: 1.1100\n","Epoch 35/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1252\n","Epoch 35: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1252 - val_loss: 1.1147\n","Epoch 36/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1220\n","Epoch 36: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1220 - val_loss: 1.1198\n","Epoch 37/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1196\n","Epoch 37: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1196 - val_loss: 1.1277\n","Epoch 38/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1168\n","Epoch 38: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1168 - val_loss: 1.1338\n","Epoch 39/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1153\n","Epoch 39: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1153 - val_loss: 1.1412\n","Epoch 40/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1125\n","Epoch 40: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1125 - val_loss: 1.1466\n","Epoch 41/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1098\n","Epoch 41: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1098 - val_loss: 1.1496\n","Epoch 42/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1083\n","Epoch 42: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1083 - val_loss: 1.1563\n","Epoch 43/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1057\n","Epoch 43: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1057 - val_loss: 1.1626\n","Epoch 44/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1031\n","Epoch 44: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.1031 - val_loss: 1.1669\n","Epoch 45/100\n","11/11 [==============================] - ETA: 0s - loss: 0.1019\n","Epoch 45: val_loss did not improve from 0.71402\n","11/11 [==============================] - 23s 2s/step - loss: 0.1019 - val_loss: 1.1736\n","Epoch 46/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0985\n","Epoch 46: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0985 - val_loss: 1.1794\n","Epoch 47/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0972\n","Epoch 47: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0972 - val_loss: 1.1840\n","Epoch 48/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0945\n","Epoch 48: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0945 - val_loss: 1.1923\n","Epoch 49/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0930\n","Epoch 49: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0930 - val_loss: 1.1948\n","Epoch 50/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0914\n","Epoch 50: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0914 - val_loss: 1.1956\n","Epoch 51/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0895\n","Epoch 51: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0895 - val_loss: 1.2035\n","Epoch 52/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0882\n","Epoch 52: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0882 - val_loss: 1.2082\n","Epoch 53/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 53: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0863 - val_loss: 1.2139\n","Epoch 54/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0848\n","Epoch 54: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0848 - val_loss: 1.2174\n","Epoch 55/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0828\n","Epoch 55: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0828 - val_loss: 1.2215\n","Epoch 56/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 56: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0810 - val_loss: 1.2267\n","Epoch 57/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0796\n","Epoch 57: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0796 - val_loss: 1.2334\n","Epoch 58/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0776\n","Epoch 58: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0776 - val_loss: 1.2313\n","Epoch 59/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0760\n","Epoch 59: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0760 - val_loss: 1.2421\n","Epoch 60/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 60: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0744 - val_loss: 1.2442\n","Epoch 61/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0731\n","Epoch 61: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0731 - val_loss: 1.2517\n","Epoch 62/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0718\n","Epoch 62: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0718 - val_loss: 1.2549\n","Epoch 63/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0707\n","Epoch 63: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0707 - val_loss: 1.2609\n","Epoch 64/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0688\n","Epoch 64: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0688 - val_loss: 1.2654\n","Epoch 65/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0679\n","Epoch 65: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0679 - val_loss: 1.2717\n","Epoch 66/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0662\n","Epoch 66: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0662 - val_loss: 1.2747\n","Epoch 67/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0657\n","Epoch 67: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0657 - val_loss: 1.2772\n","Epoch 68/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0637\n","Epoch 68: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0637 - val_loss: 1.2887\n","Epoch 69/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0636\n","Epoch 69: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0636 - val_loss: 1.2895\n","Epoch 70/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0617\n","Epoch 70: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0617 - val_loss: 1.2856\n","Epoch 71/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0609\n","Epoch 71: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0609 - val_loss: 1.2944\n","Epoch 72/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0598\n","Epoch 72: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0598 - val_loss: 1.2961\n","Epoch 73/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0587\n","Epoch 73: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0587 - val_loss: 1.3021\n","Epoch 74/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0578\n","Epoch 74: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0578 - val_loss: 1.3065\n","Epoch 75/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0567\n","Epoch 75: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0567 - val_loss: 1.3141\n","Epoch 76/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0552\n","Epoch 76: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0552 - val_loss: 1.3145\n","Epoch 77/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0546\n","Epoch 77: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0546 - val_loss: 1.3139\n","Epoch 78/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0539\n","Epoch 78: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0539 - val_loss: 1.3214\n","Epoch 79/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0525\n","Epoch 79: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0525 - val_loss: 1.3274\n","Epoch 80/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 80: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0519 - val_loss: 1.3329\n","Epoch 81/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 81: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0504 - val_loss: 1.3340\n","Epoch 82/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 82: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0500 - val_loss: 1.3414\n","Epoch 83/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 83: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0494 - val_loss: 1.3448\n","Epoch 84/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 84: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0487 - val_loss: 1.3500\n","Epoch 85/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 85: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0477 - val_loss: 1.3549\n","Epoch 86/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 86: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0467 - val_loss: 1.3527\n","Epoch 87/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 87: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0462 - val_loss: 1.3603\n","Epoch 88/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 88: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0454 - val_loss: 1.3623\n","Epoch 89/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 89: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0447 - val_loss: 1.3640\n","Epoch 90/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 90: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0440 - val_loss: 1.3690\n","Epoch 91/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 91: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0432 - val_loss: 1.3717\n","Epoch 92/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 92: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0427 - val_loss: 1.3802\n","Epoch 93/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 93: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0423 - val_loss: 1.3783\n","Epoch 94/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 94: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0415 - val_loss: 1.3834\n","Epoch 95/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 95: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0409 - val_loss: 1.3903\n","Epoch 96/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 96: val_loss did not improve from 0.71402\n","11/11 [==============================] - 23s 2s/step - loss: 0.0402 - val_loss: 1.3920\n","Epoch 97/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 97: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0400 - val_loss: 1.3942\n","Epoch 98/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 98: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0389 - val_loss: 1.3983\n","Epoch 99/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 99: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0390 - val_loss: 1.4055\n","Epoch 100/100\n","11/11 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 100: val_loss did not improve from 0.71402\n","11/11 [==============================] - 24s 2s/step - loss: 0.0380 - val_loss: 1.4089\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.legend(['train','validation'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"4L589fF_3eln","executionInfo":{"status":"ok","timestamp":1658061684772,"user_tz":-360,"elapsed":511,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"5dbaf87e-87f1-400e-ff26-e3a17ff8762c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7fdc34f5d9d0>"]},"metadata":{},"execution_count":82},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Eeqv_vH5pMpb"},"source":["## Inferencing on the models"]},{"cell_type":"markdown","metadata":{"id":"o4PAtzGrk8pq"},"source":["### 1) Defining inference models\n","We create inference models which help in predicting translations.\n","\n","**Encoder inference model** : Takes the English sentence as input and outputs LSTM states ( `h` and `c` ).\n","\n","**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the French input seqeunces ( ones not having the `<start>` tag ). It will output the translations of the English sentence which we fed to the encoder model and its state values.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"UNhVkiZLvdTq"},"source":["\n","def make_inference_models():\n","    \n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","    \n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","    \n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    \n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","    \n","    return encoder_model , decoder_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djEPrfJBmZE-"},"source":["### 2) Making some translations\n","\n","\n","1.   First, we take a English sequence and predict the state values using `enc_model`.\n","2.   We set the state values in the decoder's LSTM.\n","3.   Then, we generate a sequence which contains the `<start>` element.\n","4.   We input this sequence in the `dec_model`.\n","5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n","6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum sequence length.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Y_hrJcNP-mXb"},"source":["\n","def str_to_tokens( sentence : str ):\n","    words = sentence.lower().split()\n","    tokens_list = list()\n","    for word in words:\n","          print(\"word \", word,eng_word_dict.get(word,1) )\n","          my_word=  eng_word_dict.get(word,1)\n","          tokens_list.append(my_word) \n","  \n","    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Mfco9WKukhS","cellView":"both","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658051757400,"user_tz":-360,"elapsed":1721,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"bc4690bb-6a21-45ae-91f0-11bde90d0fd7"},"source":["\n","enc_model , dec_model = make_inference_models()\n","\n","#encoder_input_data.shape[0] \n","for epoch in range(1 ):\n","    states_values = enc_model.predict( str_to_tokens(\"I love food\" ) )\n","    empty_target_seq = np.zeros( ( 1 , 1 ) )\n","    empty_target_seq[0, 0] = hindi_word_dict['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","    while not stop_condition :\n","        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","        sampled_word = None\n","        for word , index in hindi_word_dict.items() :\n","            if sampled_word_index == index :\n","                decoded_translation += ' {}'.format( word )\n","                sampled_word = word\n","        \n","        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","            stop_condition = True\n","            \n","        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","        empty_target_seq[ 0 , 0 ] = sampled_word_index\n","        states_values = [ h , c ] \n","\n","    # print(\"Decoded Traslation \", decoded_translation )\n","    print(f\"{bcolors.OKGREEN}Decoded Traslation: { decoded_translation}{bcolors.ENDC}\")\n","\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["word  i 3\n","word  love 233\n","word  food 340\n","WARNING:tensorflow:Model was constructed with shape (None, 27) for input KerasTensor(type_spec=TensorSpec(shape=(None, 27), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\"), but it was called on an input with incompatible shape (None, 1).\n","\u001b[92mDecoded Traslation:  मैं खुश हूँ। end\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### Save Encoder and Decoder Parameters"],"metadata":{"id":"P1Wf3vttALhP"}},{"cell_type":"code","source":["model_path= '/content/drive/MyDrive/Machine Learning/NMT_Word_Level/My Version/model/'\n","# save encoder model\n","enc_model.save( model_path+'enc_model.h5' ) \n","# save decoder model\n","dec_model.save( model_path+'dec_model.h5' ) \n","# save  model\n","model.save( model_path+'model.h5' ) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1KvVW7Wlb0-","executionInfo":{"status":"ok","timestamp":1658051763151,"user_tz":-360,"elapsed":1467,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"54e4aadb-fa49-4ae5-e039-64fed1cfd35e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":["# encoder parameters\n","encoder_parameters={\n","    'max_encoder_seq_length': max_input_length,\n","    'num_encoder_tokens': num_eng_tokens,\n","   \n","}\n","encoder_dictionary= eng_word_dict\n","\n","# decoder parameters\n","decoder_parameters={\n","    'max_decoder_seq_length':  max_output_length,\n","    'num_decoder_tokens': num_hindi_tokens,\n","\n","}\n","\n","decoder_dictionary=  hindi_word_dict\n","\n"],"metadata":{"id":"ZOph9zDmoi0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","paramters_path= model_path= '/content/drive/MyDrive/Machine Learning/NMT_Word_Level/My Version/parameters/'\n","dictionaries_path= model_path= '/content/drive/MyDrive/Machine Learning/NMT_Word_Level/My Version/dictionaries/'\n","\n","# save encoder parameter\n","with open(paramters_path+'encoder_parameters.pickle', 'wb') as handle:\n","    pickle.dump(encoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder dictionary\n","with open(dictionaries_path+'encoder_dictionary.pickle', 'wb') as handle:\n","    pickle.dump(encoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder parameter\n","with open(paramters_path+'decoder_parameters.pickle', 'wb') as handle:\n","    pickle.dump(decoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder parameter\n","with open(dictionaries_path+'decoder_dictionary.pickle', 'wb') as handle:\n","    pickle.dump(decoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n","\n"],"metadata":{"id":"tCRJRA979x7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NK6GXJNOqFRf","executionInfo":{"status":"ok","timestamp":1658046902291,"user_tz":-360,"elapsed":5955,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"adf7a14d-7f46-409e-b6af-e78ab15a27c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}