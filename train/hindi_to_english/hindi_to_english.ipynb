{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNV/wvX6/kC/8SYG1rMIkW1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"f13a492e1b6947379b6181d5a55af0a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20695a5381c44308a7410217234a30d3","IPY_MODEL_7ed75f0a75404b86a6a1413f422260e2","IPY_MODEL_6f7e354432eb4d29839afa55236ee04c"],"layout":"IPY_MODEL_0e78fe6fd3db4658bf47d66860612fdc"}},"20695a5381c44308a7410217234a30d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2964179cf68436cae1f8aacf63085c4","placeholder":"​","style":"IPY_MODEL_b0dfe2d41f0d463a8d99c4600658bd55","value":"100%"}},"7ed75f0a75404b86a6a1413f422260e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72376cade4f54c6f9457f619c86ca30e","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9aca7cf3a614b0d8464c0d7bc0368a8","value":3}},"6f7e354432eb4d29839afa55236ee04c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eddb38bbb93a49908d4cff3e83e9942f","placeholder":"​","style":"IPY_MODEL_b1d35b728fda41429d185270f947b914","value":" 3/3 [00:01&lt;00:00,  1.56it/s]"}},"0e78fe6fd3db4658bf47d66860612fdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2964179cf68436cae1f8aacf63085c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0dfe2d41f0d463a8d99c4600658bd55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72376cade4f54c6f9457f619c86ca30e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9aca7cf3a614b0d8464c0d7bc0368a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eddb38bbb93a49908d4cff3e83e9942f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d35b728fda41429d185270f947b914":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["### TASK: Create Encder-Decoder LSTM model to convert Hindi sentences to English sentences.  \n","We are going to use word level embedding"],"metadata":{"id":"YkLw2eBCN5Tu"}},{"cell_type":"code","source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'"],"metadata":{"id":"Qxm6DVwmngRe","executionInfo":{"status":"ok","timestamp":1668932462706,"user_tz":-360,"elapsed":594,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import tracemalloc\n"],"metadata":{"id":"NQnsUu26FJHn","executionInfo":{"status":"ok","timestamp":1668932464110,"user_tz":-360,"elapsed":5,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"n_lobornFab1","executionInfo":{"status":"ok","timestamp":1668932466081,"user_tz":-360,"elapsed":579,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# tracemalloc.start()\n","\n","# print(tracemalloc.get_traced_memory())\n","\n","# tracemalloc.stop()\n"],"metadata":{"id":"pLMfIjM0FW0Z","executionInfo":{"status":"ok","timestamp":1668932466082,"user_tz":-360,"elapsed":6,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#Mount Drive"],"metadata":{"id":"PcMV--RFZfpC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFVFlI7WZigZ","executionInfo":{"status":"ok","timestamp":1668932470477,"user_tz":-360,"elapsed":2720,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"a650fd9a-796c-4c08-a5de-722905a9e68a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Set Data Size"],"metadata":{"id":"yQdgcBBSjFFN"}},{"cell_type":"code","source":["data_size=15000"],"metadata":{"id":"t1pR6c1FjQ-9","executionInfo":{"status":"ok","timestamp":1668932470477,"user_tz":-360,"elapsed":8,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Preparing Data"],"metadata":{"id":"SDmvvJONOW-b"}},{"cell_type":"markdown","source":["### 1 .  Import Libraries"],"metadata":{"id":"bDhGmxabOeih"}},{"cell_type":"code","metadata":{"id":"qK2TWV1nm48Q","executionInfo":{"status":"ok","timestamp":1668932471898,"user_tz":-360,"elapsed":4,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"source":["\n","#%tensorflow_version 2.x\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","import pandas as pd\n","import gc\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 2 . Read Data : Many Things\n"],"metadata":{"id":"kq8_kYcaOwYa"}},{"cell_type":"code","metadata":{"id":"27OzmS-MIymc","executionInfo":{"status":"ok","timestamp":1668932474408,"user_tz":-360,"elapsed":479,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"source":["# !wget http://www.manythings.org/anki/hin-eng.zip -O hin-eng.zip\n","\n","# ## IIT dataset : https://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.zip\n","\n","# !unzip hin-eng.zip\n","\n","# lines = pd.read_table( 'hin.txt' , names=[ 'eng' , 'hindi' ] )\n","# lines.reset_index( level=0 , inplace=True )\n","# lines.rename( columns={ 'index' : 'eng' , 'eng' : 'hindi' , 'hindi' : 'c' } , inplace=True )\n","# lines = lines.drop( 'c' , 1 )\n","# lines= lines[:data_size]\n","\n","# lines.tail()\n","\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#2. Read Data : IIT Bombay "],"metadata":{"id":"ZYicvRqwYpl2"}},{"cell_type":"code","source":["!pip install datasets==1.18.1\n","from datasets import load_dataset\n","dataset = load_dataset(\"cfilt/iitb-english-hindi\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":726,"referenced_widgets":["f13a492e1b6947379b6181d5a55af0a5","20695a5381c44308a7410217234a30d3","7ed75f0a75404b86a6a1413f422260e2","6f7e354432eb4d29839afa55236ee04c","0e78fe6fd3db4658bf47d66860612fdc","a2964179cf68436cae1f8aacf63085c4","b0dfe2d41f0d463a8d99c4600658bd55","72376cade4f54c6f9457f619c86ca30e","c9aca7cf3a614b0d8464c0d7bc0368a8","eddb38bbb93a49908d4cff3e83e9942f","b1d35b728fda41429d185270f947b914"]},"id":"LJuiMoqljZmE","executionInfo":{"status":"ok","timestamp":1668932484402,"user_tz":-360,"elapsed":7205,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"7cf06459-4c40-40b1-9b24-121d1cd91361"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets==1.18.1 in /usr/local/lib/python3.7/dist-packages (1.18.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (4.64.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (2022.10.0)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (6.0.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (1.3.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (0.70.14)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (21.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (3.1.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (0.3.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (4.13.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (0.11.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets==1.18.1) (3.8.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (22.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (4.1.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (2.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.18.1) (1.3.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.1) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.1) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.18.1) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.18.1) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.18.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.18.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.18.1) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.18.1) (3.10.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.18.1) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.18.1) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.18.1) (1.15.0)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration cfilt--iitb-english-hindi-e9387d78518bc7f8\n","WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/cfilt--iitb-english-hindi-e9387d78518bc7f8/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f13a492e1b6947379b6181d5a55af0a5"}},"metadata":{}}]},{"cell_type":"code","source":["tracemalloc.start()\n","\n","\n","pairs=[] \n","for translation_pair in dataset[\"train\"][\"translation\"]:\n","  source_sentence = translation_pair[\"hi\"]\n","  target_sentence = translation_pair[\"en\"]\n","  pairs.append([source_sentence, target_sentence])\n","\n","\n","lines= pd.DataFrame(columns=[ \"hindi\",\"eng\"], data=pairs)\n","lines= lines[:data_size]\n","lines.tail()\n"," \n","print(tracemalloc.get_traced_memory())\n","\n"],"metadata":{"id":"pKJUzeRvPYm2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668932528101,"user_tz":-360,"elapsed":43708,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"1a75c98e-a741-4be1-c697-4fc6c0e2795e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(766916029, 1334826751)\n"]}]},{"cell_type":"markdown","source":["## Empty Garbage"],"metadata":{"id":"rAu9jCQ7h8Gy"}},{"cell_type":"code","source":["del dataset\n","del pairs\n","gc.collect()\n","print(tracemalloc.get_traced_memory())\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zn07HwX2iBj_","executionInfo":{"status":"ok","timestamp":1668932529295,"user_tz":-360,"elapsed":1201,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"d2e2464c-e15a-47cb-c939-34c822eedf30"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(766928225, 1334826751)\n"]}]},{"cell_type":"markdown","source":["# Get Data Size"],"metadata":{"id":"cqwV-0prZKwN"}},{"cell_type":"code","source":["lines.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_MSfU5RK4pj","executionInfo":{"status":"ok","timestamp":1668932571217,"user_tz":-360,"elapsed":406,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"bf6f496c-9eba-4100-a2c4-002b0646361e"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["hindi    15000\n","eng      15000\n","dtype: int64"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"-dgIdfjIRLDN"},"source":["### 3) Preparing input data for the Encoder ( `encoder_input_data` )\n","The Encoder model will be fed input data which are **preprocessed English sentences**. Following preprocessing is done:\n","\n","\n","1.   Tokenizing the Hindi sentences from `hindi_lines`.\n","2.   Determining the maximum length of the Hindi sentence that's `max_input_length`.\n","3.   Padding the `tokenized_hindi_lines` to the max_input_length.\n","4.   Determining the vocabulary size ( `num_hindi_tokens` ) for English words.\n","\n","\n","\n"]},{"cell_type":"code","source":["hindi_lines = list()\n","for line in lines.hindi:\n","    hindi_lines.append( line ) \n","\n","tokenizer = preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts( hindi_lines ) \n","tokenized_hindi_lines = tokenizer.texts_to_sequences( hindi_lines ) \n","\n","length_list = list()\n","for token_seq in tokenized_hindi_lines:\n","    length_list.append( len( token_seq ))\n","max_input_length = np.array( length_list ).max()\n","print( 'Hindi max length is {}'.format( max_input_length ))\n","\n","padded_hindi_lines = preprocessing.sequence.pad_sequences( tokenized_hindi_lines , maxlen=max_input_length , padding='post' )\n","encoder_input_data = np.array( padded_hindi_lines )\n","print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n","\n","hindi_word_dict = tokenizer.word_index\n","num_hindi_tokens = len( hindi_word_dict )+1\n","print( 'Number of Hindi tokens = {}'.format( num_hindi_tokens))\n","\n","# print(\"Dictionary Eng word to tokens\", eng_word_dict)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","print(tracemalloc.get_traced_memory())\n","\n","\n","\n","\n","\"\"\"\n","Oov tokens are out of vocabulary tokens used to replace unknown words.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"UZqA3VG9PdLn","executionInfo":{"status":"ok","timestamp":1668932610650,"user_tz":-360,"elapsed":950,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"89fa2ad8-c671-4444-d160-c550181e49d8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Hindi max length is 48\n","Encoder input data shape -> (15000, 48)\n","Number of Hindi tokens = 2504\n","(624153878, 1334826751)\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nOov tokens are out of vocabulary tokens used to replace unknown words.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"cRwAd310SPkG"},"source":["### 4) Preparing input data for the Decoder ( `decoder_input_data` )\n","The Decoder model will be fed the preprocessed Hindi lines. Preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\n","\n","\n","*   Append `<START>` tag at the first position in  each Hindi sentence.\n","*   Append `<END>` tag at the last position in  each Hindi sentence.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"deB0oX_0pj8R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668932622596,"user_tz":-360,"elapsed":930,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"e486e275-86dc-4f7c-d433-30bce317c30f"},"source":["\n","eng_lines = list()\n","for line in lines.eng:\n","    eng_lines.append( '<START> ' + line + ' <END>' )  \n","\n","tokenizer = preprocessing.text.Tokenizer(oov_token=1)\n","tokenizer.fit_on_texts( eng_lines ) \n","tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) \n","\n","length_list = list()\n","for token_seq in tokenized_eng_lines:\n","    length_list.append( len( token_seq ))\n","max_output_length = np.array( length_list ).max()\n","print( 'English max length is {}'.format( max_output_length ))\n","\n","padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_output_length, padding='post' )\n","decoder_input_data = np.array( padded_eng_lines  )\n","print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n","\n","eng_word_dict = tokenizer.word_index\n","num_eng_tokens = len( eng_word_dict )+1\n","print( 'Number of English tokens = {}'.format( num_eng_tokens))\n","\n","# print(\"Dictionary Hindi word to tokens\", hindi_word_dict)\n","print(tracemalloc.get_traced_memory())\n","\n","\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["English max length is 64\n","Decoder input data shape -> (15000, 64)\n","Number of English tokens = 2036\n","(627568460, 1334826751)\n"]}]},{"cell_type":"markdown","metadata":{"id":"DJTcSlygTQ_V"},"source":["### 5) Preparing target data for the Decoder ( decoder_target_data ) \n","\n","We take a copy of `tokenized_hindi_lines` and modify it like this.\n","\n","\n","\n","1.  Remove the `<start>` tag which we appended earlier. Hence, the word ( which is `<start>` in this case  ) will be removed.\n","2.   Convert the `padded_hindi_lines` ( ones which do not have `<start>` tag ) to one-hot vectors.\n","\n","For example :\n","\n","```\n"," [ '<start>' , 'hello' , 'world' , '<end>' ]\n","\n","```\n","\n","wil become \n","\n","```\n"," [ 'hello' , 'world' , '<end>' ]\n","\n","```\n"]},{"cell_type":"markdown","source":["## Code Crashes Here"],"metadata":{"id":"VkeFkK2PC7TB"}},{"cell_type":"code","metadata":{"id":"NPCTmeL7qj3T"},"source":["\n","decoder_target_data = list()\n","for token_seq in tokenized_eng_lines:\n","    decoder_target_data.append( token_seq[ 1 : ] ) \n","   \n","\n","\n","    \n","padded_eng_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n","onehot_eng_lines = utils.to_categorical( padded_eng_lines , num_eng_tokens )\n","decoder_target_data = np.array( onehot_eng_lines )\n","# print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_N71uykUPbe"},"source":["### 1) Defining the Encoder-Decoder model\n","The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n","\n","\n","*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n","*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n","*   LSTM layer : Provide access to Long-Short Term cells.\n","\n","Working : \n","\n","1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n","2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n","3.   These states are set in the LSTM cell of the decoder.\n","4.   The decoder_input_data comes in through the Embedding layer.\n","5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM"],"metadata":{"id":"2ZG3ceruwFXB","executionInfo":{"status":"ok","timestamp":1668932458804,"user_tz":-360,"elapsed":10911,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["encoder_inputs = Input(shape=( max_input_length ,  ))\n","encoder_embedding = Embedding( num_hindi_tokens, 256 , mask_zero=True ) (encoder_inputs)\n","encoder_outputs , state_h , state_c = LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n","encoder_states = [ state_h , state_c ]\n","\n","decoder_inputs = Input(shape=( max_output_length , ))\n","decoder_embedding = Embedding( num_eng_tokens, 256 , mask_zero=True) (decoder_inputs)\n","decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n","decoder_dense = Dense( num_eng_tokens , activation=tf.keras.activations.softmax ) \n","output = decoder_dense ( decoder_outputs )\n","\n","model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n","model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n","\n","model.summary()\n"],"metadata":{"id":"3vLS0nc0waKJ","colab":{"base_uri":"https://localhost:8080/","height":257},"executionInfo":{"status":"error","timestamp":1668932459374,"user_tz":-360,"elapsed":573,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"1cc367ce-0b22-45e0-d426-5dc65ad27e8e"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2eceb1cf2844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmax_input_length\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_hindi_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mstate_h\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'max_input_length' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"n9g_8sR7WWf3"},"source":["### 2) Training the model\n","We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."]},{"cell_type":"code","source":["mc = tf.keras.callbacks.ModelCheckpoint('my_nmt_model_min_loss.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"],"metadata":{"id":"8HDkBQXA2xdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnd2H27qt4Hy","cellView":"code"},"source":["\n","history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=100 ,\n","                     validation_split = 0.1,\n","                   callbacks=[mc], verbose=1    ) \n","model.save( 'model.h5' ) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","saved_model = keras.models.load_model('model.h5')\n","saved_model.summary()"],"metadata":{"id":"nkMjc5yWJJ0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.utils.vis_utils import plot_model\n","plot_model(saved_model, to_file='modelsummary.png', show_shapes=True, show_layer_names=True)"],"metadata":{"id":"Ksnqjy-VJxXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_inputs = saved_model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = saved_model.layers[4].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","latent_dim = 256  # Note: may be need to save in drive as well\n"],"metadata":{"id":"Xo-dWR09KFqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.legend(['train','validation'])"],"metadata":{"id":"4L589fF_3eln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eeqv_vH5pMpb"},"source":["## Inferencing on the models"]},{"cell_type":"markdown","metadata":{"id":"o4PAtzGrk8pq"},"source":["### 1) Defining inference models\n","We create inference models which help in predicting translations.\n","\n","**Encoder inference model** : Takes the English sentence as input and outputs LSTM states ( `h` and `c` ).\n","\n","**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the French input seqeunces ( ones not having the `<start>` tag ). It will output the translations of the English sentence which we fed to the encoder model and its state values.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"UNhVkiZLvdTq"},"source":["\n","def make_inference_models():\n","    \n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","    \n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","    \n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    \n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","    \n","    return encoder_model , decoder_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djEPrfJBmZE-"},"source":["### 2) Making some translations\n","\n","\n","1.   First, we take a English sequence and predict the state values using `enc_model`.\n","2.   We set the state values in the decoder's LSTM.\n","3.   Then, we generate a sequence which contains the `<start>` element.\n","4.   We input this sequence in the `dec_model`.\n","5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n","6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum sequence length.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Y_hrJcNP-mXb"},"source":["\n","def str_to_tokens( sentence : str ):\n","    words = sentence.lower().split()\n","    tokens_list = list()\n","    for word in words:\n","          print(\"word \", word,hindi_word_dict.get(word,1) )\n","          my_word=  hindi_word_dict.get(word,1)\n","          tokens_list.append(my_word) \n","  \n","    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Mfco9WKukhS","cellView":"both"},"source":["\n","enc_model , dec_model = make_inference_models()\n","\n","#encoder_input_data.shape[0] \n","for epoch in range(1 ):\n","    states_values = enc_model.predict( str_to_tokens(\"मुझे खाने से प्यार है\" ) )\n","    empty_target_seq = np.zeros( ( 1 , 1 ) )\n","    empty_target_seq[0, 0] = eng_word_dict['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","    while not stop_condition :\n","        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","        sampled_word = None\n","        for word , index in eng_word_dict.items() :\n","            if sampled_word_index == index :\n","                decoded_translation += ' {}'.format( word )\n","                sampled_word = word\n","        \n","        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","            stop_condition = True\n","            \n","        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","        empty_target_seq[ 0 , 0 ] = sampled_word_index\n","        states_values = [ h , c ] \n","\n","    # print(\"Decoded Traslation \", decoded_translation )\n","    print(f\"{bcolors.OKGREEN}Decoded Traslation: { decoded_translation}{bcolors.ENDC}\")\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Encoder and Decoder Parameters"],"metadata":{"id":"P1Wf3vttALhP"}},{"cell_type":"code","source":["%cd \"drive/MyDrive/Machine Learning/GitHub Projects/machine-translation/models/\"\n","model_name = str(lines.count().eng)\n","!mkdir $model_name\n","!ls\n","# save encoder model\n","enc_model.save( model_name+'/enc_model.h5' ) \n","# save decoder model\n","dec_model.save( model_name+'/dec_model.h5' ) \n","# save  model\n","saved_model.save( model_name+'/model.h5' ) "],"metadata":{"id":"K1KvVW7Wlb0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encoder parameters\n","encoder_parameters={\n","    'max_encoder_seq_length': max_input_length,\n","    'num_encoder_tokens': num_hindi_tokens,\n","    \n","   \n","}\n","encoder_dictionary=  hindi_word_dict\n","\n","\n","# decoder parameters\n","decoder_parameters={\n","    'max_decoder_seq_length':  max_output_length,\n","    'num_decoder_tokens': num_eng_tokens,\n","\n","}\n","\n","decoder_dictionary=  eng_word_dict\n","\n"],"metadata":{"id":"ZOph9zDmoi0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters=model_name+\"/parameters/\"\n","dictionaries=model_name+\"/dictionaries/\"\n","!mkdir $parameters\n","!mkdir $dictionaries\n"],"metadata":{"id":"1_5jr6u9U8I5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","paramters_path=parameters\n","dictionaries_path=dictionaries\n","\n","# save encoder parameter\n","with open(paramters_path+'encoder_parameters.pickle', 'wb') as handle:\n","    pickle.dump(encoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder dictionary\n","with open(dictionaries_path+'encoder_dictionary.pickle', 'wb') as handle:\n","    pickle.dump(encoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder parameter\n","with open(paramters_path+'decoder_parameters.pickle', 'wb') as handle:\n","    pickle.dump(decoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder parameter\n","with open(dictionaries_path+'decoder_dictionary.pickle', 'wb') as handle:\n","    pickle.dump(decoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n","\n"],"metadata":{"id":"tCRJRA979x7c"},"execution_count":null,"outputs":[]}]}