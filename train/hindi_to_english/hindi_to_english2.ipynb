{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "#pd.set_option('display.expand_frame_repr', False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Hindi-English IIT Data from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "\n",
    "pairs=[] \n",
    "for translation_pair in dataset[\"train\"][\"translation\"]:\n",
    "  source_sentence = translation_pair[\"hi\"]\n",
    "  target_sentence = translation_pair[\"en\"]\n",
    "  pairs.append([source_sentence, target_sentence])\n",
    "\n",
    "lines= pd.DataFrame(columns=[ \"hindi\",\"eng\"], data=pairs)\n",
    "lines= lines[:10000]\n",
    "lines.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Remove Punctuation\n",
    "lines['hindi']= [ (sent.translate(str.maketrans('', '', string.punctuation)) )    for sent in lines['hindi']  ]\n",
    "lines['eng']= [ (sent.translate(str.maketrans('', '', string.punctuation)) )    for sent in lines['eng']  ]\n",
    "######### Remove Punctuation\n",
    "\n",
    "######### Convert To Lowercase \n",
    "lines['hindi']= [ (sent.lower())  for sent in lines['hindi']   ]\n",
    "lines['eng']= [ (sent.lower())  for sent in lines['eng']   ]\n",
    "######### Convert To Lowercase \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  Popualte the lsit with sentence lengths\n",
    "\n",
    "hindi_sent_length_list=[  (len(sent.split()))  for sent in lines['hindi'] ]\n",
    "eng_sent_length_list=[  (len(sent.split()))  for sent in lines['eng'] ]\n",
    "\n",
    "length_df = pd.DataFrame({'eng':eng_sent_length_list, 'hindi':hindi_sent_length_list})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Tokenize) Convert each sentence to list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ Function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "############ Function to build a tokenizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Create english tokenizer\n",
    "eng_tokenizer = tokenization(lines['eng'])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "################ Create english tokenizer\n",
    "\n",
    "################ Create hindi tokenizer\n",
    "hindi_tokenizer = tokenization(lines['hindi'])\n",
    "hindi_vocab_size = len(hindi_tokenizer.word_index) + 1\n",
    "hindi_length = 8\n",
    "print('Hindi Vocabulary Size: %d' % hindi_vocab_size)\n",
    "################ Create hindi tokenizer\n",
    "\n",
    "\n",
    "## To print eng dictionary\n",
    "# eng_tokenizer.word_index\n",
    "\n",
    "## To print hindi dictionary\n",
    "# hindi_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode amd Pad Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 2D list where first element is hindi and second element is english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_eng=lines[['hindi','eng']].to_numpy()\n",
    "hindi_eng[:5]  #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test set\n",
    "train, test = train_test_split(hindi_eng, test_size=0.2, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Display Kernel Name is Kanye ",
   "language": "python",
   "name": "west-kanye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
