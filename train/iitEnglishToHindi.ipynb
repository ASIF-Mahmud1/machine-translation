{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IIT-2 NMT:EnglishToHindi(Working).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPwD0bTWkGmh1OMJnnhdoB2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### TASK: Create Encder-Decoder LSTM model to convert English sentences to Hindi sentences.  \n","We are going to use word level embedding"],"metadata":{"id":"YkLw2eBCN5Tu"}},{"cell_type":"code","source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'"],"metadata":{"id":"Qxm6DVwmngRe","executionInfo":{"status":"ok","timestamp":1658241882345,"user_tz":-360,"elapsed":612,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Kaggle Configuration (Run Once)"],"metadata":{"id":"wmCcnpmP0hYJ"}},{"cell_type":"code","source":["# ! pip install kaggle\n","\n","# from google.colab import files\n","\n","# uploaded = files.upload()\n","\n","# for fn in uploaded.keys():\n","#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","#       name=fn, length=len(uploaded[fn])))\n","  \n","# # Then move kaggle.json into the folder where the API expects to find it.\n","# !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n","\n","# !kaggle datasets download aiswaryaramachandran/hindienglish-corpora/download?datasetVersionNumber=1\n","\n","# !unzip hindienglish-corpora\n"],"metadata":{"id":"UT4IsXTD0m_W","executionInfo":{"status":"ok","timestamp":1658241883049,"user_tz":-360,"elapsed":5,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Preparing Data"],"metadata":{"id":"SDmvvJONOW-b"}},{"cell_type":"markdown","source":["### 1 .  Import Libraries"],"metadata":{"id":"bDhGmxabOeih"}},{"cell_type":"code","metadata":{"id":"qK2TWV1nm48Q","executionInfo":{"status":"ok","timestamp":1658241888602,"user_tz":-360,"elapsed":5557,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"source":["\n","#%tensorflow_version 2.x\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","import pandas as pd\n","import math\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### 2 . Read Data\n"],"metadata":{"id":"kq8_kYcaOwYa"}},{"cell_type":"code","metadata":{"id":"27OzmS-MIymc","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1658241889237,"user_tz":-360,"elapsed":652,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"9ada8ae6-290f-4797-b5a9-8bf370e8ed39"},"source":["\n","num_words= 5000 ## number of tokens\n","data = pd.read_csv('Hindi_English_Truncated_Corpus.csv')\n","data.tail()\n","data=data[:10000]\n","max_sentence_length= 15\n","data['english_sentence']=data['english_sentence'].apply(lambda x: str(x))\n","data['english_sentence']=data['english_sentence'].apply(lambda x:\" \".join(x.split()[:max_sentence_length]) )\n","data['hindi_sentence']=data['hindi_sentence'].apply(lambda x:\" \".join(x.split()[:max_sentence_length]) )\n","\n","\n","\n","df = pd.DataFrame({'eng': data['english_sentence'], 'hindi': data['hindi_sentence']})\n","lines= df\n","half_dataSize= math.floor(len(data.index)  /200 ) \n","lines.head()\n","\n"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 eng  \\\n","0  politicians do not have permission to do what ...   \n","1         I'd like to tell you about one such child,   \n","2  This percentage is even greater than the perce...   \n","3  what we really mean is that they're bad at not...   \n","4  .The ending portion of these Vedas is called U...   \n","\n","                                               hindi  \n","0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n","1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n","2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n","3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n","4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "],"text/html":["\n","  <div id=\"df-22575b44-e429-4156-b798-434dc4f16ec1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eng</th>\n","      <th>hindi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>politicians do not have permission to do what ...</td>\n","      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I'd like to tell you about one such child,</td>\n","      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>This percentage is even greater than the perce...</td>\n","      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>what we really mean is that they're bad at not...</td>\n","      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>.The ending portion of these Vedas is called U...</td>\n","      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22575b44-e429-4156-b798-434dc4f16ec1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-22575b44-e429-4156-b798-434dc4f16ec1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-22575b44-e429-4156-b798-434dc4f16ec1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["num_words=500"],"metadata":{"id":"pKJUzeRvPYm2","executionInfo":{"status":"ok","timestamp":1658241889238,"user_tz":-360,"elapsed":8,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dgIdfjIRLDN"},"source":["### 3) Preparing input data for the Encoder ( `encoder_input_data` )\n","The Encoder model will be fed input data which are **preprocessed English sentences**. Following preprocessing is done:\n","\n","\n","1.   Tokenizing the English sentences from `eng_lines`.\n","2.   Determining the maximum length of the English sentence that's `max_input_length`.\n","3.   Padding the `tokenized_eng_lines` to the max_input_length.\n","4.   Determining the vocabulary size ( `num_eng_tokens` ) for English words.\n","\n","\n","\n"]},{"cell_type":"code","source":["eng_lines = list()\n","for line in lines.eng:\n","    eng_lines.append( line ) \n","\n","tokenizer = preprocessing.text.Tokenizer(num_words=num_words, oov_token=1)\n","tokenizer.fit_on_texts( eng_lines ) \n","tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) \n","\n","length_list = list()\n","for token_seq in tokenized_eng_lines:\n","    length_list.append( len( token_seq ))\n","max_input_length = np.array( length_list ).max()\n","print( 'English max length is {}'.format( max_input_length ))\n","\n","padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\n","encoder_input_data = np.array( padded_eng_lines )\n","print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n","\n","eng_word_dict = tokenizer.word_index\n","num_eng_tokens = len( eng_word_dict )+1\n","print( 'Number of English tokens = {}'.format( num_eng_tokens))\n","\n","# print(\"Dictionary Eng word to tokens\", eng_word_dict)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Oov tokens are out of vocabulary tokens used to replace unknown words.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZqA3VG9PdLn","executionInfo":{"status":"ok","timestamp":1658241889616,"user_tz":-360,"elapsed":385,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"a7e34606-ccdd-4e15-8cee-237e7a84abfa"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["English max length is 26\n","Encoder input data shape -> (10000, 26)\n","Number of English tokens = 14559\n"]}]},{"cell_type":"markdown","metadata":{"id":"cRwAd310SPkG"},"source":["### 4) Preparing input data for the Decoder ( `decoder_input_data` )\n","The Decoder model will be fed the preprocessed Hindi lines. Preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\n","\n","\n","*   Append `<START>` tag at the first position in  each Hindi sentence.\n","*   Append `<END>` tag at the last position in  each Hindi sentence.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"deB0oX_0pj8R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658241890567,"user_tz":-360,"elapsed":956,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"dfc3e938-fe20-440e-f135-7442eaf836d5"},"source":["\n","hindi_lines = list()\n","for line in lines.hindi:\n","    hindi_lines.append( '<START> ' + line + ' <END>' )  \n","\n","tokenizer = preprocessing.text.Tokenizer(num_words=num_words)\n","tokenizer.fit_on_texts( hindi_lines ) \n","tokenized_hindi_lines = tokenizer.texts_to_sequences( hindi_lines ) \n","\n","length_list = list()\n","for token_seq in tokenized_hindi_lines:\n","    length_list.append( len( token_seq ))\n","max_output_length = np.array( length_list ).max()\n","print( 'Hindi max length is {}'.format( max_output_length ))\n","\n","padded_hindi_lines = preprocessing.sequence.pad_sequences( tokenized_hindi_lines , maxlen=max_output_length, padding='post' )\n","decoder_input_data = np.array( padded_hindi_lines  )\n","print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n","\n","hindi_word_dict = tokenizer.word_index\n","num_hindi_tokens = len( hindi_word_dict )+1\n","print( 'Number of Hindi tokens = {}'.format( num_hindi_tokens))\n","\n","# print(\"Dictionary Hindi word to tokens\", hindi_word_dict)\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Hindi max length is 17\n","Decoder input data shape -> (10000, 17)\n","Number of Hindi tokens = 16820\n"]}]},{"cell_type":"markdown","metadata":{"id":"DJTcSlygTQ_V"},"source":["### 5) Preparing target data for the Decoder ( decoder_target_data ) \n","\n","We take a copy of `tokenized_hindi_lines` and modify it like this.\n","\n","\n","\n","1.  Remove the `<start>` tag which we appended earlier. Hence, the word ( which is `<start>` in this case  ) will be removed.\n","2.   Convert the `padded_hindi_lines` ( ones which do not have `<start>` tag ) to one-hot vectors.\n","\n","For example :\n","\n","```\n"," [ '<start>' , 'hello' , 'world' , '<end>' ]\n","\n","```\n","\n","wil become \n","\n","```\n"," [ 'hello' , 'world' , '<end>' ]\n","\n","```\n"]},{"cell_type":"code","metadata":{"id":"NPCTmeL7qj3T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658242175245,"user_tz":-360,"elapsed":1120,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"866b6101-7cc6-40fd-d934-bfe27dc4f8f0"},"source":["EXPERIEMNT= 600\n","num_hindi_tokens=EXPERIEMNT #  COMMENT OUT THIS LINE\n","decoder_target_data = list()\n","for token_seq in tokenized_hindi_lines:\n","    decoder_target_data.append( token_seq[ 1 : ] ) \n","    \n","padded_hindi_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n","onehot_hindi_lines = utils.to_categorical( padded_hindi_lines , num_hindi_tokens )  \n","decoder_target_data = np.array( onehot_hindi_lines )\n","print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder target data shape -> (10000, 17, 600)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbIJ9GnL4koG","executionInfo":{"status":"ok","timestamp":1658241909380,"user_tz":-360,"elapsed":434,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"f415f735-5d24-4b95-c01b-802effd744c6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16820"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"M_N71uykUPbe"},"source":["### 1) Defining the Encoder-Decoder model\n","The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n","\n","\n","*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n","*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n","*   LSTM layer : Provide access to Long-Short Term cells.\n","\n","Working : \n","\n","1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n","2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n","3.   These states are set in the LSTM cell of the decoder.\n","4.   The decoder_input_data comes in through the Embedding layer.\n","5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM"],"metadata":{"id":"2ZG3ceruwFXB","executionInfo":{"status":"ok","timestamp":1658242180014,"user_tz":-360,"elapsed":399,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["encoder_inputs = Input(shape=( max_input_length ,  ))\n","encoder_embedding = Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\n","encoder_outputs , state_h , state_c = LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n","encoder_states = [ state_h , state_c ]\n","\n","decoder_inputs = Input(shape=( max_output_length , ))\n","decoder_embedding = Embedding( num_hindi_tokens, 256 , mask_zero=True) (decoder_inputs)\n","decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n","decoder_dense = Dense( num_hindi_tokens , activation=tf.keras.activations.softmax ) \n","output = decoder_dense ( decoder_outputs )\n","\n","model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n","model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n","\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vLS0nc0waKJ","executionInfo":{"status":"ok","timestamp":1658242181779,"user_tz":-360,"elapsed":466,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"5ce59ebd-1b4e-4dfd-deec-02f501dfd044"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 26)]         0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 17)]         0           []                               \n","                                                                                                  \n"," embedding_2 (Embedding)        (None, 26, 256)      3727104     ['input_3[0][0]']                \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 17, 256)      153600      ['input_4[0][0]']                \n","                                                                                                  \n"," lstm_2 (LSTM)                  [(None, 256),        525312      ['embedding_2[0][0]']            \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_3 (LSTM)                  [(None, 17, 256),    525312      ['embedding_3[0][0]',            \n","                                 (None, 256),                     'lstm_2[0][1]',                 \n","                                 (None, 256)]                     'lstm_2[0][2]']                 \n","                                                                                                  \n"," dense_1 (Dense)                (None, 17, 600)      154200      ['lstm_3[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,085,528\n","Trainable params: 5,085,528\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"n9g_8sR7WWf3"},"source":["### 2) Training the model\n","We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."]},{"cell_type":"code","source":["mc = tf.keras.callbacks.ModelCheckpoint('my_nmt_model_min_loss.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"],"metadata":{"id":"8HDkBQXA2xdx","executionInfo":{"status":"ok","timestamp":1658242187821,"user_tz":-360,"elapsed":409,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnd2H27qt4Hy","cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658247366618,"user_tz":-360,"elapsed":5176913,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"24234f40-dfde-4159-82bb-5ebb8103f196"},"source":["\n","history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=100 ,\n","                     validation_split = 0.1,\n","                   callbacks=[mc], verbose=1    ) \n","model.save( 'model.h5' ) \n"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","36/36 [==============================] - ETA: 0s - loss: 2.5455\n","Epoch 1: val_loss improved from inf to 2.20286, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 62s 2s/step - loss: 2.5455 - val_loss: 2.2029\n","Epoch 2/100\n","36/36 [==============================] - ETA: 0s - loss: 2.1585\n","Epoch 2: val_loss improved from 2.20286 to 2.09119, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 54s 2s/step - loss: 2.1585 - val_loss: 2.0912\n","Epoch 3/100\n","36/36 [==============================] - ETA: 0s - loss: 2.0916\n","Epoch 3: val_loss improved from 2.09119 to 2.05713, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 54s 2s/step - loss: 2.0916 - val_loss: 2.0571\n","Epoch 4/100\n","36/36 [==============================] - ETA: 0s - loss: 2.0518\n","Epoch 4: val_loss improved from 2.05713 to 2.01786, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 54s 2s/step - loss: 2.0518 - val_loss: 2.0179\n","Epoch 5/100\n","36/36 [==============================] - ETA: 0s - loss: 2.0080\n","Epoch 5: val_loss improved from 2.01786 to 1.97299, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 55s 2s/step - loss: 2.0080 - val_loss: 1.9730\n","Epoch 6/100\n","36/36 [==============================] - ETA: 0s - loss: 1.9675\n","Epoch 6: val_loss improved from 1.97299 to 1.94387, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 54s 2s/step - loss: 1.9675 - val_loss: 1.9439\n","Epoch 7/100\n","36/36 [==============================] - ETA: 0s - loss: 1.9374\n","Epoch 7: val_loss improved from 1.94387 to 1.92270, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 54s 1s/step - loss: 1.9374 - val_loss: 1.9227\n","Epoch 8/100\n","36/36 [==============================] - ETA: 0s - loss: 1.9133\n","Epoch 8: val_loss improved from 1.92270 to 1.90180, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.9133 - val_loss: 1.9018\n","Epoch 9/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8903\n","Epoch 9: val_loss improved from 1.90180 to 1.88383, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.8903 - val_loss: 1.8838\n","Epoch 10/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8692\n","Epoch 10: val_loss improved from 1.88383 to 1.86771, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.8692 - val_loss: 1.8677\n","Epoch 11/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8499\n","Epoch 11: val_loss improved from 1.86771 to 1.85852, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.8499 - val_loss: 1.8585\n","Epoch 12/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8336\n","Epoch 12: val_loss improved from 1.85852 to 1.84665, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 53s 1s/step - loss: 1.8336 - val_loss: 1.8467\n","Epoch 13/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8173\n","Epoch 13: val_loss improved from 1.84665 to 1.83398, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.8173 - val_loss: 1.8340\n","Epoch 14/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8011\n","Epoch 14: val_loss improved from 1.83398 to 1.82304, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.8011 - val_loss: 1.8230\n","Epoch 15/100\n","36/36 [==============================] - ETA: 0s - loss: 1.7841\n","Epoch 15: val_loss improved from 1.82304 to 1.81461, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.7841 - val_loss: 1.8146\n","Epoch 16/100\n","36/36 [==============================] - ETA: 0s - loss: 1.7703\n","Epoch 16: val_loss improved from 1.81461 to 1.80278, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.7703 - val_loss: 1.8028\n","Epoch 17/100\n","36/36 [==============================] - ETA: 0s - loss: 1.7548\n","Epoch 17: val_loss improved from 1.80278 to 1.79266, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.7548 - val_loss: 1.7927\n","Epoch 18/100\n","36/36 [==============================] - ETA: 0s - loss: 1.7394\n","Epoch 18: val_loss improved from 1.79266 to 1.78337, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.7394 - val_loss: 1.7834\n","Epoch 19/100\n","36/36 [==============================] - ETA: 0s - loss: 1.7252\n","Epoch 19: val_loss improved from 1.78337 to 1.77725, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.7252 - val_loss: 1.7773\n","Epoch 20/100\n","36/36 [==============================] - ETA: 0s - loss: 1.7116\n","Epoch 20: val_loss improved from 1.77725 to 1.76743, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.7116 - val_loss: 1.7674\n","Epoch 21/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6978\n","Epoch 21: val_loss improved from 1.76743 to 1.75728, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.6978 - val_loss: 1.7573\n","Epoch 22/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6823\n","Epoch 22: val_loss improved from 1.75728 to 1.75049, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.6823 - val_loss: 1.7505\n","Epoch 23/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6678\n","Epoch 23: val_loss improved from 1.75049 to 1.74328, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.6678 - val_loss: 1.7433\n","Epoch 24/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6535\n","Epoch 24: val_loss improved from 1.74328 to 1.73738, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.6535 - val_loss: 1.7374\n","Epoch 25/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6397\n","Epoch 25: val_loss improved from 1.73738 to 1.72821, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.6397 - val_loss: 1.7282\n","Epoch 26/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6273\n","Epoch 26: val_loss improved from 1.72821 to 1.72297, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.6273 - val_loss: 1.7230\n","Epoch 27/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6140\n","Epoch 27: val_loss improved from 1.72297 to 1.71856, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.6140 - val_loss: 1.7186\n","Epoch 28/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6006\n","Epoch 28: val_loss improved from 1.71856 to 1.71260, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.6006 - val_loss: 1.7126\n","Epoch 29/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5882\n","Epoch 29: val_loss improved from 1.71260 to 1.70798, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.5882 - val_loss: 1.7080\n","Epoch 30/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5751\n","Epoch 30: val_loss improved from 1.70798 to 1.70222, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.5751 - val_loss: 1.7022\n","Epoch 31/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5611\n","Epoch 31: val_loss improved from 1.70222 to 1.69795, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.5611 - val_loss: 1.6979\n","Epoch 32/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5484\n","Epoch 32: val_loss improved from 1.69795 to 1.69376, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.5484 - val_loss: 1.6938\n","Epoch 33/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5356\n","Epoch 33: val_loss improved from 1.69376 to 1.69034, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.5356 - val_loss: 1.6903\n","Epoch 34/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5233\n","Epoch 34: val_loss improved from 1.69034 to 1.68696, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 53s 1s/step - loss: 1.5233 - val_loss: 1.6870\n","Epoch 35/100\n","36/36 [==============================] - ETA: 0s - loss: 1.5093\n","Epoch 35: val_loss improved from 1.68696 to 1.68545, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.5093 - val_loss: 1.6855\n","Epoch 36/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4968\n","Epoch 36: val_loss improved from 1.68545 to 1.68154, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.4968 - val_loss: 1.6815\n","Epoch 37/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4837\n","Epoch 37: val_loss improved from 1.68154 to 1.67626, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.4837 - val_loss: 1.6763\n","Epoch 38/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4705\n","Epoch 38: val_loss did not improve from 1.67626\n","36/36 [==============================] - 52s 1s/step - loss: 1.4705 - val_loss: 1.6768\n","Epoch 39/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4586\n","Epoch 39: val_loss improved from 1.67626 to 1.67442, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.4586 - val_loss: 1.6744\n","Epoch 40/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4440\n","Epoch 40: val_loss improved from 1.67442 to 1.67196, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 53s 1s/step - loss: 1.4440 - val_loss: 1.6720\n","Epoch 41/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4310\n","Epoch 41: val_loss improved from 1.67196 to 1.67024, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.4310 - val_loss: 1.6702\n","Epoch 42/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4171\n","Epoch 42: val_loss improved from 1.67024 to 1.67005, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.4171 - val_loss: 1.6701\n","Epoch 43/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4031\n","Epoch 43: val_loss did not improve from 1.67005\n","36/36 [==============================] - 52s 1s/step - loss: 1.4031 - val_loss: 1.6712\n","Epoch 44/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3900\n","Epoch 44: val_loss improved from 1.67005 to 1.66996, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 51s 1s/step - loss: 1.3900 - val_loss: 1.6700\n","Epoch 45/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3761\n","Epoch 45: val_loss did not improve from 1.66996\n","36/36 [==============================] - 53s 1s/step - loss: 1.3761 - val_loss: 1.6708\n","Epoch 46/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3629\n","Epoch 46: val_loss improved from 1.66996 to 1.66892, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 52s 1s/step - loss: 1.3629 - val_loss: 1.6689\n","Epoch 47/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3487\n","Epoch 47: val_loss did not improve from 1.66892\n","36/36 [==============================] - 51s 1s/step - loss: 1.3487 - val_loss: 1.6702\n","Epoch 48/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3368\n","Epoch 48: val_loss improved from 1.66892 to 1.66879, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 53s 1s/step - loss: 1.3368 - val_loss: 1.6688\n","Epoch 49/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3233\n","Epoch 49: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.3233 - val_loss: 1.6703\n","Epoch 50/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3077\n","Epoch 50: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.3077 - val_loss: 1.6727\n","Epoch 51/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2943\n","Epoch 51: val_loss did not improve from 1.66879\n","36/36 [==============================] - 53s 1s/step - loss: 1.2943 - val_loss: 1.6759\n","Epoch 52/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2820\n","Epoch 52: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.2820 - val_loss: 1.6746\n","Epoch 53/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2667\n","Epoch 53: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.2667 - val_loss: 1.6775\n","Epoch 54/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2545\n","Epoch 54: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 1.2545 - val_loss: 1.6839\n","Epoch 55/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2407\n","Epoch 55: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.2407 - val_loss: 1.6830\n","Epoch 56/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2274\n","Epoch 56: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.2274 - val_loss: 1.6845\n","Epoch 57/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2126\n","Epoch 57: val_loss did not improve from 1.66879\n","36/36 [==============================] - 53s 1s/step - loss: 1.2126 - val_loss: 1.6852\n","Epoch 58/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1991\n","Epoch 58: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.1991 - val_loss: 1.6895\n","Epoch 59/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1867\n","Epoch 59: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.1867 - val_loss: 1.6954\n","Epoch 60/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1724\n","Epoch 60: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 1.1724 - val_loss: 1.6980\n","Epoch 61/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1596\n","Epoch 61: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.1596 - val_loss: 1.7001\n","Epoch 62/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1452\n","Epoch 62: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.1452 - val_loss: 1.7098\n","Epoch 63/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1323\n","Epoch 63: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.1323 - val_loss: 1.7146\n","Epoch 64/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1205\n","Epoch 64: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.1205 - val_loss: 1.7155\n","Epoch 65/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1061\n","Epoch 65: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 1.1061 - val_loss: 1.7203\n","Epoch 66/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0934\n","Epoch 66: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.0934 - val_loss: 1.7279\n","Epoch 67/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0821\n","Epoch 67: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.0821 - val_loss: 1.7328\n","Epoch 68/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0700\n","Epoch 68: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 1.0700 - val_loss: 1.7343\n","Epoch 69/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0558\n","Epoch 69: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.0558 - val_loss: 1.7429\n","Epoch 70/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0435\n","Epoch 70: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.0435 - val_loss: 1.7535\n","Epoch 71/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0320\n","Epoch 71: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 1.0320 - val_loss: 1.7526\n","Epoch 72/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0196\n","Epoch 72: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.0196 - val_loss: 1.7575\n","Epoch 73/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0062\n","Epoch 73: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 1.0062 - val_loss: 1.7602\n","Epoch 74/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9943\n","Epoch 74: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.9943 - val_loss: 1.7698\n","Epoch 75/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9831\n","Epoch 75: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.9831 - val_loss: 1.7775\n","Epoch 76/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9732\n","Epoch 76: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.9732 - val_loss: 1.7784\n","Epoch 77/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9579\n","Epoch 77: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.9579 - val_loss: 1.7874\n","Epoch 78/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9465\n","Epoch 78: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.9465 - val_loss: 1.7937\n","Epoch 79/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9367\n","Epoch 79: val_loss did not improve from 1.66879\n","36/36 [==============================] - 53s 1s/step - loss: 0.9367 - val_loss: 1.8019\n","Epoch 80/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9235\n","Epoch 80: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.9235 - val_loss: 1.8082\n","Epoch 81/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9144\n","Epoch 81: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.9144 - val_loss: 1.8157\n","Epoch 82/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9028\n","Epoch 82: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.9028 - val_loss: 1.8238\n","Epoch 83/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8919\n","Epoch 83: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.8919 - val_loss: 1.8296\n","Epoch 84/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8803\n","Epoch 84: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.8803 - val_loss: 1.8399\n","Epoch 85/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8707\n","Epoch 85: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.8707 - val_loss: 1.8419\n","Epoch 86/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8580\n","Epoch 86: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.8580 - val_loss: 1.8498\n","Epoch 87/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8475\n","Epoch 87: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.8475 - val_loss: 1.8533\n","Epoch 88/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8386\n","Epoch 88: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.8386 - val_loss: 1.8619\n","Epoch 89/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8288\n","Epoch 89: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.8288 - val_loss: 1.8743\n","Epoch 90/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8187\n","Epoch 90: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.8187 - val_loss: 1.8765\n","Epoch 91/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8077\n","Epoch 91: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.8077 - val_loss: 1.8876\n","Epoch 92/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8000\n","Epoch 92: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.8000 - val_loss: 1.8894\n","Epoch 93/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7896\n","Epoch 93: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.7896 - val_loss: 1.8971\n","Epoch 94/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7811\n","Epoch 94: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.7811 - val_loss: 1.9041\n","Epoch 95/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7695\n","Epoch 95: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.7695 - val_loss: 1.9128\n","Epoch 96/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7617\n","Epoch 96: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.7617 - val_loss: 1.9161\n","Epoch 97/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7534\n","Epoch 97: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.7534 - val_loss: 1.9245\n","Epoch 98/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7435\n","Epoch 98: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.7435 - val_loss: 1.9349\n","Epoch 99/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7331\n","Epoch 99: val_loss did not improve from 1.66879\n","36/36 [==============================] - 52s 1s/step - loss: 0.7331 - val_loss: 1.9380\n","Epoch 100/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7250\n","Epoch 100: val_loss did not improve from 1.66879\n","36/36 [==============================] - 51s 1s/step - loss: 0.7250 - val_loss: 1.9442\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.legend(['train','validation'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"4L589fF_3eln","executionInfo":{"status":"ok","timestamp":1658247589371,"user_tz":-360,"elapsed":488,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"8ffffe42-f340-4dfb-d46c-867a28c1c636"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7fd6ef8924d0>"]},"metadata":{},"execution_count":22},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Eeqv_vH5pMpb"},"source":["## Inferencing on the models"]},{"cell_type":"markdown","metadata":{"id":"o4PAtzGrk8pq"},"source":["### 1) Defining inference models\n","We create inference models which help in predicting translations.\n","\n","**Encoder inference model** : Takes the English sentence as input and outputs LSTM states ( `h` and `c` ).\n","\n","**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the French input seqeunces ( ones not having the `<start>` tag ). It will output the translations of the English sentence which we fed to the encoder model and its state values.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"UNhVkiZLvdTq","executionInfo":{"status":"ok","timestamp":1658247598906,"user_tz":-360,"elapsed":420,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"source":["\n","def make_inference_models():\n","    \n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","    \n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","    \n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    \n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","    \n","    return encoder_model , decoder_model\n"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djEPrfJBmZE-"},"source":["### 2) Making some translations\n","\n","\n","1.   First, we take a English sequence and predict the state values using `enc_model`.\n","2.   We set the state values in the decoder's LSTM.\n","3.   Then, we generate a sequence which contains the `<start>` element.\n","4.   We input this sequence in the `dec_model`.\n","5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n","6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum sequence length.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Y_hrJcNP-mXb","executionInfo":{"status":"ok","timestamp":1658247604593,"user_tz":-360,"elapsed":428,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"source":["\n","def str_to_tokens( sentence : str ):\n","    words = sentence.lower().split()\n","    tokens_list = list()\n","    for word in words:\n","          print(\"word \", word,eng_word_dict.get(word,1) )\n","          my_word=  eng_word_dict.get(word,1)\n","          tokens_list.append(my_word) \n","  \n","    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Mfco9WKukhS","cellView":"both","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658248266839,"user_tz":-360,"elapsed":1764,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"90491a6d-0752-485d-a260-59b2edf510fc"},"source":["\n","enc_model , dec_model = make_inference_models()\n","\n","#encoder_input_data.shape[0] \n","for epoch in range(1 ):\n","    states_values = enc_model.predict( str_to_tokens(\"food was delicious\" ) )\n","    empty_target_seq = np.zeros( ( 1 , 1 ) )\n","    empty_target_seq[0, 0] = hindi_word_dict['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","    while not stop_condition :\n","        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","        sampled_word = None\n","        for word , index in hindi_word_dict.items() :\n","            if sampled_word_index == index :\n","                decoded_translation += ' {}'.format( word )\n","                sampled_word = word\n","        \n","        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","            stop_condition = True\n","            \n","        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","        empty_target_seq[ 0 , 0 ] = sampled_word_index\n","        states_values = [ h , c ] \n","\n","    # print(\"Decoded Traslation \", decoded_translation )\n","    print(f\"{bcolors.OKGREEN}Decoded Traslation: { decoded_translation}{bcolors.ENDC}\")\n","\n","    "],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["word  food 299\n","word  was 10\n","word  delicious 1\n","WARNING:tensorflow:Model was constructed with shape (None, 17) for input KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, 1).\n","\u001b[92mDecoded Traslation:  में end\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### Save Encoder and Decoder Parameters"],"metadata":{"id":"P1Wf3vttALhP"}},{"cell_type":"code","source":["model_path= '/content/drive/MyDrive/Machine Learning/NMT_Word_Level/My Version/model/'\n","# save encoder model\n","enc_model.save( model_path+'enc_model.h5' ) \n","# save decoder model\n","dec_model.save( model_path+'dec_model.h5' ) \n","# save  model\n","model.save( model_path+'model.h5' ) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1KvVW7Wlb0-","executionInfo":{"status":"ok","timestamp":1658248319169,"user_tz":-360,"elapsed":398,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"324fe277-61a9-4211-872a-95b8cf211f69"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":["# encoder parameters\n","encoder_parameters={\n","    'max_encoder_seq_length': max_input_length,\n","    'num_encoder_tokens': num_eng_tokens,\n","   \n","}\n","encoder_dictionary= eng_word_dict\n","\n","# decoder parameters\n","decoder_parameters={\n","    'max_decoder_seq_length':  max_output_length,\n","    'num_decoder_tokens': num_hindi_tokens,\n","\n","}\n","\n","decoder_dictionary=  hindi_word_dict\n","\n"],"metadata":{"id":"ZOph9zDmoi0s","executionInfo":{"status":"ok","timestamp":1658248321819,"user_tz":-360,"elapsed":539,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["import pickle\n","paramters_path= model_path= '/content/drive/MyDrive/Machine Learning/NMT_Word_Level/My Version/'\n","dictionaries_path= model_path= '/content/drive/MyDrive/Machine Learning/NMT_Word_Level/My Version/'\n","\n","# save encoder parameter\n","with open(paramters_path+'encoder_parameters.pickle', 'wb') as handle:\n","    pickle.dump(encoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder dictionary\n","with open(dictionaries_path+'encoder_dictionary.pickle', 'wb') as handle:\n","    pickle.dump(encoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder parameter\n","with open(paramters_path+'decoder_parameters.pickle', 'wb') as handle:\n","    pickle.dump(decoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# save encoder parameter\n","with open(dictionaries_path+'decoder_dictionary.pickle', 'wb') as handle:\n","    pickle.dump(decoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n","\n"],"metadata":{"id":"tCRJRA979x7c","executionInfo":{"status":"ok","timestamp":1658248636144,"user_tz":-360,"elapsed":409,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"NK6GXJNOqFRf","executionInfo":{"status":"error","timestamp":1658248569847,"user_tz":-360,"elapsed":4219,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"c8d12caf-902c-4500-c72f-96138df578ef"},"execution_count":43,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"]}]}]}