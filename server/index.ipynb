{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "from tensorflow import keras\n",
    "from google.transliteration import transliterate_word ## not installed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences ## causing issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ../../model/lstm/15000/lstm_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m model, src_tokenizer, target_tokenizer, src_parameters, target_parameters\n\u001b[1;32m     22\u001b[0m model_path\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../../model/lstm/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mmodel_size\n\u001b[0;32m---> 23\u001b[0m model, src_tokenizer, target_tokenizer, src_parameters, target_parameters\u001b[39m=\u001b[39m load_models_and_parameters(model_path)\n\u001b[1;32m     27\u001b[0m src_length\u001b[39m=\u001b[39msrc_parameters[\u001b[39m\"\u001b[39m\u001b[39msrc_length\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m src_vocab_size\u001b[39m=\u001b[39msrc_parameters[\u001b[39m\"\u001b[39m\u001b[39msrc_vocab_size\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mload_models_and_parameters\u001b[0;34m(model_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_models_and_parameters\u001b[39m(model_size):\n\u001b[1;32m      5\u001b[0m     path\u001b[39m=\u001b[39mmodel_size\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(path\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlstm_model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path\u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msrc_parameters.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m     10\u001b[0m         src_parameters \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(handle)\n",
      "File \u001b[0;32m~/Desktop/Projects/machine-translation/server/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/Projects/machine-translation/server/venv/lib/python3.9/site-packages/keras/saving/legacy/save.py:227\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 227\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    232\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(\n\u001b[1;32m    233\u001b[0m             filepath_str, \u001b[39mcompile\u001b[39m, options\n\u001b[1;32m    234\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at ../../model/lstm/15000/lstm_model"
     ]
    }
   ],
   "source": [
    "model_size='15000'\n",
    "\n",
    "def load_models_and_parameters(model_size):\n",
    "\n",
    "    path=model_size+'/'\n",
    "\n",
    " \n",
    "    model = keras.models.load_model(path+'lstm_model')\n",
    "    with open(path+ \"src_parameters.pickle\", 'rb') as handle:\n",
    "        src_parameters = pickle.load(handle)\n",
    "\n",
    "    with open(path+ \"src_tokenizer.pickle\", 'rb') as handle:\n",
    "        src_tokenizer = pickle.load(handle)\n",
    "\n",
    "    with open(path+ \"target_parameters.pickle\", 'rb') as handle:\n",
    "        target_parameters = pickle.load(handle)\n",
    "\n",
    "    with open(path+ \"target_tokenizer.pickle\", 'rb') as handle:\n",
    "        target_tokenizer = pickle.load(handle)\n",
    "    return model, src_tokenizer, target_tokenizer, src_parameters, target_parameters\n",
    "\n",
    "model_path= '../../model/lstm/'+model_size\n",
    "model, src_tokenizer, target_tokenizer, src_parameters, target_parameters= load_models_and_parameters(model_path)\n",
    "\n",
    "\n",
    "\n",
    "src_length=src_parameters[\"src_length\"]\n",
    "src_vocab_size=src_parameters[\"src_vocab_size\"]\n",
    "\n",
    "target_length=target_parameters[\"target_length\"]\n",
    "target_vocab_size=target_parameters[\"target_vocab_size\"]\n",
    "\n",
    "print(src_length, target_length, src_vocab_size, target_vocab_size)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):  ## pass src_tokenizer for tokenizer\n",
    "    # encode and pad sequences\n",
    "    X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n",
    "    X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n",
    "    return X\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    # map an integer to a word\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "def predict_seq(model, tokenizer, source):  ## pass target_tokenizer for tokenizer\n",
    "    # generate target from a source sequence\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "\n",
    "transliterate_eng_hindi = transliterate_word('yah hamaare desh ke lie vaastav mein anivaary vastu hai.', lang_code='hi', max_suggestions=1)\n",
    "print(transliterate_eng_hindi)\n",
    "\n",
    "encoded_hindi = encode_sequences(src_tokenizer, src_length, transliterate_eng_hindi)\n",
    "\n",
    "\n",
    "source= encoded_hindi\n",
    "tar_tokenizer=target_tokenizer\n",
    "\n",
    "translation = predict_seq(model, tar_tokenizer, source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e85fb0000938f5e2a29218cbc508a2e9fdbb88a8d53596d72d5192ab39b6de65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
