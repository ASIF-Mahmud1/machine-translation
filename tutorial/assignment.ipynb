{"cells":[{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1665382564895,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"Qxm6DVwmngRe"},"outputs":[],"source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'"]},{"cell_type":"markdown","metadata":{"id":"PcMV--RFZfpC"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","import pandas as pd\n","from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM\n","from tensorflow.keras.models import load_model\n","import string\n","import pickle\n","from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","import statistics\n","import math\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets==1.18.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (1.18.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (10.0.1)\n","Requirement already satisfied: pandas in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (1.5.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (1.23.5)\n","Requirement already satisfied: dill in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (0.3.6)\n","Requirement already satisfied: aiohttp in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (3.8.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (0.11.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (2022.11.0)\n","Requirement already satisfied: packaging in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (21.3)\n","Requirement already satisfied: multiprocess in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (0.70.14)\n","Requirement already satisfied: xxhash in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (3.1.0)\n","Requirement already satisfied: requests>=2.19.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from datasets==1.18.1) (2.28.1)\n","Requirement already satisfied: attrs>=17.3.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (21.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (2.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets==1.18.1) (6.0.2)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.1) (6.0)\n","Requirement already satisfied: filelock in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.1) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.1) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from packaging->datasets==1.18.1) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.18.1) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.18.1) (1.26.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.18.1) (2022.9.24)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from pandas->datasets==1.18.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from pandas->datasets==1.18.1) (2022.1)\n","Requirement already satisfied: six>=1.5 in /Users/learn/opt/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==1.18.1) (1.16.0)\n"]},{"name":"stderr","output_type":"stream","text":["Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n","Reusing dataset parquet (/Users/learn/.cache/huggingface/datasets/parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08b26da762174b2da68f005096fd4d08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["!pip install datasets==1.18.1\n","from datasets import load_dataset\n","dataset = load_dataset(\"cfilt/iitb-english-hindi\")"]},{"cell_type":"markdown","metadata":{"id":"SDmvvJONOW-b"},"source":["## Preparing Data"]},{"cell_type":"markdown","metadata":{"id":"YkLw2eBCN5Tu"},"source":["### TASK: Create Encder-Decoder LSTM model to convert Hindi sentences to English sentences.  "]},{"cell_type":"markdown","metadata":{"id":"ZYicvRqwYpl2"},"source":["### 2. Read Data : IIT Bombay "]},{"cell_type":"markdown","metadata":{},"source":["### Text Processing"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def removePunctuation(lines):\n","    # Remove punctuation\n","    lines[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in lines[:,0]]\n","    lines[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in lines[:,1]]\n","    return lines"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["def toLowercase(lines):\n","    for i in range(len(lines)):\n","        lines[i,0] = lines[i,0].lower()   \n","        lines[i,1] = lines[i,1].lower()\n","    return lines"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["from numpy import array\n","def createDataset(dataset, data_size,type):\n","    pairs=[] \n","    for translation_pair in dataset[type][\"translation\"][:data_size]:\n","        source_sentence = translation_pair[\"hi\"]\n","        target_sentence = translation_pair[\"en\"]\n","        pairs.append([source_sentence, target_sentence])\n","    pairs = array(pairs)\n","    pairs= toLowercase(pairs)\n","    pairs=removePunctuation(pairs)\n","\n","    lines= pd.DataFrame(columns=[ \"hindi\",\"eng\"], data=pairs)\n","    lines= lines[:data_size]\n","   \n","    return lines\n","      \n"]},{"cell_type":"markdown","metadata":{"id":"-dgIdfjIRLDN"},"source":["### 3) Preparing input data for the Encoder ( `encoder_input_data` )"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def createInputDataForEncoder(lines):\n","    hindi_lines = list()\n","    for line in lines.hindi:\n","        hindi_lines.append( line ) \n","        \n","    tokenizer = preprocessing.text.Tokenizer()\n","    tokenizer.fit_on_texts( hindi_lines ) \n","    tokenized_hindi_lines = tokenizer.texts_to_sequences( hindi_lines ) \n","    length_list = list()\n","\n","    for token_seq in tokenized_hindi_lines:\n","        length_list.append( len( token_seq ))\n","\n","    # max_input_length = np.array( length_list ).max()\n","    max_input_length = math.floor(statistics.mode(length_list))\n","\n","    padded_hindi_lines = preprocessing.sequence.pad_sequences( tokenized_hindi_lines , maxlen=max_input_length , padding='post' )\n","    encoder_input_data = np.array( padded_hindi_lines )\n","   \n","    hindi_word_dict = tokenizer.word_index\n","    num_hindi_tokens = len( hindi_word_dict )+1\n","    # print(\"Hindi Dictionary\" ,hindi_word_dict  )\n","    return max_input_length, num_hindi_tokens, encoder_input_data, hindi_word_dict\n","   \n","\n","\n","                "]},{"cell_type":"markdown","metadata":{},"source":["### Create Dataset for Decoder"]},{"cell_type":"markdown","metadata":{"id":"cRwAd310SPkG"},"source":["### 4) Preparing input data for the Decoder ( `decoder_input_data` )"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["\n","def createInputDataForDecoder(lines):\n","    eng_lines = list()\n","    for line in lines.eng:\n","        eng_lines.append( '<START> ' + line + ' <END>' )  \n","\n","    tokenizer = preprocessing.text.Tokenizer(oov_token=1)\n","    tokenizer.fit_on_texts( eng_lines ) \n","    tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) \n","\n","    length_list = list()\n","    for token_seq in tokenized_eng_lines:\n","        length_list.append( len( token_seq ))\n","    # max_output_length = np.array( length_list ).max()\n","    max_output_length = math.floor(statistics.mode(length_list))\n","    print( 'English max length is {}'.format( max_output_length ))\n","\n","    padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_output_length, padding='post' )\n","    decoder_input_data = np.array( padded_eng_lines  )\n","    print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n","\n","    eng_word_dict = tokenizer.word_index\n","    num_eng_tokens = len( eng_word_dict )+1\n","    print( 'Number of English tokens = {}'.format( num_eng_tokens))\n","\n","    return max_output_length, num_eng_tokens, decoder_input_data, eng_word_dict, tokenized_eng_lines"]},{"cell_type":"markdown","metadata":{"id":"DJTcSlygTQ_V"},"source":["### 5) Preparing target data for the Decoder ( decoder_target_data ) "]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["def createDecoderTargetData(tokenized_eng_lines, max_output_length, num_eng_tokens):\n","    decoder_target_data = list()\n","    for token_seq in tokenized_eng_lines:\n","        decoder_target_data.append( token_seq[ 1 : ] ) \n","        \n","    padded_eng_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n","    onehot_eng_lines = utils.to_categorical( padded_eng_lines , num_eng_tokens )\n","    decoder_target_data = np.array( onehot_eng_lines )\n","    print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n","    return decoder_target_data"]},{"cell_type":"markdown","metadata":{"id":"M_N71uykUPbe"},"source":["### 1) Defining the Encoder-Decoder model"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["\n","def create_encoder_decoder(encoder_inputs,encoder_states,decoder_lstm,decoder_embedding,decoder_dense,decoder_inputs):\n","    \n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","    \n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","    \n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    \n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","    \n","    return encoder_model , decoder_model"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def createEncoderDecoderModel(max_input_length, max_output_length, num_hindi_tokens, num_eng_tokens):\n","    encoder_inputs = Input(shape=( max_input_length ,  ))\n","    encoder_embedding = Embedding( num_hindi_tokens, 256 , mask_zero=True ) (encoder_inputs)\n","    encoder_outputs , state_h , state_c = LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n","    encoder_states = [ state_h , state_c ]\n","\n","    decoder_inputs = Input(shape=( max_output_length , ))\n","    decoder_embedding = Embedding( num_eng_tokens, 256 , mask_zero=True) (decoder_inputs)\n","    decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","    decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n","    decoder_dense = Dense( num_eng_tokens , activation=tf.keras.activations.softmax ) \n","    output = decoder_dense ( decoder_outputs )\n","\n","    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n","    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n","    encoder_model , decoder_model= create_encoder_decoder(encoder_inputs,encoder_states,decoder_lstm,decoder_embedding,decoder_dense,decoder_inputs)\n","    return model ,encoder_model , decoder_model\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Save Encoder And Decoder Parameters"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["\n","def save_models_and_parameters(lines,model,encoder_model , decoder_model, max_input_length, num_hindi_tokens, hindi_word_dict,max_output_length,num_eng_tokens,eng_word_dict):\n","    model_name = str(lines.count().eng)\n","    !mkdir $model_name\n","    !ls\n","    model.save( model_name+'/model.h5' ) \n","    # save encoder model\n","    encoder_model.save( model_name+'/enc_model.h5' ) \n","    # save decoder model\n","    decoder_model.save( model_name+'/dec_model.h5' ) \n","\n","    # encoder parameters\n","    encoder_parameters={\n","        'max_encoder_seq_length': max_input_length,\n","        'num_encoder_tokens': num_hindi_tokens,\n","        \n","    \n","    }\n","    encoder_dictionary=  hindi_word_dict\n","\n","\n","    # decoder parameters\n","    decoder_parameters={\n","        'max_decoder_seq_length':  max_output_length,\n","        'num_decoder_tokens': num_eng_tokens,\n","\n","    }\n","\n","    decoder_dictionary=  eng_word_dict\n","    parameters=model_name+\"/parameters/\"\n","    dictionaries=model_name+\"/dictionaries/\"\n","    !mkdir $parameters\n","    !mkdir $dictionaries\n","    paramters_path=parameters\n","    dictionaries_path=dictionaries\n","\n","    # save encoder parameter\n","    with open(paramters_path+'encoder_parameters.pickle', 'wb') as handle:\n","        pickle.dump(encoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    # save encoder dictionary\n","    with open(dictionaries_path+'encoder_dictionary.pickle', 'wb') as handle:\n","        pickle.dump(encoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    # save encoder parameter\n","    with open(paramters_path+'decoder_parameters.pickle', 'wb') as handle:\n","        pickle.dump(decoder_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    # save encoder parameter\n","    with open(dictionaries_path+'decoder_dictionary.pickle', 'wb') as handle:\n","        pickle.dump(decoder_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n","    print(\"Model \",model_name,\" saved successfully!\")    \n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Handle Helper Functions "]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m1000\u001b[m\u001b[m                     \u001b[34m15000\u001b[m\u001b[m                    assignment.ipynb\n","\u001b[34m10000\u001b[m\u001b[m                    \u001b[34m5000\u001b[m\u001b[m                     my_nmt_model_min_loss.h5\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['/Users/learn/Desktop/Projects/machine-translation/tutorial', '/Users/learn/.vscode/extensions/ms-toolsai.jupyter-2022.11.1003412109/pythonFiles', '/Users/learn/.vscode/extensions/ms-toolsai.jupyter-2022.11.1003412109/pythonFiles/lib/python', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python39.zip', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/lib-dynload', '', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages', '/Users/learn/.cache/huggingface/modules', '/Users/learn/Desktop/Projects/helper', '/Users/learn/Desktop/Projects/utils', '/Users/learn/Desktop/Projects/machine-translation/test/helper', '/Users/learn/Desktop/Projects/machine-translation/utils']\n","['/Users/learn/Desktop/Projects/machine-translation/tutorial', '/Users/learn/.vscode/extensions/ms-toolsai.jupyter-2022.11.1003412109/pythonFiles', '/Users/learn/.vscode/extensions/ms-toolsai.jupyter-2022.11.1003412109/pythonFiles/lib/python', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python39.zip', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/lib-dynload', '', '/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages', '/Users/learn/.cache/huggingface/modules', '/Users/learn/Desktop/Projects/helper', '/Users/learn/Desktop/Projects/utils', '/Users/learn/Desktop/Projects/machine-translation/test/helper', '/Users/learn/Desktop/Projects/machine-translation/utils', '/Users/learn/Desktop/Projects/machine-translation/test/helper', '/Users/learn/Desktop/Projects/machine-translation/utils']\n"]}],"source":["import os\n","import sys\n","\n","def handle_helper_functions():\n","    print(sys.path)\n","    directory_path = os.path.abspath(os.path.join('../../helper'))\n","    if directory_path not in sys.path:\n","        sys.path.append(directory_path)    \n","\n","    translation_path=os.path.abspath(os.path.join('../../utils')) \n","    if translation_path not in sys.path:\n","        sys.path.append(translation_path)    #/Users/learn/Desktop/Projects/machine-translation/test/helper\n","      #/Users/learn/Desktop/Projects/machine-translation/utils   \n","    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/test/helper')                                         #/Users/learn/Desktop/Projects/machine-translation/utils   \n","    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/utils')\n","    print(sys.path)\n","    \n","\n","handle_helper_functions()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","import tensorflow_text as text\n","from scoreTest import get_cosine_val, get_BLEU_score, get_ROUGE_score\n","from translate import  translate_sentence"]},{"cell_type":"markdown","metadata":{},"source":["### Get Summary Statistics For Every Model"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def get_model_statistics_summary(model_path,path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary, encoderPath, decoderPath, lines ):\n","\n","################################## START  ##################################\n","    reconstructed_model = keras.models.load_model(model_path)\n","    plot_model(reconstructed_model, to_file='modelsummary.png', show_shapes=True, show_layer_names=True)\n","    reconstructed_model.summary()\n","\n","\n","    ## Load Dictionaries and Parameters \n","    path_encoder_parameters= path_encoder_parameters\n","    path_encoder_dictionary= path_encoder_dictionary\n","    path_decoder_parameters= path_decoder_parameters\n","    path_decoder_dictionary= path_decoder_dictionary\n","\n","    # loading\n","    with open(path_encoder_parameters, 'rb') as handle:\n","        encoder_parameters = pickle.load(handle)\n","\n","    # loading\n","    with open(path_encoder_dictionary, 'rb') as handle:\n","        encoder_dictionary = pickle.load(handle)\n","\n","    # loading\n","    with open(path_decoder_parameters, 'rb') as handle:\n","        decoder_parameters= pickle.load(handle)\n","\n","    # loading\n","    with open(path_decoder_dictionary, 'rb') as handle:\n","        decoder_dictionary = pickle.load(handle)    \n","\n","    print(encoder_parameters)\n","    # encoder_dictionary\n","    print(decoder_parameters)\n","    # decoder_dictionary\n","\n","    encoder_inputs = reconstructed_model.input[0]  # input_1\n","    encoder_outputs, state_h_enc, state_c_enc = reconstructed_model.layers[4].output  # lstm_1\n","    encoder_states = [state_h_enc, state_c_enc]\n","    encoder_model = keras.Model(encoder_inputs, encoder_states)\n","    latent_dim = 256  # Note: may be need to save in drive as well\n","\n","\n","    num_decoder_tokens =decoder_parameters['num_decoder_tokens']\n","    max_output_length= decoder_parameters['max_decoder_seq_length']\n","    max_input_length= encoder_parameters['max_encoder_seq_length']\n","\n","    encoder_word_dict=encoder_dictionary\n","    decoder_word_dict= decoder_dictionary\n","\n","\n","    decoder_inputs = Input(shape=( max_output_length , ))\n","    decoder_embedding = Embedding( num_decoder_tokens, 256 , mask_zero=True) (decoder_inputs)\n","\n","    decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","    decoder_dense = Dense( num_decoder_tokens , activation=tf.keras.activations.softmax ) \n","\n","\n","   \n","\n","    def make_inference_models():\n","        \n","            encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","            \n","            decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","            decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","            \n","            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","            \n","            decoder_outputs, state_h, state_c = decoder_lstm(\n","                decoder_embedding , initial_state=decoder_states_inputs)\n","            decoder_states = [state_h, state_c]\n","            decoder_outputs = decoder_dense(decoder_outputs)\n","            decoder_model = tf.keras.models.Model(\n","                [decoder_inputs] + decoder_states_inputs,\n","                [decoder_outputs] + decoder_states)\n","            \n","            return encoder_model , decoder_model\n","\n","\n","    enc_model , dec_model = make_inference_models()\n","\n","\n","    # # Test Previous Model\n","\n","\n","    # encoderPath= encoderPath\n","    # decoderPath= decoderPath\n","\n","    # # loading\n","\n","    # enc_model =  load_model(encoderPath)\n","    # dec_model  =  load_model(decoderPath)\n","\n","\n","\n","################################## END  ##################################\n","\n","    ## Get sentences to test the model\n","\n","    sample_sentences= lines\n","    sample_sentences\n","\n","    # Reference Token \n","\n","    reference_tokens=[]\n","\n","    for line in sample_sentences['eng']:\n","        # print( line.split() ) \n","        reference_tokens.append( line.split() )\n","\n","    df = pd.DataFrame(      columns=['reference', 'candidate', 'bleu_score'],  )\n","\n","    df[\"reference\"]= reference_tokens\n","\n","    ####### START Calculate Cosine Similarity for two sentences\n","    scores=[]\n","    for line in sample_sentences['eng']:\n","        translation= translate_sentence(line, enc_model,dec_model, encoder_word_dict,  decoder_word_dict , max_input_length, preprocessing)\n","        result= get_cosine_val (translation, line)\n","        scores.append(result)\n","\n","    df[\"cosine_similarity\"]= scores    ## Cosine score calculated\n","\n","    ####### END Calculate Cosine Similarity for two sentences\n","    \n","\n","    # Candidate Tokens \n","    candidate_tokens=[]\n","\n","\n","    for line in sample_sentences['hindi']:\n","    \n","        result= translate_sentence(line, enc_model,dec_model, encoder_word_dict,  decoder_word_dict , max_input_length, preprocessing)\n","        temp =result.split()\n","        temp= temp[:-1]\n","        candidate_tokens.append(temp)\n","        \n","\n","    df[\"candidate\"]= candidate_tokens\n","\n","\n","    ## Calculate BLEU score\n","\n","    scores=get_BLEU_score(df,sentence_bleu)\n","    df[\"bleu_score\"]= scores    ## BLEU score calculated\n","\n","    ## Calcualte ROUGE score\n","    scores= get_ROUGE_score(df, pd, tf,text)\n","    df[\"rouge_score\"]= scores  ## ROUGE score calculated\n","\n","    rouge_metric= pd.DataFrame.from_records(df['rouge_score'])\n","\n","    average_f_measure = rouge_metric['f_measure'].mean()\n","    average_p_measure = rouge_metric['p_measure'].mean()\n","    average_r_measure = rouge_metric['r_measure'].mean()\n","    average_cosine= df['cosine_similarity'].mean()\n","    average_bleu= df['bleu_score'].mean()\n","\n","    ## return BLEU and ROUGE score to the list \n","    return [average_f_measure, average_p_measure,average_r_measure, average_cosine, average_bleu]\n","\n","        \n","\n","    "]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["data_size=100000\n","\n","pool_oftexts=createDataset(dataset=dataset,data_size=data_size, type=\"train\")\n","pool_oftexts=pool_oftexts[500:]\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["99500"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["len(pool_oftexts)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":814,"status":"ok","timestamp":1665382649109,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"3vLS0nc0waKJ","outputId":"158e53d8-3236-464b-de51-4ec1ab1ac19d"},"outputs":[],"source":["model_size=[1000, 5000, 10000, 15000]\n","my_list_of_trained_models=[] "]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on\n"," hindi    1000\n","eng      1000\n","dtype: int64\n","English max length is 4\n","Decoder input data shape -> (1000, 4)\n","Number of English tokens = 1412\n","Decoder target data shape -> (1000, 4, 1412)\n","Epoch 1/100\n","3/4 [=====================>........] - ETA: 0s - loss: 6.9036\n","Epoch 1: val_loss improved from inf to 6.86413, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 4s 189ms/step - loss: 6.9244 - val_loss: 6.8641\n","Epoch 2/100\n","3/4 [=====================>........] - ETA: 0s - loss: 6.9043\n","Epoch 2: val_loss improved from 6.86413 to 6.81154, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 68ms/step - loss: 6.8889 - val_loss: 6.8115\n","Epoch 3/100\n","3/4 [=====================>........] - ETA: 0s - loss: 6.8209\n","Epoch 3: val_loss improved from 6.81154 to 6.67010, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 68ms/step - loss: 6.8074 - val_loss: 6.6701\n","Epoch 4/100\n","4/4 [==============================] - ETA: 0s - loss: 6.5699\n","Epoch 4: val_loss improved from 6.67010 to 6.23798, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 111ms/step - loss: 6.5699 - val_loss: 6.2380\n","Epoch 5/100\n","3/4 [=====================>........] - ETA: 0s - loss: 5.9532\n","Epoch 5: val_loss improved from 6.23798 to 5.28308, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 89ms/step - loss: 5.8625 - val_loss: 5.2831\n","Epoch 6/100\n","4/4 [==============================] - ETA: 0s - loss: 4.9096\n","Epoch 6: val_loss improved from 5.28308 to 5.03221, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 4.9096 - val_loss: 5.0322\n","Epoch 7/100\n","4/4 [==============================] - ETA: 0s - loss: 4.7790\n","Epoch 7: val_loss improved from 5.03221 to 4.98531, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 88ms/step - loss: 4.7790 - val_loss: 4.9853\n","Epoch 8/100\n","4/4 [==============================] - ETA: 0s - loss: 4.5964\n","Epoch 8: val_loss improved from 4.98531 to 4.94993, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 93ms/step - loss: 4.5964 - val_loss: 4.9499\n","Epoch 9/100\n","4/4 [==============================] - ETA: 0s - loss: 4.5266\n","Epoch 9: val_loss did not improve from 4.94993\n","4/4 [==============================] - 0s 80ms/step - loss: 4.5266 - val_loss: 4.9588\n","Epoch 10/100\n","4/4 [==============================] - ETA: 0s - loss: 4.4653\n","Epoch 10: val_loss improved from 4.94993 to 4.91838, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 97ms/step - loss: 4.4653 - val_loss: 4.9184\n","Epoch 11/100\n","3/4 [=====================>........] - ETA: 0s - loss: 4.3904\n","Epoch 11: val_loss improved from 4.91838 to 4.90411, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 88ms/step - loss: 4.3950 - val_loss: 4.9041\n","Epoch 12/100\n","4/4 [==============================] - ETA: 0s - loss: 4.3608\n","Epoch 12: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 78ms/step - loss: 4.3608 - val_loss: 4.9217\n","Epoch 13/100\n","4/4 [==============================] - ETA: 0s - loss: 4.3414\n","Epoch 13: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 84ms/step - loss: 4.3414 - val_loss: 4.9228\n","Epoch 14/100\n","4/4 [==============================] - ETA: 0s - loss: 4.3077\n","Epoch 14: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 81ms/step - loss: 4.3077 - val_loss: 4.9142\n","Epoch 15/100\n","4/4 [==============================] - ETA: 0s - loss: 4.2791\n","Epoch 15: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 91ms/step - loss: 4.2791 - val_loss: 4.9137\n","Epoch 16/100\n","4/4 [==============================] - ETA: 0s - loss: 4.2584\n","Epoch 16: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 80ms/step - loss: 4.2584 - val_loss: 4.9115\n","Epoch 17/100\n","4/4 [==============================] - ETA: 0s - loss: 4.2313\n","Epoch 17: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 81ms/step - loss: 4.2313 - val_loss: 4.9117\n","Epoch 18/100\n","4/4 [==============================] - ETA: 0s - loss: 4.2029\n","Epoch 18: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 84ms/step - loss: 4.2029 - val_loss: 4.9153\n","Epoch 19/100\n","3/4 [=====================>........] - ETA: 0s - loss: 4.1669\n","Epoch 19: val_loss did not improve from 4.90411\n","4/4 [==============================] - 0s 79ms/step - loss: 4.1742 - val_loss: 4.9102\n","Epoch 20/100\n","4/4 [==============================] - ETA: 0s - loss: 4.1392\n","Epoch 20: val_loss improved from 4.90411 to 4.89342, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 94ms/step - loss: 4.1392 - val_loss: 4.8934\n","Epoch 21/100\n","4/4 [==============================] - ETA: 0s - loss: 4.1005\n","Epoch 21: val_loss improved from 4.89342 to 4.87324, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 4.1005 - val_loss: 4.8732\n","Epoch 22/100\n","4/4 [==============================] - ETA: 0s - loss: 4.0564\n","Epoch 22: val_loss improved from 4.87324 to 4.85324, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 4.0564 - val_loss: 4.8532\n","Epoch 23/100\n","4/4 [==============================] - ETA: 0s - loss: 4.0081\n","Epoch 23: val_loss improved from 4.85324 to 4.83052, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 4.0081 - val_loss: 4.8305\n","Epoch 24/100\n","4/4 [==============================] - ETA: 0s - loss: 3.9574\n","Epoch 24: val_loss improved from 4.83052 to 4.80570, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 93ms/step - loss: 3.9574 - val_loss: 4.8057\n","Epoch 25/100\n","4/4 [==============================] - ETA: 0s - loss: 3.9044\n","Epoch 25: val_loss improved from 4.80570 to 4.77112, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 3.9044 - val_loss: 4.7711\n","Epoch 26/100\n","4/4 [==============================] - ETA: 0s - loss: 3.8489\n","Epoch 26: val_loss improved from 4.77112 to 4.74177, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 91ms/step - loss: 3.8489 - val_loss: 4.7418\n","Epoch 27/100\n","4/4 [==============================] - ETA: 0s - loss: 3.7954\n","Epoch 27: val_loss improved from 4.74177 to 4.70341, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 3.7954 - val_loss: 4.7034\n","Epoch 28/100\n","4/4 [==============================] - ETA: 0s - loss: 3.7403\n","Epoch 28: val_loss improved from 4.70341 to 4.67064, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 3.7403 - val_loss: 4.6706\n","Epoch 29/100\n","4/4 [==============================] - ETA: 0s - loss: 3.6881\n","Epoch 29: val_loss improved from 4.67064 to 4.64196, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 3.6881 - val_loss: 4.6420\n","Epoch 30/100\n","4/4 [==============================] - ETA: 0s - loss: 3.6360\n","Epoch 30: val_loss improved from 4.64196 to 4.61173, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 3.6360 - val_loss: 4.6117\n","Epoch 31/100\n","4/4 [==============================] - ETA: 0s - loss: 3.5860\n","Epoch 31: val_loss improved from 4.61173 to 4.58165, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 3.5860 - val_loss: 4.5816\n","Epoch 32/100\n","4/4 [==============================] - ETA: 0s - loss: 3.5379\n","Epoch 32: val_loss improved from 4.58165 to 4.55283, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 98ms/step - loss: 3.5379 - val_loss: 4.5528\n","Epoch 33/100\n","4/4 [==============================] - ETA: 0s - loss: 3.4864\n","Epoch 33: val_loss improved from 4.55283 to 4.53562, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 3.4864 - val_loss: 4.5356\n","Epoch 34/100\n","4/4 [==============================] - ETA: 0s - loss: 3.4386\n","Epoch 34: val_loss improved from 4.53562 to 4.50854, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 3.4386 - val_loss: 4.5085\n","Epoch 35/100\n","4/4 [==============================] - ETA: 0s - loss: 3.3918\n","Epoch 35: val_loss improved from 4.50854 to 4.49199, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 3.3918 - val_loss: 4.4920\n","Epoch 36/100\n","4/4 [==============================] - ETA: 0s - loss: 3.3439\n","Epoch 36: val_loss improved from 4.49199 to 4.46706, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 91ms/step - loss: 3.3439 - val_loss: 4.4671\n","Epoch 37/100\n","4/4 [==============================] - ETA: 0s - loss: 3.2951\n","Epoch 37: val_loss improved from 4.46706 to 4.45416, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 3.2951 - val_loss: 4.4542\n","Epoch 38/100\n","4/4 [==============================] - ETA: 0s - loss: 3.2506\n","Epoch 38: val_loss improved from 4.45416 to 4.43801, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 3.2506 - val_loss: 4.4380\n","Epoch 39/100\n","4/4 [==============================] - ETA: 0s - loss: 3.2005\n","Epoch 39: val_loss improved from 4.43801 to 4.42937, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 94ms/step - loss: 3.2005 - val_loss: 4.4294\n","Epoch 40/100\n","4/4 [==============================] - ETA: 0s - loss: 3.1530\n","Epoch 40: val_loss improved from 4.42937 to 4.41346, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 93ms/step - loss: 3.1530 - val_loss: 4.4135\n","Epoch 41/100\n","4/4 [==============================] - ETA: 0s - loss: 3.1026\n","Epoch 41: val_loss improved from 4.41346 to 4.40259, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 93ms/step - loss: 3.1026 - val_loss: 4.4026\n","Epoch 42/100\n","4/4 [==============================] - ETA: 0s - loss: 3.0550\n","Epoch 42: val_loss improved from 4.40259 to 4.39347, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 3.0550 - val_loss: 4.3935\n","Epoch 43/100\n","4/4 [==============================] - ETA: 0s - loss: 3.0043\n","Epoch 43: val_loss did not improve from 4.39347\n","4/4 [==============================] - 0s 80ms/step - loss: 3.0043 - val_loss: 4.3939\n","Epoch 44/100\n","4/4 [==============================] - ETA: 0s - loss: 2.9554\n","Epoch 44: val_loss improved from 4.39347 to 4.36772, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 98ms/step - loss: 2.9554 - val_loss: 4.3677\n","Epoch 45/100\n","4/4 [==============================] - ETA: 0s - loss: 2.9006\n","Epoch 45: val_loss did not improve from 4.36772\n","4/4 [==============================] - 0s 81ms/step - loss: 2.9006 - val_loss: 4.3719\n","Epoch 46/100\n","4/4 [==============================] - ETA: 0s - loss: 2.8529\n","Epoch 46: val_loss improved from 4.36772 to 4.34838, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 98ms/step - loss: 2.8529 - val_loss: 4.3484\n","Epoch 47/100\n","4/4 [==============================] - ETA: 0s - loss: 2.7987\n","Epoch 47: val_loss did not improve from 4.34838\n","4/4 [==============================] - 0s 81ms/step - loss: 2.7987 - val_loss: 4.3489\n","Epoch 48/100\n","4/4 [==============================] - ETA: 0s - loss: 2.7427\n","Epoch 48: val_loss improved from 4.34838 to 4.34069, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 99ms/step - loss: 2.7427 - val_loss: 4.3407\n","Epoch 49/100\n","4/4 [==============================] - ETA: 0s - loss: 2.6904\n","Epoch 49: val_loss improved from 4.34069 to 4.31555, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 92ms/step - loss: 2.6904 - val_loss: 4.3155\n","Epoch 50/100\n","4/4 [==============================] - ETA: 0s - loss: 2.6354\n","Epoch 50: val_loss did not improve from 4.31555\n","4/4 [==============================] - 0s 82ms/step - loss: 2.6354 - val_loss: 4.3176\n","Epoch 51/100\n","4/4 [==============================] - ETA: 0s - loss: 2.5766\n","Epoch 51: val_loss improved from 4.31555 to 4.29073, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 2.5766 - val_loss: 4.2907\n","Epoch 52/100\n","4/4 [==============================] - ETA: 0s - loss: 2.5196\n","Epoch 52: val_loss improved from 4.29073 to 4.28889, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 93ms/step - loss: 2.5196 - val_loss: 4.2889\n","Epoch 53/100\n","4/4 [==============================] - ETA: 0s - loss: 2.4631\n","Epoch 53: val_loss improved from 4.28889 to 4.28370, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 114ms/step - loss: 2.4631 - val_loss: 4.2837\n","Epoch 54/100\n","4/4 [==============================] - ETA: 0s - loss: 2.4027\n","Epoch 54: val_loss improved from 4.28370 to 4.26829, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 90ms/step - loss: 2.4027 - val_loss: 4.2683\n","Epoch 55/100\n","4/4 [==============================] - ETA: 0s - loss: 2.3391\n","Epoch 55: val_loss improved from 4.26829 to 4.26679, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 2.3391 - val_loss: 4.2668\n","Epoch 56/100\n","4/4 [==============================] - ETA: 0s - loss: 2.2797\n","Epoch 56: val_loss improved from 4.26679 to 4.24259, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 2.2797 - val_loss: 4.2426\n","Epoch 57/100\n","4/4 [==============================] - ETA: 0s - loss: 2.2197\n","Epoch 57: val_loss did not improve from 4.24259\n","4/4 [==============================] - 0s 78ms/step - loss: 2.2197 - val_loss: 4.2632\n","Epoch 58/100\n","4/4 [==============================] - ETA: 0s - loss: 2.1603\n","Epoch 58: val_loss improved from 4.24259 to 4.22268, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 2.1603 - val_loss: 4.2227\n","Epoch 59/100\n","4/4 [==============================] - ETA: 0s - loss: 2.1000\n","Epoch 59: val_loss did not improve from 4.22268\n","4/4 [==============================] - 0s 78ms/step - loss: 2.1000 - val_loss: 4.2404\n","Epoch 60/100\n","4/4 [==============================] - ETA: 0s - loss: 2.0370\n","Epoch 60: val_loss improved from 4.22268 to 4.21621, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 97ms/step - loss: 2.0370 - val_loss: 4.2162\n","Epoch 61/100\n","4/4 [==============================] - ETA: 0s - loss: 1.9739\n","Epoch 61: val_loss improved from 4.21621 to 4.21109, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 1.9739 - val_loss: 4.2111\n","Epoch 62/100\n","4/4 [==============================] - ETA: 0s - loss: 1.9142\n","Epoch 62: val_loss improved from 4.21109 to 4.20411, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 93ms/step - loss: 1.9142 - val_loss: 4.2041\n","Epoch 63/100\n","4/4 [==============================] - ETA: 0s - loss: 1.8538\n","Epoch 63: val_loss improved from 4.20411 to 4.18700, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 100ms/step - loss: 1.8538 - val_loss: 4.1870\n","Epoch 64/100\n","4/4 [==============================] - ETA: 0s - loss: 1.7903\n","Epoch 64: val_loss did not improve from 4.18700\n","4/4 [==============================] - 0s 77ms/step - loss: 1.7903 - val_loss: 4.1892\n","Epoch 65/100\n","4/4 [==============================] - ETA: 0s - loss: 1.7305\n","Epoch 65: val_loss improved from 4.18700 to 4.18191, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 102ms/step - loss: 1.7305 - val_loss: 4.1819\n","Epoch 66/100\n","4/4 [==============================] - ETA: 0s - loss: 1.6703\n","Epoch 66: val_loss improved from 4.18191 to 4.17424, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 90ms/step - loss: 1.6703 - val_loss: 4.1742\n","Epoch 67/100\n","4/4 [==============================] - ETA: 0s - loss: 1.6138\n","Epoch 67: val_loss did not improve from 4.17424\n","4/4 [==============================] - 0s 81ms/step - loss: 1.6138 - val_loss: 4.1752\n","Epoch 68/100\n","4/4 [==============================] - ETA: 0s - loss: 1.5569\n","Epoch 68: val_loss improved from 4.17424 to 4.16403, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 95ms/step - loss: 1.5569 - val_loss: 4.1640\n","Epoch 69/100\n","4/4 [==============================] - ETA: 0s - loss: 1.4980\n","Epoch 69: val_loss improved from 4.16403 to 4.14458, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 98ms/step - loss: 1.4980 - val_loss: 4.1446\n","Epoch 70/100\n","4/4 [==============================] - ETA: 0s - loss: 1.4384\n","Epoch 70: val_loss did not improve from 4.14458\n","4/4 [==============================] - 0s 81ms/step - loss: 1.4384 - val_loss: 4.1538\n","Epoch 71/100\n","4/4 [==============================] - ETA: 0s - loss: 1.3851\n","Epoch 71: val_loss improved from 4.14458 to 4.14272, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 98ms/step - loss: 1.3851 - val_loss: 4.1427\n","Epoch 72/100\n","4/4 [==============================] - ETA: 0s - loss: 1.3298\n","Epoch 72: val_loss improved from 4.14272 to 4.13671, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 91ms/step - loss: 1.3298 - val_loss: 4.1367\n","Epoch 73/100\n","4/4 [==============================] - ETA: 0s - loss: 1.2761\n","Epoch 73: val_loss did not improve from 4.13671\n","4/4 [==============================] - 0s 80ms/step - loss: 1.2761 - val_loss: 4.1441\n","Epoch 74/100\n","4/4 [==============================] - ETA: 0s - loss: 1.2268\n","Epoch 74: val_loss improved from 4.13671 to 4.12480, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 98ms/step - loss: 1.2268 - val_loss: 4.1248\n","Epoch 75/100\n","4/4 [==============================] - ETA: 0s - loss: 1.1766\n","Epoch 75: val_loss did not improve from 4.12480\n","4/4 [==============================] - 0s 76ms/step - loss: 1.1766 - val_loss: 4.1552\n","Epoch 76/100\n","4/4 [==============================] - ETA: 0s - loss: 1.1254\n","Epoch 76: val_loss did not improve from 4.12480\n","4/4 [==============================] - 0s 88ms/step - loss: 1.1254 - val_loss: 4.1364\n","Epoch 77/100\n","4/4 [==============================] - ETA: 0s - loss: 1.0794\n","Epoch 77: val_loss did not improve from 4.12480\n","4/4 [==============================] - 0s 85ms/step - loss: 1.0794 - val_loss: 4.1287\n","Epoch 78/100\n","4/4 [==============================] - ETA: 0s - loss: 1.0300\n","Epoch 78: val_loss did not improve from 4.12480\n","4/4 [==============================] - 0s 86ms/step - loss: 1.0300 - val_loss: 4.1457\n","Epoch 79/100\n","4/4 [==============================] - ETA: 0s - loss: 0.9889\n","Epoch 79: val_loss did not improve from 4.12480\n","4/4 [==============================] - 0s 84ms/step - loss: 0.9889 - val_loss: 4.1274\n","Epoch 80/100\n","4/4 [==============================] - ETA: 0s - loss: 0.9440\n","Epoch 80: val_loss did not improve from 4.12480\n","4/4 [==============================] - 0s 84ms/step - loss: 0.9440 - val_loss: 4.1474\n","Epoch 81/100\n","4/4 [==============================] - ETA: 0s - loss: 0.9049\n","Epoch 81: val_loss improved from 4.12480 to 4.11941, saving model to my_nmt_model_min_loss.h5\n","4/4 [==============================] - 0s 96ms/step - loss: 0.9049 - val_loss: 4.1194\n","Epoch 82/100\n","4/4 [==============================] - ETA: 0s - loss: 0.8626\n","Epoch 82: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 78ms/step - loss: 0.8626 - val_loss: 4.1550\n","Epoch 83/100\n","4/4 [==============================] - ETA: 0s - loss: 0.8296\n","Epoch 83: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 85ms/step - loss: 0.8296 - val_loss: 4.1244\n","Epoch 84/100\n","4/4 [==============================] - ETA: 0s - loss: 0.7897\n","Epoch 84: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 83ms/step - loss: 0.7897 - val_loss: 4.1557\n","Epoch 85/100\n","4/4 [==============================] - ETA: 0s - loss: 0.7552\n","Epoch 85: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 84ms/step - loss: 0.7552 - val_loss: 4.1286\n","Epoch 86/100\n","4/4 [==============================] - ETA: 0s - loss: 0.7263\n","Epoch 86: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 82ms/step - loss: 0.7263 - val_loss: 4.1601\n","Epoch 87/100\n","4/4 [==============================] - ETA: 0s - loss: 0.6928\n","Epoch 87: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 89ms/step - loss: 0.6928 - val_loss: 4.1330\n","Epoch 88/100\n","4/4 [==============================] - ETA: 0s - loss: 0.6641\n","Epoch 88: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 95ms/step - loss: 0.6641 - val_loss: 4.1603\n","Epoch 89/100\n","4/4 [==============================] - ETA: 0s - loss: 0.6342\n","Epoch 89: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 92ms/step - loss: 0.6342 - val_loss: 4.1453\n","Epoch 90/100\n","4/4 [==============================] - ETA: 0s - loss: 0.6077\n","Epoch 90: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 93ms/step - loss: 0.6077 - val_loss: 4.1593\n","Epoch 91/100\n","4/4 [==============================] - ETA: 0s - loss: 0.5812\n","Epoch 91: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 86ms/step - loss: 0.5812 - val_loss: 4.1504\n","Epoch 92/100\n","4/4 [==============================] - ETA: 0s - loss: 0.5565\n","Epoch 92: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 85ms/step - loss: 0.5565 - val_loss: 4.1703\n","Epoch 93/100\n","4/4 [==============================] - ETA: 0s - loss: 0.5347\n","Epoch 93: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 89ms/step - loss: 0.5347 - val_loss: 4.1553\n","Epoch 94/100\n","4/4 [==============================] - ETA: 0s - loss: 0.5130\n","Epoch 94: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 81ms/step - loss: 0.5130 - val_loss: 4.1612\n","Epoch 95/100\n","4/4 [==============================] - ETA: 0s - loss: 0.4912\n","Epoch 95: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 85ms/step - loss: 0.4912 - val_loss: 4.1683\n","Epoch 96/100\n","4/4 [==============================] - ETA: 0s - loss: 0.4721\n","Epoch 96: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 82ms/step - loss: 0.4721 - val_loss: 4.1660\n","Epoch 97/100\n","4/4 [==============================] - ETA: 0s - loss: 0.4515\n","Epoch 97: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 83ms/step - loss: 0.4515 - val_loss: 4.1839\n","Epoch 98/100\n","4/4 [==============================] - ETA: 0s - loss: 0.4327\n","Epoch 98: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 82ms/step - loss: 0.4327 - val_loss: 4.1650\n","Epoch 99/100\n","4/4 [==============================] - ETA: 0s - loss: 0.4162\n","Epoch 99: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 81ms/step - loss: 0.4162 - val_loss: 4.1797\n","Epoch 100/100\n","4/4 [==============================] - ETA: 0s - loss: 0.4017\n","Epoch 100: val_loss did not improve from 4.11941\n","4/4 [==============================] - 0s 83ms/step - loss: 0.4017 - val_loss: 4.1721\n","mkdir: 1000: File exists\n","\u001b[34m1000\u001b[m\u001b[m                     \u001b[34m15000\u001b[m\u001b[m                    assignment.ipynb\n","\u001b[34m10000\u001b[m\u001b[m                    \u001b[34m5000\u001b[m\u001b[m                     my_nmt_model_min_loss.h5\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","mkdir: 1000/parameters/: File exists\n","mkdir: 1000/dictionaries/: File exists\n","Model  1000  saved successfully!\n","Training on\n"," hindi    5000\n","eng      5000\n","dtype: int64\n","English max length is 4\n","Decoder input data shape -> (5000, 4)\n","Number of English tokens = 2942\n","Decoder target data shape -> (5000, 4, 2942)\n","Epoch 1/100\n","18/18 [==============================] - ETA: 0s - loss: 7.4445\n","Epoch 1: val_loss improved from inf to 6.54002, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 4s 126ms/step - loss: 7.4445 - val_loss: 6.5400\n","Epoch 2/100\n","18/18 [==============================] - ETA: 0s - loss: 5.2624\n","Epoch 2: val_loss improved from 6.54002 to 4.84720, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 99ms/step - loss: 5.2624 - val_loss: 4.8472\n","Epoch 3/100\n","18/18 [==============================] - ETA: 0s - loss: 4.7106\n","Epoch 3: val_loss improved from 4.84720 to 4.68873, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 101ms/step - loss: 4.7106 - val_loss: 4.6887\n","Epoch 4/100\n","18/18 [==============================] - ETA: 0s - loss: 4.5639\n","Epoch 4: val_loss improved from 4.68873 to 4.59734, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 4.5639 - val_loss: 4.5973\n","Epoch 5/100\n","18/18 [==============================] - ETA: 0s - loss: 4.4215\n","Epoch 5: val_loss improved from 4.59734 to 4.44434, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 4.4215 - val_loss: 4.4443\n","Epoch 6/100\n","18/18 [==============================] - ETA: 0s - loss: 4.2348\n","Epoch 6: val_loss improved from 4.44434 to 4.27887, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 4.2348 - val_loss: 4.2789\n","Epoch 7/100\n","18/18 [==============================] - ETA: 0s - loss: 4.0632\n","Epoch 7: val_loss improved from 4.27887 to 4.14458, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 4.0632 - val_loss: 4.1446\n","Epoch 8/100\n","18/18 [==============================] - ETA: 0s - loss: 3.9115\n","Epoch 8: val_loss improved from 4.14458 to 4.03106, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 3.9115 - val_loss: 4.0311\n","Epoch 9/100\n","18/18 [==============================] - ETA: 0s - loss: 3.7854\n","Epoch 9: val_loss improved from 4.03106 to 3.93649, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 3.7854 - val_loss: 3.9365\n","Epoch 10/100\n","18/18 [==============================] - ETA: 0s - loss: 3.6697\n","Epoch 10: val_loss improved from 3.93649 to 3.84315, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 3.6697 - val_loss: 3.8432\n","Epoch 11/100\n","18/18 [==============================] - ETA: 0s - loss: 3.5626\n","Epoch 11: val_loss improved from 3.84315 to 3.77760, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 3.5626 - val_loss: 3.7776\n","Epoch 12/100\n","18/18 [==============================] - ETA: 0s - loss: 3.4551\n","Epoch 12: val_loss improved from 3.77760 to 3.70351, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 3.4551 - val_loss: 3.7035\n","Epoch 13/100\n","18/18 [==============================] - ETA: 0s - loss: 3.3465\n","Epoch 13: val_loss improved from 3.70351 to 3.63550, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 3.3465 - val_loss: 3.6355\n","Epoch 14/100\n","18/18 [==============================] - ETA: 0s - loss: 3.2383\n","Epoch 14: val_loss improved from 3.63550 to 3.57070, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 101ms/step - loss: 3.2383 - val_loss: 3.5707\n","Epoch 15/100\n","18/18 [==============================] - ETA: 0s - loss: 3.1284\n","Epoch 15: val_loss improved from 3.57070 to 3.51593, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 3.1284 - val_loss: 3.5159\n","Epoch 16/100\n","18/18 [==============================] - ETA: 0s - loss: 3.0196\n","Epoch 16: val_loss improved from 3.51593 to 3.45004, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 3.0196 - val_loss: 3.4500\n","Epoch 17/100\n","18/18 [==============================] - ETA: 0s - loss: 2.9095\n","Epoch 17: val_loss improved from 3.45004 to 3.39189, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 2.9095 - val_loss: 3.3919\n","Epoch 18/100\n","18/18 [==============================] - ETA: 0s - loss: 2.7976\n","Epoch 18: val_loss improved from 3.39189 to 3.33181, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 2.7976 - val_loss: 3.3318\n","Epoch 19/100\n","18/18 [==============================] - ETA: 0s - loss: 2.6811\n","Epoch 19: val_loss improved from 3.33181 to 3.27432, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 2.6811 - val_loss: 3.2743\n","Epoch 20/100\n","18/18 [==============================] - ETA: 0s - loss: 2.5665\n","Epoch 20: val_loss improved from 3.27432 to 3.21617, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 2.5665 - val_loss: 3.2162\n","Epoch 21/100\n","18/18 [==============================] - ETA: 0s - loss: 2.4500\n","Epoch 21: val_loss improved from 3.21617 to 3.16552, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 110ms/step - loss: 2.4500 - val_loss: 3.1655\n","Epoch 22/100\n","18/18 [==============================] - ETA: 0s - loss: 2.3378\n","Epoch 22: val_loss improved from 3.16552 to 3.11303, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 106ms/step - loss: 2.3378 - val_loss: 3.1130\n","Epoch 23/100\n","18/18 [==============================] - ETA: 0s - loss: 2.2231\n","Epoch 23: val_loss improved from 3.11303 to 3.06352, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 2.2231 - val_loss: 3.0635\n","Epoch 24/100\n","18/18 [==============================] - ETA: 0s - loss: 2.1090\n","Epoch 24: val_loss improved from 3.06352 to 3.00725, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 107ms/step - loss: 2.1090 - val_loss: 3.0072\n","Epoch 25/100\n","18/18 [==============================] - ETA: 0s - loss: 2.0000\n","Epoch 25: val_loss improved from 3.00725 to 2.95482, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 2.0000 - val_loss: 2.9548\n","Epoch 26/100\n","18/18 [==============================] - ETA: 0s - loss: 1.8925\n","Epoch 26: val_loss improved from 2.95482 to 2.90869, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 1.8925 - val_loss: 2.9087\n","Epoch 27/100\n","18/18 [==============================] - ETA: 0s - loss: 1.7878\n","Epoch 27: val_loss improved from 2.90869 to 2.85381, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 1.7878 - val_loss: 2.8538\n","Epoch 28/100\n","18/18 [==============================] - ETA: 0s - loss: 1.6858\n","Epoch 28: val_loss improved from 2.85381 to 2.81217, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 1.6858 - val_loss: 2.8122\n","Epoch 29/100\n","18/18 [==============================] - ETA: 0s - loss: 1.5877\n","Epoch 29: val_loss improved from 2.81217 to 2.76844, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 1.5877 - val_loss: 2.7684\n","Epoch 30/100\n","18/18 [==============================] - ETA: 0s - loss: 1.4955\n","Epoch 30: val_loss improved from 2.76844 to 2.73227, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 109ms/step - loss: 1.4955 - val_loss: 2.7323\n","Epoch 31/100\n","18/18 [==============================] - ETA: 0s - loss: 1.4070\n","Epoch 31: val_loss improved from 2.73227 to 2.69878, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 1.4070 - val_loss: 2.6988\n","Epoch 32/100\n","18/18 [==============================] - ETA: 0s - loss: 1.3212\n","Epoch 32: val_loss improved from 2.69878 to 2.66569, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 106ms/step - loss: 1.3212 - val_loss: 2.6657\n","Epoch 33/100\n","18/18 [==============================] - ETA: 0s - loss: 1.2392\n","Epoch 33: val_loss improved from 2.66569 to 2.63090, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 116ms/step - loss: 1.2392 - val_loss: 2.6309\n","Epoch 34/100\n","18/18 [==============================] - ETA: 0s - loss: 1.1604\n","Epoch 34: val_loss improved from 2.63090 to 2.59229, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 116ms/step - loss: 1.1604 - val_loss: 2.5923\n","Epoch 35/100\n","18/18 [==============================] - ETA: 0s - loss: 1.0873\n","Epoch 35: val_loss improved from 2.59229 to 2.56656, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 113ms/step - loss: 1.0873 - val_loss: 2.5666\n","Epoch 36/100\n","18/18 [==============================] - ETA: 0s - loss: 1.0182\n","Epoch 36: val_loss improved from 2.56656 to 2.54162, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 106ms/step - loss: 1.0182 - val_loss: 2.5416\n","Epoch 37/100\n","18/18 [==============================] - ETA: 0s - loss: 0.9517\n","Epoch 37: val_loss improved from 2.54162 to 2.51835, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 0.9517 - val_loss: 2.5184\n","Epoch 38/100\n","18/18 [==============================] - ETA: 0s - loss: 0.8905\n","Epoch 38: val_loss improved from 2.51835 to 2.49551, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 0.8905 - val_loss: 2.4955\n","Epoch 39/100\n","18/18 [==============================] - ETA: 0s - loss: 0.8323\n","Epoch 39: val_loss improved from 2.49551 to 2.47775, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 0.8323 - val_loss: 2.4778\n","Epoch 40/100\n","18/18 [==============================] - ETA: 0s - loss: 0.7782\n","Epoch 40: val_loss improved from 2.47775 to 2.45677, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 0.7782 - val_loss: 2.4568\n","Epoch 41/100\n","18/18 [==============================] - ETA: 0s - loss: 0.7243\n","Epoch 41: val_loss improved from 2.45677 to 2.44643, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 0.7243 - val_loss: 2.4464\n","Epoch 42/100\n","18/18 [==============================] - ETA: 0s - loss: 0.6785\n","Epoch 42: val_loss improved from 2.44643 to 2.42974, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 0.6785 - val_loss: 2.4297\n","Epoch 43/100\n","18/18 [==============================] - ETA: 0s - loss: 0.6333\n","Epoch 43: val_loss improved from 2.42974 to 2.41668, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 0.6333 - val_loss: 2.4167\n","Epoch 44/100\n","18/18 [==============================] - ETA: 0s - loss: 0.5937\n","Epoch 44: val_loss improved from 2.41668 to 2.40508, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 0.5937 - val_loss: 2.4051\n","Epoch 45/100\n","18/18 [==============================] - ETA: 0s - loss: 0.5549\n","Epoch 45: val_loss improved from 2.40508 to 2.39784, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 102ms/step - loss: 0.5549 - val_loss: 2.3978\n","Epoch 46/100\n","18/18 [==============================] - ETA: 0s - loss: 0.5194\n","Epoch 46: val_loss improved from 2.39784 to 2.38831, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 103ms/step - loss: 0.5194 - val_loss: 2.3883\n","Epoch 47/100\n","18/18 [==============================] - ETA: 0s - loss: 0.4854\n","Epoch 47: val_loss improved from 2.38831 to 2.37631, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 0.4854 - val_loss: 2.3763\n","Epoch 48/100\n","18/18 [==============================] - ETA: 0s - loss: 0.4529\n","Epoch 48: val_loss improved from 2.37631 to 2.37085, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 0.4529 - val_loss: 2.3708\n","Epoch 49/100\n","18/18 [==============================] - ETA: 0s - loss: 0.4257\n","Epoch 49: val_loss improved from 2.37085 to 2.36209, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 105ms/step - loss: 0.4257 - val_loss: 2.3621\n","Epoch 50/100\n","18/18 [==============================] - ETA: 0s - loss: 0.3992\n","Epoch 50: val_loss improved from 2.36209 to 2.35799, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 109ms/step - loss: 0.3992 - val_loss: 2.3580\n","Epoch 51/100\n","18/18 [==============================] - ETA: 0s - loss: 0.3746\n","Epoch 51: val_loss improved from 2.35799 to 2.35693, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 108ms/step - loss: 0.3746 - val_loss: 2.3569\n","Epoch 52/100\n","18/18 [==============================] - ETA: 0s - loss: 0.3504\n","Epoch 52: val_loss improved from 2.35693 to 2.35630, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 106ms/step - loss: 0.3504 - val_loss: 2.3563\n","Epoch 53/100\n","18/18 [==============================] - ETA: 0s - loss: 0.3285\n","Epoch 53: val_loss improved from 2.35630 to 2.34232, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 104ms/step - loss: 0.3285 - val_loss: 2.3423\n","Epoch 54/100\n","18/18 [==============================] - ETA: 0s - loss: 0.3104\n","Epoch 54: val_loss did not improve from 2.34232\n","18/18 [==============================] - 2s 103ms/step - loss: 0.3104 - val_loss: 2.3435\n","Epoch 55/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2924\n","Epoch 55: val_loss did not improve from 2.34232\n","18/18 [==============================] - 2s 102ms/step - loss: 0.2924 - val_loss: 2.3522\n","Epoch 56/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2749\n","Epoch 56: val_loss did not improve from 2.34232\n","18/18 [==============================] - 2s 101ms/step - loss: 0.2749 - val_loss: 2.3454\n","Epoch 57/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2582\n","Epoch 57: val_loss improved from 2.34232 to 2.33557, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 109ms/step - loss: 0.2582 - val_loss: 2.3356\n","Epoch 58/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2432\n","Epoch 58: val_loss improved from 2.33557 to 2.33273, saving model to my_nmt_model_min_loss.h5\n","18/18 [==============================] - 2s 107ms/step - loss: 0.2432 - val_loss: 2.3327\n","Epoch 59/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2292\n","Epoch 59: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.2292 - val_loss: 2.3404\n","Epoch 60/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2152\n","Epoch 60: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.2152 - val_loss: 2.3396\n","Epoch 61/100\n","18/18 [==============================] - ETA: 0s - loss: 0.2040\n","Epoch 61: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.2040 - val_loss: 2.3390\n","Epoch 62/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1924\n","Epoch 62: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 105ms/step - loss: 0.1924 - val_loss: 2.3397\n","Epoch 63/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1830\n","Epoch 63: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.1830 - val_loss: 2.3425\n","Epoch 64/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1703\n","Epoch 64: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.1703 - val_loss: 2.3496\n","Epoch 65/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1628\n","Epoch 65: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.1628 - val_loss: 2.3487\n","Epoch 66/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1554\n","Epoch 66: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.1554 - val_loss: 2.3458\n","Epoch 67/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1473\n","Epoch 67: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 105ms/step - loss: 0.1473 - val_loss: 2.3530\n","Epoch 68/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1401\n","Epoch 68: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.1401 - val_loss: 2.3483\n","Epoch 69/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1323\n","Epoch 69: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 104ms/step - loss: 0.1323 - val_loss: 2.3585\n","Epoch 70/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1265\n","Epoch 70: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.1265 - val_loss: 2.3513\n","Epoch 71/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1197\n","Epoch 71: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.1197 - val_loss: 2.3598\n","Epoch 72/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1136\n","Epoch 72: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.1136 - val_loss: 2.3604\n","Epoch 73/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1088\n","Epoch 73: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 106ms/step - loss: 0.1088 - val_loss: 2.3590\n","Epoch 74/100\n","18/18 [==============================] - ETA: 0s - loss: 0.1051\n","Epoch 74: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.1051 - val_loss: 2.3664\n","Epoch 75/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0993\n","Epoch 75: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.0993 - val_loss: 2.3633\n","Epoch 76/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0968\n","Epoch 76: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.0968 - val_loss: 2.3687\n","Epoch 77/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0910\n","Epoch 77: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0910 - val_loss: 2.3698\n","Epoch 78/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0877\n","Epoch 78: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0877 - val_loss: 2.3717\n","Epoch 79/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0852\n","Epoch 79: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.0852 - val_loss: 2.3724\n","Epoch 80/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 80: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0810 - val_loss: 2.3773\n","Epoch 81/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0790\n","Epoch 81: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0790 - val_loss: 2.3784\n","Epoch 82/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0753\n","Epoch 82: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 105ms/step - loss: 0.0753 - val_loss: 2.3815\n","Epoch 83/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0745\n","Epoch 83: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0745 - val_loss: 2.3826\n","Epoch 84/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0718\n","Epoch 84: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0718 - val_loss: 2.3837\n","Epoch 85/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0695\n","Epoch 85: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0695 - val_loss: 2.3868\n","Epoch 86/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0677\n","Epoch 86: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 101ms/step - loss: 0.0677 - val_loss: 2.3890\n","Epoch 87/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0659\n","Epoch 87: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 106ms/step - loss: 0.0659 - val_loss: 2.3906\n","Epoch 88/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0635\n","Epoch 88: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0635 - val_loss: 2.3936\n","Epoch 89/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 89: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0622 - val_loss: 2.3958\n","Epoch 90/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0596\n","Epoch 90: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0596 - val_loss: 2.3986\n","Epoch 91/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0580\n","Epoch 91: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 106ms/step - loss: 0.0580 - val_loss: 2.3972\n","Epoch 92/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0570\n","Epoch 92: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0570 - val_loss: 2.3954\n","Epoch 93/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0555\n","Epoch 93: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 106ms/step - loss: 0.0555 - val_loss: 2.3970\n","Epoch 94/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 94: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 107ms/step - loss: 0.0543 - val_loss: 2.3944\n","Epoch 95/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 95: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 106ms/step - loss: 0.0534 - val_loss: 2.4049\n","Epoch 96/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 96: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0524 - val_loss: 2.4057\n","Epoch 97/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0508\n","Epoch 97: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 102ms/step - loss: 0.0508 - val_loss: 2.4042\n","Epoch 98/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 98: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0503 - val_loss: 2.4078\n","Epoch 99/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 99: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 103ms/step - loss: 0.0484 - val_loss: 2.4153\n","Epoch 100/100\n","18/18 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 100: val_loss did not improve from 2.33273\n","18/18 [==============================] - 2s 106ms/step - loss: 0.0487 - val_loss: 2.4157\n","mkdir: 5000: File exists\n","\u001b[34m1000\u001b[m\u001b[m                     \u001b[34m15000\u001b[m\u001b[m                    assignment.ipynb\n","\u001b[34m10000\u001b[m\u001b[m                    \u001b[34m5000\u001b[m\u001b[m                     my_nmt_model_min_loss.h5\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","mkdir: 5000/parameters/: File exists\n","mkdir: 5000/dictionaries/: File exists\n","Model  5000  saved successfully!\n","Training on\n"," hindi    10000\n","eng      10000\n","dtype: int64\n","English max length is 4\n","Decoder input data shape -> (10000, 4)\n","Number of English tokens = 3804\n","Decoder target data shape -> (10000, 4, 3804)\n","Epoch 1/100\n","36/36 [==============================] - ETA: 0s - loss: 6.5540\n","Epoch 1: val_loss improved from inf to 5.00817, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 7s 124ms/step - loss: 6.5540 - val_loss: 5.0082\n","Epoch 2/100\n","36/36 [==============================] - ETA: 0s - loss: 4.7361\n","Epoch 2: val_loss improved from 5.00817 to 4.65356, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 4.7361 - val_loss: 4.6536\n","Epoch 3/100\n","36/36 [==============================] - ETA: 0s - loss: 4.4140\n","Epoch 3: val_loss improved from 4.65356 to 4.32408, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 4.4140 - val_loss: 4.3241\n","Epoch 4/100\n","36/36 [==============================] - ETA: 0s - loss: 4.0921\n","Epoch 4: val_loss improved from 4.32408 to 4.06084, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 4.0921 - val_loss: 4.0608\n","Epoch 5/100\n","36/36 [==============================] - ETA: 0s - loss: 3.8507\n","Epoch 5: val_loss improved from 4.06084 to 3.88035, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 3.8507 - val_loss: 3.8804\n","Epoch 6/100\n","36/36 [==============================] - ETA: 0s - loss: 3.6661\n","Epoch 6: val_loss improved from 3.88035 to 3.74114, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 3.6661 - val_loss: 3.7411\n","Epoch 7/100\n","36/36 [==============================] - ETA: 0s - loss: 3.4984\n","Epoch 7: val_loss improved from 3.74114 to 3.60344, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 115ms/step - loss: 3.4984 - val_loss: 3.6034\n","Epoch 8/100\n","36/36 [==============================] - ETA: 0s - loss: 3.3254\n","Epoch 8: val_loss improved from 3.60344 to 3.46065, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 3.3254 - val_loss: 3.4607\n","Epoch 9/100\n","36/36 [==============================] - ETA: 0s - loss: 3.1424\n","Epoch 9: val_loss improved from 3.46065 to 3.32191, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 3.1424 - val_loss: 3.3219\n","Epoch 10/100\n","36/36 [==============================] - ETA: 0s - loss: 2.9563\n","Epoch 10: val_loss improved from 3.32191 to 3.17572, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 2.9563 - val_loss: 3.1757\n","Epoch 11/100\n","36/36 [==============================] - ETA: 0s - loss: 2.7588\n","Epoch 11: val_loss improved from 3.17572 to 3.01798, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 2.7588 - val_loss: 3.0180\n","Epoch 12/100\n","36/36 [==============================] - ETA: 0s - loss: 2.5588\n","Epoch 12: val_loss improved from 3.01798 to 2.86136, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 2.5588 - val_loss: 2.8614\n","Epoch 13/100\n","36/36 [==============================] - ETA: 0s - loss: 2.3537\n","Epoch 13: val_loss improved from 2.86136 to 2.71420, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 121ms/step - loss: 2.3537 - val_loss: 2.7142\n","Epoch 14/100\n","36/36 [==============================] - ETA: 0s - loss: 2.1606\n","Epoch 14: val_loss improved from 2.71420 to 2.57762, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 2.1606 - val_loss: 2.5776\n","Epoch 15/100\n","36/36 [==============================] - ETA: 0s - loss: 1.9768\n","Epoch 15: val_loss improved from 2.57762 to 2.45413, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 1.9768 - val_loss: 2.4541\n","Epoch 16/100\n","36/36 [==============================] - ETA: 0s - loss: 1.8043\n","Epoch 16: val_loss improved from 2.45413 to 2.33426, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 1.8043 - val_loss: 2.3343\n","Epoch 17/100\n","36/36 [==============================] - ETA: 0s - loss: 1.6433\n","Epoch 17: val_loss improved from 2.33426 to 2.23182, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 119ms/step - loss: 1.6433 - val_loss: 2.2318\n","Epoch 18/100\n","36/36 [==============================] - ETA: 0s - loss: 1.4968\n","Epoch 18: val_loss improved from 2.23182 to 2.14666, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 1.4968 - val_loss: 2.1467\n","Epoch 19/100\n","36/36 [==============================] - ETA: 0s - loss: 1.3633\n","Epoch 19: val_loss improved from 2.14666 to 2.05127, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 124ms/step - loss: 1.3633 - val_loss: 2.0513\n","Epoch 20/100\n","36/36 [==============================] - ETA: 0s - loss: 1.2377\n","Epoch 20: val_loss improved from 2.05127 to 1.97533, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 5s 128ms/step - loss: 1.2377 - val_loss: 1.9753\n","Epoch 21/100\n","36/36 [==============================] - ETA: 0s - loss: 1.1240\n","Epoch 21: val_loss improved from 1.97533 to 1.89436, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 122ms/step - loss: 1.1240 - val_loss: 1.8944\n","Epoch 22/100\n","36/36 [==============================] - ETA: 0s - loss: 1.0206\n","Epoch 22: val_loss improved from 1.89436 to 1.83445, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 1.0206 - val_loss: 1.8344\n","Epoch 23/100\n","36/36 [==============================] - ETA: 0s - loss: 0.9233\n","Epoch 23: val_loss improved from 1.83445 to 1.77559, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.9233 - val_loss: 1.7756\n","Epoch 24/100\n","36/36 [==============================] - ETA: 0s - loss: 0.8367\n","Epoch 24: val_loss improved from 1.77559 to 1.71928, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.8367 - val_loss: 1.7193\n","Epoch 25/100\n","36/36 [==============================] - ETA: 0s - loss: 0.7557\n","Epoch 25: val_loss improved from 1.71928 to 1.66739, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 115ms/step - loss: 0.7557 - val_loss: 1.6674\n","Epoch 26/100\n","36/36 [==============================] - ETA: 0s - loss: 0.6836\n","Epoch 26: val_loss improved from 1.66739 to 1.61729, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 0.6836 - val_loss: 1.6173\n","Epoch 27/100\n","36/36 [==============================] - ETA: 0s - loss: 0.6194\n","Epoch 27: val_loss improved from 1.61729 to 1.58216, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 0.6194 - val_loss: 1.5822\n","Epoch 28/100\n","36/36 [==============================] - ETA: 0s - loss: 0.5588\n","Epoch 28: val_loss improved from 1.58216 to 1.54781, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 0.5588 - val_loss: 1.5478\n","Epoch 29/100\n","36/36 [==============================] - ETA: 0s - loss: 0.5057\n","Epoch 29: val_loss improved from 1.54781 to 1.52014, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 116ms/step - loss: 0.5057 - val_loss: 1.5201\n","Epoch 30/100\n","36/36 [==============================] - ETA: 0s - loss: 0.4563\n","Epoch 30: val_loss improved from 1.52014 to 1.49143, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 0.4563 - val_loss: 1.4914\n","Epoch 31/100\n","36/36 [==============================] - ETA: 0s - loss: 0.4138\n","Epoch 31: val_loss improved from 1.49143 to 1.46987, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 0.4138 - val_loss: 1.4699\n","Epoch 32/100\n","36/36 [==============================] - ETA: 0s - loss: 0.3754\n","Epoch 32: val_loss improved from 1.46987 to 1.45804, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 0.3754 - val_loss: 1.4580\n","Epoch 33/100\n","36/36 [==============================] - ETA: 0s - loss: 0.3424\n","Epoch 33: val_loss improved from 1.45804 to 1.43138, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 118ms/step - loss: 0.3424 - val_loss: 1.4314\n","Epoch 34/100\n","36/36 [==============================] - ETA: 0s - loss: 0.3115\n","Epoch 34: val_loss improved from 1.43138 to 1.42432, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 119ms/step - loss: 0.3115 - val_loss: 1.4243\n","Epoch 35/100\n","36/36 [==============================] - ETA: 0s - loss: 0.2851\n","Epoch 35: val_loss improved from 1.42432 to 1.41205, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.2851 - val_loss: 1.4120\n","Epoch 36/100\n","36/36 [==============================] - ETA: 0s - loss: 0.2606\n","Epoch 36: val_loss improved from 1.41205 to 1.40309, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.2606 - val_loss: 1.4031\n","Epoch 37/100\n","36/36 [==============================] - ETA: 0s - loss: 0.2391\n","Epoch 37: val_loss improved from 1.40309 to 1.39908, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.2391 - val_loss: 1.3991\n","Epoch 38/100\n","36/36 [==============================] - ETA: 0s - loss: 0.2205\n","Epoch 38: val_loss improved from 1.39908 to 1.38985, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.2205 - val_loss: 1.3898\n","Epoch 39/100\n","36/36 [==============================] - ETA: 0s - loss: 0.2039\n","Epoch 39: val_loss improved from 1.38985 to 1.38145, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.2039 - val_loss: 1.3815\n","Epoch 40/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1888\n","Epoch 40: val_loss improved from 1.38145 to 1.37742, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 117ms/step - loss: 0.1888 - val_loss: 1.3774\n","Epoch 41/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1744\n","Epoch 41: val_loss improved from 1.37742 to 1.37297, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 123ms/step - loss: 0.1744 - val_loss: 1.3730\n","Epoch 42/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1629\n","Epoch 42: val_loss did not improve from 1.37297\n","36/36 [==============================] - 5s 142ms/step - loss: 0.1629 - val_loss: 1.3773\n","Epoch 43/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1508\n","Epoch 43: val_loss did not improve from 1.37297\n","36/36 [==============================] - 4s 124ms/step - loss: 0.1508 - val_loss: 1.3741\n","Epoch 44/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1412\n","Epoch 44: val_loss improved from 1.37297 to 1.37039, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 124ms/step - loss: 0.1412 - val_loss: 1.3704\n","Epoch 45/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1329\n","Epoch 45: val_loss improved from 1.37039 to 1.36741, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 113ms/step - loss: 0.1329 - val_loss: 1.3674\n","Epoch 46/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1249\n","Epoch 46: val_loss improved from 1.36741 to 1.36657, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 111ms/step - loss: 0.1249 - val_loss: 1.3666\n","Epoch 47/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1177\n","Epoch 47: val_loss did not improve from 1.36657\n","36/36 [==============================] - 4s 110ms/step - loss: 0.1177 - val_loss: 1.3699\n","Epoch 48/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1107\n","Epoch 48: val_loss did not improve from 1.36657\n","36/36 [==============================] - 4s 109ms/step - loss: 0.1107 - val_loss: 1.3707\n","Epoch 49/100\n","36/36 [==============================] - ETA: 0s - loss: 0.1057\n","Epoch 49: val_loss did not improve from 1.36657\n","36/36 [==============================] - 4s 110ms/step - loss: 0.1057 - val_loss: 1.3699\n","Epoch 50/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0998\n","Epoch 50: val_loss did not improve from 1.36657\n","36/36 [==============================] - 4s 110ms/step - loss: 0.0998 - val_loss: 1.3789\n","Epoch 51/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0951\n","Epoch 51: val_loss improved from 1.36657 to 1.36604, saving model to my_nmt_model_min_loss.h5\n","36/36 [==============================] - 4s 111ms/step - loss: 0.0951 - val_loss: 1.3660\n","Epoch 52/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0904\n","Epoch 52: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 110ms/step - loss: 0.0904 - val_loss: 1.3716\n","Epoch 53/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0859\n","Epoch 53: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 109ms/step - loss: 0.0859 - val_loss: 1.3680\n","Epoch 54/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0821\n","Epoch 54: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 110ms/step - loss: 0.0821 - val_loss: 1.3805\n","Epoch 55/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0788\n","Epoch 55: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 116ms/step - loss: 0.0788 - val_loss: 1.3740\n","Epoch 56/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0757\n","Epoch 56: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 126ms/step - loss: 0.0757 - val_loss: 1.3773\n","Epoch 57/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 57: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 118ms/step - loss: 0.0727 - val_loss: 1.3793\n","Epoch 58/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0705\n","Epoch 58: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0705 - val_loss: 1.3826\n","Epoch 59/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0684\n","Epoch 59: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 140ms/step - loss: 0.0684 - val_loss: 1.3732\n","Epoch 60/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0655\n","Epoch 60: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 149ms/step - loss: 0.0655 - val_loss: 1.3731\n","Epoch 61/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0638\n","Epoch 61: val_loss did not improve from 1.36604\n","36/36 [==============================] - 6s 173ms/step - loss: 0.0638 - val_loss: 1.3800\n","Epoch 62/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0616\n","Epoch 62: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 150ms/step - loss: 0.0616 - val_loss: 1.3869\n","Epoch 63/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0602\n","Epoch 63: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 133ms/step - loss: 0.0602 - val_loss: 1.3829\n","Epoch 64/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 64: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 146ms/step - loss: 0.0577 - val_loss: 1.3830\n","Epoch 65/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0562\n","Epoch 65: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 141ms/step - loss: 0.0562 - val_loss: 1.3853\n","Epoch 66/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0550\n","Epoch 66: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 138ms/step - loss: 0.0550 - val_loss: 1.3859\n","Epoch 67/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0541\n","Epoch 67: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0541 - val_loss: 1.3929\n","Epoch 68/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 68: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 118ms/step - loss: 0.0530 - val_loss: 1.3899\n","Epoch 69/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 69: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 127ms/step - loss: 0.0519 - val_loss: 1.3982\n","Epoch 70/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0505\n","Epoch 70: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 128ms/step - loss: 0.0505 - val_loss: 1.3972\n","Epoch 71/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 71: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 129ms/step - loss: 0.0500 - val_loss: 1.4019\n","Epoch 72/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0488\n","Epoch 72: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 128ms/step - loss: 0.0488 - val_loss: 1.4033\n","Epoch 73/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0476\n","Epoch 73: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 125ms/step - loss: 0.0476 - val_loss: 1.4041\n","Epoch 74/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 74: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 129ms/step - loss: 0.0471 - val_loss: 1.4072\n","Epoch 75/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0457\n","Epoch 75: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 127ms/step - loss: 0.0457 - val_loss: 1.4051\n","Epoch 76/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 76: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 128ms/step - loss: 0.0458 - val_loss: 1.4127\n","Epoch 77/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 77: val_loss did not improve from 1.36604\n","36/36 [==============================] - 5s 126ms/step - loss: 0.0454 - val_loss: 1.4063\n","Epoch 78/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 78: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 117ms/step - loss: 0.0447 - val_loss: 1.4071\n","Epoch 79/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 79: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0435 - val_loss: 1.4128\n","Epoch 80/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 80: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0432 - val_loss: 1.4110\n","Epoch 81/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 81: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0425 - val_loss: 1.4080\n","Epoch 82/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 82: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0427 - val_loss: 1.4113\n","Epoch 83/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 83: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0411 - val_loss: 1.4140\n","Epoch 84/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 84: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0407 - val_loss: 1.4192\n","Epoch 85/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 85: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 113ms/step - loss: 0.0407 - val_loss: 1.4172\n","Epoch 86/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 86: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0398 - val_loss: 1.4223\n","Epoch 87/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 87: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0398 - val_loss: 1.4237\n","Epoch 88/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 88: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 113ms/step - loss: 0.0402 - val_loss: 1.4245\n","Epoch 89/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 89: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 113ms/step - loss: 0.0391 - val_loss: 1.4213\n","Epoch 90/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 90: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0390 - val_loss: 1.4318\n","Epoch 91/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 91: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0387 - val_loss: 1.4331\n","Epoch 92/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 92: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 113ms/step - loss: 0.0384 - val_loss: 1.4347\n","Epoch 93/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 93: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0379 - val_loss: 1.4406\n","Epoch 94/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 94: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0377 - val_loss: 1.4399\n","Epoch 95/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 95: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 112ms/step - loss: 0.0377 - val_loss: 1.4402\n","Epoch 96/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 96: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0368 - val_loss: 1.4355\n","Epoch 97/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 97: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0369 - val_loss: 1.4385\n","Epoch 98/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 98: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 114ms/step - loss: 0.0371 - val_loss: 1.4399\n","Epoch 99/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 99: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 113ms/step - loss: 0.0368 - val_loss: 1.4431\n","Epoch 100/100\n","36/36 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 100: val_loss did not improve from 1.36604\n","36/36 [==============================] - 4s 115ms/step - loss: 0.0367 - val_loss: 1.4369\n","mkdir: 10000: File exists\n","\u001b[34m1000\u001b[m\u001b[m                     \u001b[34m15000\u001b[m\u001b[m                    assignment.ipynb\n","\u001b[34m10000\u001b[m\u001b[m                    \u001b[34m5000\u001b[m\u001b[m                     my_nmt_model_min_loss.h5\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","mkdir: 10000/parameters/: File exists\n","mkdir: 10000/dictionaries/: File exists\n","Model  10000  saved successfully!\n","Training on\n"," hindi    15000\n","eng      15000\n","dtype: int64\n","English max length is 4\n","Decoder input data shape -> (15000, 4)\n","Number of English tokens = 4310\n","Decoder target data shape -> (15000, 4, 4310)\n","Epoch 1/100\n","54/54 [==============================] - ETA: 0s - loss: 6.0615\n","Epoch 1: val_loss improved from inf to 4.85312, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 9s 128ms/step - loss: 6.0615 - val_loss: 4.8531\n","Epoch 2/100\n","54/54 [==============================] - ETA: 0s - loss: 4.6064\n","Epoch 2: val_loss improved from 4.85312 to 4.53354, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 4.6064 - val_loss: 4.5335\n","Epoch 3/100\n","54/54 [==============================] - ETA: 0s - loss: 4.1889\n","Epoch 3: val_loss improved from 4.53354 to 4.10721, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 4.1889 - val_loss: 4.1072\n","Epoch 4/100\n","54/54 [==============================] - ETA: 0s - loss: 3.8396\n","Epoch 4: val_loss improved from 4.10721 to 3.84511, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 3.8396 - val_loss: 3.8451\n","Epoch 5/100\n","54/54 [==============================] - ETA: 0s - loss: 3.5865\n","Epoch 5: val_loss improved from 3.84511 to 3.62673, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 3.5865 - val_loss: 3.6267\n","Epoch 6/100\n","54/54 [==============================] - ETA: 0s - loss: 3.3423\n","Epoch 6: val_loss improved from 3.62673 to 3.40096, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 3.3423 - val_loss: 3.4010\n","Epoch 7/100\n","54/54 [==============================] - ETA: 0s - loss: 3.0847\n","Epoch 7: val_loss improved from 3.40096 to 3.17240, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 3.0847 - val_loss: 3.1724\n","Epoch 8/100\n","54/54 [==============================] - ETA: 0s - loss: 2.8188\n","Epoch 8: val_loss improved from 3.17240 to 2.93677, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 2.8188 - val_loss: 2.9368\n","Epoch 9/100\n","54/54 [==============================] - ETA: 0s - loss: 2.5527\n","Epoch 9: val_loss improved from 2.93677 to 2.70478, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 2.5527 - val_loss: 2.7048\n","Epoch 10/100\n","54/54 [==============================] - ETA: 0s - loss: 2.2937\n","Epoch 10: val_loss improved from 2.70478 to 2.48858, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 124ms/step - loss: 2.2937 - val_loss: 2.4886\n","Epoch 11/100\n","54/54 [==============================] - ETA: 0s - loss: 2.0508\n","Epoch 11: val_loss improved from 2.48858 to 2.29139, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 124ms/step - loss: 2.0508 - val_loss: 2.2914\n","Epoch 12/100\n","54/54 [==============================] - ETA: 0s - loss: 1.8265\n","Epoch 12: val_loss improved from 2.29139 to 2.11515, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 1.8265 - val_loss: 2.1151\n","Epoch 13/100\n","54/54 [==============================] - ETA: 0s - loss: 1.6220\n","Epoch 13: val_loss improved from 2.11515 to 1.95700, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 124ms/step - loss: 1.6220 - val_loss: 1.9570\n","Epoch 14/100\n","54/54 [==============================] - ETA: 0s - loss: 1.4395\n","Epoch 14: val_loss improved from 1.95700 to 1.82309, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 1.4395 - val_loss: 1.8231\n","Epoch 15/100\n","54/54 [==============================] - ETA: 0s - loss: 1.2733\n","Epoch 15: val_loss improved from 1.82309 to 1.70538, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 130ms/step - loss: 1.2733 - val_loss: 1.7054\n","Epoch 16/100\n","54/54 [==============================] - ETA: 0s - loss: 1.1280\n","Epoch 16: val_loss improved from 1.70538 to 1.60041, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 128ms/step - loss: 1.1280 - val_loss: 1.6004\n","Epoch 17/100\n","54/54 [==============================] - ETA: 0s - loss: 0.9933\n","Epoch 17: val_loss improved from 1.60041 to 1.50959, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.9933 - val_loss: 1.5096\n","Epoch 18/100\n","54/54 [==============================] - ETA: 0s - loss: 0.8763\n","Epoch 18: val_loss improved from 1.50959 to 1.42713, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.8763 - val_loss: 1.4271\n","Epoch 19/100\n","54/54 [==============================] - ETA: 0s - loss: 0.7707\n","Epoch 19: val_loss improved from 1.42713 to 1.35555, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 128ms/step - loss: 0.7707 - val_loss: 1.3555\n","Epoch 20/100\n","54/54 [==============================] - ETA: 0s - loss: 0.6798\n","Epoch 20: val_loss improved from 1.35555 to 1.29647, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.6798 - val_loss: 1.2965\n","Epoch 21/100\n","54/54 [==============================] - ETA: 0s - loss: 0.5984\n","Epoch 21: val_loss improved from 1.29647 to 1.24339, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.5984 - val_loss: 1.2434\n","Epoch 22/100\n","54/54 [==============================] - ETA: 0s - loss: 0.5281\n","Epoch 22: val_loss improved from 1.24339 to 1.19856, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.5281 - val_loss: 1.1986\n","Epoch 23/100\n","54/54 [==============================] - ETA: 0s - loss: 0.4674\n","Epoch 23: val_loss improved from 1.19856 to 1.15434, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.4674 - val_loss: 1.1543\n","Epoch 24/100\n","54/54 [==============================] - ETA: 0s - loss: 0.4143\n","Epoch 24: val_loss improved from 1.15434 to 1.11793, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.4143 - val_loss: 1.1179\n","Epoch 25/100\n","54/54 [==============================] - ETA: 0s - loss: 0.3686\n","Epoch 25: val_loss improved from 1.11793 to 1.09186, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.3686 - val_loss: 1.0919\n","Epoch 26/100\n","54/54 [==============================] - ETA: 0s - loss: 0.3285\n","Epoch 26: val_loss improved from 1.09186 to 1.06901, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.3285 - val_loss: 1.0690\n","Epoch 27/100\n","54/54 [==============================] - ETA: 0s - loss: 0.2949\n","Epoch 27: val_loss improved from 1.06901 to 1.05358, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.2949 - val_loss: 1.0536\n","Epoch 28/100\n","54/54 [==============================] - ETA: 0s - loss: 0.2648\n","Epoch 28: val_loss improved from 1.05358 to 1.03276, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 124ms/step - loss: 0.2648 - val_loss: 1.0328\n","Epoch 29/100\n","54/54 [==============================] - ETA: 0s - loss: 0.2400\n","Epoch 29: val_loss improved from 1.03276 to 1.02072, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.2400 - val_loss: 1.0207\n","Epoch 30/100\n","54/54 [==============================] - ETA: 0s - loss: 0.2176\n","Epoch 30: val_loss improved from 1.02072 to 1.01310, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.2176 - val_loss: 1.0131\n","Epoch 31/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1987\n","Epoch 31: val_loss improved from 1.01310 to 0.99921, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.1987 - val_loss: 0.9992\n","Epoch 32/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1822\n","Epoch 32: val_loss improved from 0.99921 to 0.99529, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.1822 - val_loss: 0.9953\n","Epoch 33/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1668\n","Epoch 33: val_loss improved from 0.99529 to 0.98938, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 126ms/step - loss: 0.1668 - val_loss: 0.9894\n","Epoch 34/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1540\n","Epoch 34: val_loss improved from 0.98938 to 0.98088, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.1540 - val_loss: 0.9809\n","Epoch 35/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1435\n","Epoch 35: val_loss improved from 0.98088 to 0.97721, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.1435 - val_loss: 0.9772\n","Epoch 36/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1337\n","Epoch 36: val_loss improved from 0.97721 to 0.97151, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.1337 - val_loss: 0.9715\n","Epoch 37/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1244\n","Epoch 37: val_loss did not improve from 0.97151\n","54/54 [==============================] - 7s 124ms/step - loss: 0.1244 - val_loss: 0.9720\n","Epoch 38/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1170\n","Epoch 38: val_loss improved from 0.97151 to 0.96955, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 127ms/step - loss: 0.1170 - val_loss: 0.9695\n","Epoch 39/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1096\n","Epoch 39: val_loss did not improve from 0.96955\n","54/54 [==============================] - 7s 125ms/step - loss: 0.1096 - val_loss: 0.9744\n","Epoch 40/100\n","54/54 [==============================] - ETA: 0s - loss: 0.1042\n","Epoch 40: val_loss did not improve from 0.96955\n","54/54 [==============================] - 7s 123ms/step - loss: 0.1042 - val_loss: 0.9726\n","Epoch 41/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0981\n","Epoch 41: val_loss did not improve from 0.96955\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0981 - val_loss: 0.9725\n","Epoch 42/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0928\n","Epoch 42: val_loss did not improve from 0.96955\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0928 - val_loss: 0.9718\n","Epoch 43/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0885\n","Epoch 43: val_loss did not improve from 0.96955\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0885 - val_loss: 0.9718\n","Epoch 44/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0846\n","Epoch 44: val_loss did not improve from 0.96955\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0846 - val_loss: 0.9733\n","Epoch 45/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0813\n","Epoch 45: val_loss improved from 0.96955 to 0.96817, saving model to my_nmt_model_min_loss.h5\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0813 - val_loss: 0.9682\n","Epoch 46/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0781\n","Epoch 46: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0781 - val_loss: 0.9704\n","Epoch 47/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0743\n","Epoch 47: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0743 - val_loss: 0.9750\n","Epoch 48/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0719\n","Epoch 48: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0719 - val_loss: 0.9721\n","Epoch 49/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0696\n","Epoch 49: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0696 - val_loss: 0.9783\n","Epoch 50/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0671\n","Epoch 50: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0671 - val_loss: 0.9772\n","Epoch 51/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0651\n","Epoch 51: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0651 - val_loss: 0.9718\n","Epoch 52/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0638\n","Epoch 52: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0638 - val_loss: 0.9766\n","Epoch 53/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0624\n","Epoch 53: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0624 - val_loss: 0.9803\n","Epoch 54/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0601\n","Epoch 54: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0601 - val_loss: 0.9888\n","Epoch 55/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0590\n","Epoch 55: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0590 - val_loss: 0.9864\n","Epoch 56/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0578\n","Epoch 56: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0578 - val_loss: 0.9862\n","Epoch 57/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0572\n","Epoch 57: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0572 - val_loss: 0.9878\n","Epoch 58/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0556\n","Epoch 58: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0556 - val_loss: 0.9880\n","Epoch 59/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 59: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0547 - val_loss: 0.9856\n","Epoch 60/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 60: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0543 - val_loss: 0.9849\n","Epoch 61/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0535\n","Epoch 61: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0535 - val_loss: 0.9889\n","Epoch 62/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 62: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0519 - val_loss: 0.9944\n","Epoch 63/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 63: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0514 - val_loss: 0.9915\n","Epoch 64/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 64: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0514 - val_loss: 0.9890\n","Epoch 65/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0505\n","Epoch 65: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0505 - val_loss: 0.9931\n","Epoch 66/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 66: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0497 - val_loss: 0.9964\n","Epoch 67/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0496\n","Epoch 67: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0496 - val_loss: 0.9934\n","Epoch 68/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0491\n","Epoch 68: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0491 - val_loss: 0.9926\n","Epoch 69/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0485\n","Epoch 69: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0485 - val_loss: 0.9971\n","Epoch 70/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0478\n","Epoch 70: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0478 - val_loss: 0.9922\n","Epoch 71/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0482\n","Epoch 71: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0482 - val_loss: 0.9987\n","Epoch 72/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 72: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0471 - val_loss: 0.9961\n","Epoch 73/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0472\n","Epoch 73: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0472 - val_loss: 1.0038\n","Epoch 74/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0465\n","Epoch 74: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0465 - val_loss: 0.9954\n","Epoch 75/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 75: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0462 - val_loss: 1.0002\n","Epoch 76/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0461\n","Epoch 76: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0461 - val_loss: 1.0029\n","Epoch 77/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0456\n","Epoch 77: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0456 - val_loss: 1.0048\n","Epoch 78/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 78: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0455 - val_loss: 1.0075\n","Epoch 79/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 79: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 129ms/step - loss: 0.0452 - val_loss: 0.9991\n","Epoch 80/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 80: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 128ms/step - loss: 0.0444 - val_loss: 1.0007\n","Epoch 81/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 81: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 126ms/step - loss: 0.0446 - val_loss: 1.0044\n","Epoch 82/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 82: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0444 - val_loss: 1.0043\n","Epoch 83/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0441\n","Epoch 83: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 128ms/step - loss: 0.0441 - val_loss: 1.0068\n","Epoch 84/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0439\n","Epoch 84: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0439 - val_loss: 1.0126\n","Epoch 85/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 85: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0437 - val_loss: 1.0074\n","Epoch 86/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 86: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0436 - val_loss: 1.0103\n","Epoch 87/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 87: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0435 - val_loss: 1.0083\n","Epoch 88/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 88: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0428 - val_loss: 1.0076\n","Epoch 89/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 89: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0429 - val_loss: 1.0182\n","Epoch 90/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0431\n","Epoch 90: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0431 - val_loss: 1.0066\n","Epoch 91/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 91: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0428 - val_loss: 1.0176\n","Epoch 92/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 92: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 123ms/step - loss: 0.0424 - val_loss: 1.0180\n","Epoch 93/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 93: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0428 - val_loss: 1.0142\n","Epoch 94/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 94: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0421 - val_loss: 1.0147\n","Epoch 95/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 95: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 126ms/step - loss: 0.0420 - val_loss: 1.0171\n","Epoch 96/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 96: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0421 - val_loss: 1.0224\n","Epoch 97/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 97: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0422 - val_loss: 1.0206\n","Epoch 98/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 98: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0421 - val_loss: 1.0276\n","Epoch 99/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 99: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 125ms/step - loss: 0.0413 - val_loss: 1.0222\n","Epoch 100/100\n","54/54 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 100: val_loss did not improve from 0.96817\n","54/54 [==============================] - 7s 124ms/step - loss: 0.0417 - val_loss: 1.0203\n","mkdir: 15000: File exists\n","\u001b[34m1000\u001b[m\u001b[m                     \u001b[34m15000\u001b[m\u001b[m                    assignment.ipynb\n","\u001b[34m10000\u001b[m\u001b[m                    \u001b[34m5000\u001b[m\u001b[m                     my_nmt_model_min_loss.h5\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","mkdir: 15000/parameters/: File exists\n","mkdir: 15000/dictionaries/: File exists\n","Model  15000  saved successfully!\n"]}],"source":["for item in model_size:\n","    lines= pool_oftexts.sample(n = item)\n","    print(\"Training on\\n\", lines.count() )\n","   \n","    max_input_length, num_hindi_tokens, encoder_input_data, input_dict= createInputDataForEncoder(lines=lines)\n","    max_output_length, num_eng_tokens, decoder_input_data, eng_word_dict, tokenized_eng_lines =createInputDataForDecoder(lines=lines)\n","    decoder_target_data= createDecoderTargetData(tokenized_eng_lines, max_output_length, num_eng_tokens)\n","    model, encoder_model , decoder_model= createEncoderDecoderModel(max_input_length, max_output_length, num_hindi_tokens, num_eng_tokens)\n","    mc = tf.keras.callbacks.ModelCheckpoint('my_nmt_model_min_loss.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","    history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250,  epochs=100  ,validation_split = 0.1, callbacks=[mc], verbose=1    ) \n","    my_list_of_trained_models.append(model)\n","    save_models_and_parameters(lines,model,encoder_model , decoder_model, max_input_length,num_hindi_tokens,input_dict,max_output_length, num_eng_tokens,eng_word_dict)\n","\n","\n","## Think what should be done with variable history\n","      "]},{"cell_type":"markdown","metadata":{},"source":["### Get Test Sentences"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hindi</th>\n","      <th>eng</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2495</th>\n","      <td>          ...</td>\n","      <td>numerous spectacular arrangements were made to...</td>\n","    </tr>\n","    <tr>\n","      <th>2496</th>\n","      <td>         ...</td>\n","      <td>jodhpur city was selected for the spectacular ...</td>\n","    </tr>\n","    <tr>\n","      <th>2497</th>\n","      <td>    11    ...</td>\n","      <td>balsamand lake about 11 kilometres from the fa...</td>\n","    </tr>\n","    <tr>\n","      <th>2498</th>\n","      <td>       ...</td>\n","      <td>all the guests will join the lakshmi puja for ...</td>\n","    </tr>\n","    <tr>\n","      <th>2499</th>\n","      <td>        </td>\n","      <td>a dance program has also been organised after ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  hindi  \\\n","2495            ...   \n","2496           ...   \n","2497      11    ...   \n","2498         ...   \n","2499                     \n","\n","                                                    eng  \n","2495  numerous spectacular arrangements were made to...  \n","2496  jodhpur city was selected for the spectacular ...  \n","2497  balsamand lake about 11 kilometres from the fa...  \n","2498  all the guests will join the lakshmi puja for ...  \n","2499  a dance program has also been organised after ...  "]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["data_size=2500\n","lines=createDataset(dataset=dataset,data_size=data_size, type=\"test\")\n","lines.tail()"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pydot in /Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages (1.4.2)\n","Requirement already satisfied: graphviz in /Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages (0.20.1)\n","Requirement already satisfied: pyparsing>=2.1.4 in /Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages (from pydot) (3.0.9)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pydot graphviz"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["def to_string(number):\n","  return str(number) \n","\n","string_numbers_iterator = map(to_string, model_size)\n","model_list = list(string_numbers_iterator)\n","path='./'\n","\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n","Model: \"model_24\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_29 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_30 (InputLayer)          [(None, 4)]          0           []                               \n","                                                                                                  \n"," embedding_12 (Embedding)       (None, 2, 256)       425216      ['input_29[0][0]']               \n","                                                                                                  \n"," embedding_13 (Embedding)       (None, 4, 256)       361472      ['input_30[0][0]']               \n","                                                                                                  \n"," lstm_12 (LSTM)                 [(None, 256),        525312      ['embedding_12[0][0]']           \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_13 (LSTM)                 [(None, 4, 256),     525312      ['embedding_13[0][0]',           \n","                                 (None, 256),                     'lstm_12[0][1]',                \n","                                 (None, 256)]                     'lstm_12[0][2]']                \n","                                                                                                  \n"," dense_8 (Dense)                (None, 4, 1412)      362884      ['lstm_13[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 2,200,196\n","Trainable params: 2,200,196\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 2, 'num_encoder_tokens': 1661}\n","{'max_decoder_seq_length': 4, 'num_decoder_tokens': 1412}\n","WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='input_45'), name='input_45', description=\"created by layer 'input_45'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stderr","output_type":"stream","text":["/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["['1000', 0.00031373271, 0.0018, 0.00017630175, 0.0033456900322988404, 6.6163196945638895e-06]\n","You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n","Model: \"model_27\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_33 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_34 (InputLayer)          [(None, 4)]          0           []                               \n","                                                                                                  \n"," embedding_14 (Embedding)       (None, 2, 256)       951552      ['input_33[0][0]']               \n","                                                                                                  \n"," embedding_15 (Embedding)       (None, 4, 256)       753152      ['input_34[0][0]']               \n","                                                                                                  \n"," lstm_14 (LSTM)                 [(None, 256),        525312      ['embedding_14[0][0]']           \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_15 (LSTM)                 [(None, 4, 256),     525312      ['embedding_15[0][0]',           \n","                                 (None, 256),                     'lstm_14[0][1]',                \n","                                 (None, 256)]                     'lstm_14[0][2]']                \n","                                                                                                  \n"," dense_9 (Dense)                (None, 4, 2942)      756094      ['lstm_15[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,511,422\n","Trainable params: 3,511,422\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 2, 'num_encoder_tokens': 3717}\n","{'max_decoder_seq_length': 4, 'num_decoder_tokens': 2942}\n","WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='input_48'), name='input_48', description=\"created by layer 'input_48'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stderr","output_type":"stream","text":["/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["['5000', 0.00015525662, 0.0008, 8.747926e-05, 0.0007566595876638358, 2.3329645653125815e-06]\n","You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n","Model: \"model_30\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_37 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_38 (InputLayer)          [(None, 4)]          0           []                               \n","                                                                                                  \n"," embedding_16 (Embedding)       (None, 2, 256)       1277952     ['input_37[0][0]']               \n","                                                                                                  \n"," embedding_17 (Embedding)       (None, 4, 256)       973824      ['input_38[0][0]']               \n","                                                                                                  \n"," lstm_16 (LSTM)                 [(None, 256),        525312      ['embedding_16[0][0]']           \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_17 (LSTM)                 [(None, 4, 256),     525312      ['embedding_17[0][0]',           \n","                                 (None, 256),                     'lstm_16[0][1]',                \n","                                 (None, 256)]                     'lstm_16[0][2]']                \n","                                                                                                  \n"," dense_10 (Dense)               (None, 4, 3804)      977628      ['lstm_17[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 4,280,028\n","Trainable params: 4,280,028\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 2, 'num_encoder_tokens': 4992}\n","{'max_decoder_seq_length': 4, 'num_decoder_tokens': 3804}\n","WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='input_51'), name='input_51', description=\"created by layer 'input_51'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stderr","output_type":"stream","text":["/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["['10000', 2e-05, 0.0002, 1.0526316e-05, 0.0002606760418889726, 3.0459959489425258e-12]\n","You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n","Model: \"model_33\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_41 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_42 (InputLayer)          [(None, 4)]          0           []                               \n","                                                                                                  \n"," embedding_18 (Embedding)       (None, 2, 256)       1471488     ['input_41[0][0]']               \n","                                                                                                  \n"," embedding_19 (Embedding)       (None, 4, 256)       1103360     ['input_42[0][0]']               \n","                                                                                                  \n"," lstm_18 (LSTM)                 [(None, 256),        525312      ['embedding_18[0][0]']           \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_19 (LSTM)                 [(None, 4, 256),     525312      ['embedding_19[0][0]',           \n","                                 (None, 256),                     'lstm_18[0][1]',                \n","                                 (None, 256)]                     'lstm_18[0][2]']                \n","                                                                                                  \n"," dense_11 (Dense)               (None, 4, 4310)      1107670     ['lstm_19[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 4,733,142\n","Trainable params: 4,733,142\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 2, 'num_encoder_tokens': 5748}\n","{'max_decoder_seq_length': 4, 'num_decoder_tokens': 4310}\n","WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='input_54'), name='input_54', description=\"created by layer 'input_54'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stderr","output_type":"stream","text":["/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/Users/learn/Desktop/Projects/machine-translation/.env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["['15000', 2.7586208e-05, 0.0002, 1.4814815e-05, 5.455954715763788e-05, 7.453306344157342e-10]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dataset Size</th>\n","      <th>average_f_measure</th>\n","      <th>average_p_measure</th>\n","      <th>average_r_measure</th>\n","      <th>average_cosine</th>\n","      <th>average_bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000</td>\n","      <td>0.000314</td>\n","      <td>0.0018</td>\n","      <td>0.000176</td>\n","      <td>0.003346</td>\n","      <td>6.616320e-06</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5000</td>\n","      <td>0.000155</td>\n","      <td>0.0008</td>\n","      <td>0.000087</td>\n","      <td>0.000757</td>\n","      <td>2.332965e-06</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10000</td>\n","      <td>0.000020</td>\n","      <td>0.0002</td>\n","      <td>0.000011</td>\n","      <td>0.000261</td>\n","      <td>3.045996e-12</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15000</td>\n","      <td>0.000028</td>\n","      <td>0.0002</td>\n","      <td>0.000015</td>\n","      <td>0.000055</td>\n","      <td>7.453306e-10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Dataset Size  average_f_measure  average_p_measure  average_r_measure  \\\n","0         1000           0.000314             0.0018           0.000176   \n","1         5000           0.000155             0.0008           0.000087   \n","2        10000           0.000020             0.0002           0.000011   \n","3        15000           0.000028             0.0002           0.000015   \n","\n","   average_cosine  average_bleu  \n","0        0.003346  6.616320e-06  \n","1        0.000757  2.332965e-06  \n","2        0.000261  3.045996e-12  \n","3        0.000055  7.453306e-10  "]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["\n","stat=[]\n","for item in model_list:\n","\n","    model_path= path+item+\"/model.h5\" \n","    path_encoder_parameters= path+item+\"/parameters/encoder_parameters.pickle\" \n","    path_encoder_dictionary= path+item+\"/dictionaries/encoder_dictionary.pickle\" \n","    path_decoder_parameters= path+item+\"/parameters/decoder_parameters.pickle\" \n","    path_decoder_dictionary= path+item+\"/dictionaries/decoder_dictionary.pickle\" \n","    encoderPath= path+item+\"/enc_model.h5\" \n","    decoderPath= path+item+\"/dec_model.h5\" \n","    # print(model_path, path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary,encoderPath,decoderPath)\n","    result= get_model_statistics_summary(model_path, path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary,encoderPath,decoderPath, lines)\n","    result.insert(0, item)\n","    stat.append(result)\n","    print(result)\n","\n","table =pd.DataFrame(columns=[\"Dataset Size\",\"average_f_measure\", \"average_p_measure\",\"average_r_measure\", \"average_cosine\" ,\"average_bleu\"], data=stat)\n","table"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizations"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1500x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plot\n","from matplotlib.ticker import ScalarFormatter\n","#,figsize=(10,15)\n","\n","table.plot.bar(x=\"Dataset Size\",figsize=(15,10))\n","plot.show(block=True)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["table.plot.bar(x=\"Dataset Size\",y=\"average_bleu\")\n","plot.yscale(\"log\")\n","plot.gca().yaxis.set_major_formatter(ScalarFormatter())\n","\n","plot.show(block=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Save Performance Metrics"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","now = datetime.now()\n","dt_string = now.strftime(\"%d-%m-%Y-%H:%M\")\n","path= '../analytics/'+dt_string+\".json\"\n","table.to_json(r''+path,orient='records')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Read Performance Metrics"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1500x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["read_json=pd.read_json(path)\n","\n","read_json.plot.bar(x=\"Dataset Size\",figsize=(15,10))\n","plot.show(block=True)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPbuzsP8tcdD86ug5Jj9rQt","collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"YE","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"231fa661f6864ef19e499bbd2782d03ff99f363f5ebfe416ba9cf7c065dc127b"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"07296beee2fd400f84e2caf6b9ef76f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09f98484eada471d8becbb8762e78b94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a931a1173e344bdb0e8c2104da3ea91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c068c2a397441dcbddc167e162a80bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d9a54d5022f41d8abdacc313b747b0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d24014eea31c4d22837f828e5b4273b9","placeholder":"","style":"IPY_MODEL_507a4422cef64a8f85594bf5d2d93f24","value":"100%"}},"0df862ceb9ca474a8021be540338fe68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f0cfe769ed24dd8bdc7fc9d547b3b3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88131cdcfe1f49c68cf0443e6c4fb779","placeholder":"","style":"IPY_MODEL_246d1267e25644489feccf71a9cb5db7","value":"Downloading: 100%"}},"0faf97f237604bbfa5c600f213bd3b41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13134d521d5943b7b44609349f8eda1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0425a1417284aca96aee56b5d168ed4","placeholder":"","style":"IPY_MODEL_f6f0be530e9a4977b4f455f80ab31f3b","value":" 500k/500k [00:00&lt;00:00, 973kB/s]"}},"14ad4c2521a6428186b51aa5e136a229":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fe801b9c65b4782ab0787ab3d2fde9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"209c7549565348d4afc1450244ee8eba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4da39e8e714f5483792d7797c296a0","placeholder":"","style":"IPY_MODEL_0a931a1173e344bdb0e8c2104da3ea91","value":"Downloading: 100%"}},"246d1267e25644489feccf71a9cb5db7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2aaa63e9d4054c21b80bec29494869cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_959645e85ec84da3a3c4b90db77e1a14","placeholder":"","style":"IPY_MODEL_3e50f291f18949c299ac294cd1083401","value":" 953/953 [00:00&lt;00:00, 26.3kB/s]"}},"2c18fe3687324ff4a7049fe30877245f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3167a0128d9b4c54aff20b0064190028":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f0cfe769ed24dd8bdc7fc9d547b3b3a","IPY_MODEL_89156de1426748f2b05f483ed8c945d3","IPY_MODEL_13134d521d5943b7b44609349f8eda1d"],"layout":"IPY_MODEL_b0f33ab398fd4a80a24708846d8eaec8"}},"3422b361c8cc4b65a80f56504fbf0f88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6e3ef66b71b4bba93f52596004d46b5","placeholder":"","style":"IPY_MODEL_fb3c03d70ea446379afacb94ba65a369","value":" 85.7k/85.7k [00:00&lt;00:00, 289kB/s]"}},"3e50f291f18949c299ac294cd1083401":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49f27321705944a9928923b77f323101":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff9cb1acf1bd4648a5571fc796029bfb","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_537c0dea0c6340e798304e723e7e767c","value":3}},"4f613b00507e4372b52f9b969fffaa6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f109d2a5da5466f839dd0ebfb415e59","placeholder":"","style":"IPY_MODEL_ea25012e855042d3902dfbf8757ed9fd","value":" 3/3 [00:00&lt;00:00,  4.83it/s]"}},"507a4422cef64a8f85594bf5d2d93f24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52eee3e0afaa4d1bbaf4ab294f5ef3a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"537c0dea0c6340e798304e723e7e767c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5416bae8c41f4f679ac711715788c5ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6a4a7f321a741eeb6dbc2ddaa9fdef9","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ab3244d0e094c26ba163044d0c00217","value":3}},"57b7ce7cf6e441789904fb72147f781a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d9a54d5022f41d8abdacc313b747b0a","IPY_MODEL_5416bae8c41f4f679ac711715788c5ae","IPY_MODEL_654c8185fa034eb9833be0fb39ce50d8"],"layout":"IPY_MODEL_d37ce324cfe2434c9b90fcf14c796b2b"}},"5c9d357f9cc94c3b83034a7f35177053":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6195741e9ebb457fba44faf0def130b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"654c8185fa034eb9833be0fb39ce50d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0e1940636b04bbeb92dffea98d8f48b","placeholder":"","style":"IPY_MODEL_f8031b4a67e0489dacf475a2a7a87cd3","value":" 3/3 [00:00&lt;00:00, 61.38it/s]"}},"6770f58b9f5e4aaba0a8f664ab54ad78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09f98484eada471d8becbb8762e78b94","placeholder":"","style":"IPY_MODEL_f3243e6908fc482e87a2533a10be2806","value":"Downloading: 100%"}},"6b8b066d84f8436ca162ca4f6e8798b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b28a5f08a7b42fab29ac15593a8ff34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f109d2a5da5466f839dd0ebfb415e59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8067f022562a41ccb588a337e00cf749":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db190de188ac4634952618b8382df373","placeholder":"","style":"IPY_MODEL_bd03148515ac496193433505e5423a96","value":"Downloading: 100%"}},"80eac01a97704940b4ce08138821d028":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2b2afa325334fd680ffb134d50b7958","IPY_MODEL_49f27321705944a9928923b77f323101","IPY_MODEL_4f613b00507e4372b52f9b969fffaa6a"],"layout":"IPY_MODEL_14ad4c2521a6428186b51aa5e136a229"}},"86c2f35a748e4ab1aad4a90e86104717":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb53c3cfa24748c9a2c688c37058f86e","max":189602204,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c830ef6b0bbd43d795f09af169050e57","value":189602204}},"878363153d6f4a31b1f5120740e73d40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88131cdcfe1f49c68cf0443e6c4fb779":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89156de1426748f2b05f483ed8c945d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef74935abbb54fad915c1411b7754f10","max":500080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fff895ab62d24e8cb966311d49f80d2a","value":500080}},"9399968906354864a5cd7e5fec0ae091":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"959645e85ec84da3a3c4b90db77e1a14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99885539e2f24c778af563523077bf8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6770f58b9f5e4aaba0a8f664ab54ad78","IPY_MODEL_e14e654788e440608067f614985e6844","IPY_MODEL_2aaa63e9d4054c21b80bec29494869cb"],"layout":"IPY_MODEL_6195741e9ebb457fba44faf0def130b5"}},"9ab3244d0e094c26ba163044d0c00217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c50be6a41c24acaad24ad84a22f7f4d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e4da39e8e714f5483792d7797c296a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0e1940636b04bbeb92dffea98d8f48b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aac6a6f4e1a240208964d9c2ea4a6eaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab42fd299ec647c091ef6824335e2aea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0425a1417284aca96aee56b5d168ed4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0f33ab398fd4a80a24708846d8eaec8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4fcddefd2294eca93346c37a95ad900":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52eee3e0afaa4d1bbaf4ab294f5ef3a7","max":85707,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0df862ceb9ca474a8021be540338fe68","value":85707}},"b6a4a7f321a741eeb6dbc2ddaa9fdef9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b810f4d841ff48c39e6a6fbbe5da41cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_209c7549565348d4afc1450244ee8eba","IPY_MODEL_86c2f35a748e4ab1aad4a90e86104717","IPY_MODEL_e0a1bfce7fe94873b8583928e0cf31fd"],"layout":"IPY_MODEL_5c9d357f9cc94c3b83034a7f35177053"}},"bb53c3cfa24748c9a2c688c37058f86e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd03148515ac496193433505e5423a96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4b5c3feb28e4cdfb102db6e2b269d7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c56b4a6df2f749ac8e4cfef3ea938e17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9399968906354864a5cd7e5fec0ae091","placeholder":"","style":"IPY_MODEL_2c18fe3687324ff4a7049fe30877245f","value":" 3/3 [00:10&lt;00:00,  2.97s/it]"}},"c830ef6b0bbd43d795f09af169050e57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d24014eea31c4d22837f828e5b4273b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2a2a995c34b4351b2752e092e811296":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_07296beee2fd400f84e2caf6b9ef76f4","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4b5c3feb28e4cdfb102db6e2b269d7c","value":3}},"d37ce324cfe2434c9b90fcf14c796b2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4c7b516a71b495891d18b2e3c91bdb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8067f022562a41ccb588a337e00cf749","IPY_MODEL_b4fcddefd2294eca93346c37a95ad900","IPY_MODEL_3422b361c8cc4b65a80f56504fbf0f88"],"layout":"IPY_MODEL_878363153d6f4a31b1f5120740e73d40"}},"d6e3ef66b71b4bba93f52596004d46b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da397a9c72064840872f0e0db6191a82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db190de188ac4634952618b8382df373":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0a1bfce7fe94873b8583928e0cf31fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b28a5f08a7b42fab29ac15593a8ff34","placeholder":"","style":"IPY_MODEL_ab42fd299ec647c091ef6824335e2aea","value":" 190M/190M [00:05&lt;00:00, 44.1MB/s]"}},"e14e654788e440608067f614985e6844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c50be6a41c24acaad24ad84a22f7f4d","max":953,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aac6a6f4e1a240208964d9c2ea4a6eaf","value":953}},"e91669941017427f931fb176ba81ff07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb169a2650694040879b06ae96f898bf","IPY_MODEL_d2a2a995c34b4351b2752e092e811296","IPY_MODEL_c56b4a6df2f749ac8e4cfef3ea938e17"],"layout":"IPY_MODEL_0c068c2a397441dcbddc167e162a80bc"}},"ea25012e855042d3902dfbf8757ed9fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb169a2650694040879b06ae96f898bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da397a9c72064840872f0e0db6191a82","placeholder":"","style":"IPY_MODEL_1fe801b9c65b4782ab0787ab3d2fde9d","value":"100%"}},"ef74935abbb54fad915c1411b7754f10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2b2afa325334fd680ffb134d50b7958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0faf97f237604bbfa5c600f213bd3b41","placeholder":"","style":"IPY_MODEL_6b8b066d84f8436ca162ca4f6e8798b4","value":"100%"}},"f3243e6908fc482e87a2533a10be2806":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6f0be530e9a4977b4f455f80ab31f3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8031b4a67e0489dacf475a2a7a87cd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb3c03d70ea446379afacb94ba65a369":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff9cb1acf1bd4648a5571fc796029bfb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff895ab62d24e8cb966311d49f80d2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
