{"cells":[{"cell_type":"markdown","metadata":{"id":"B453NNRC0joS"},"source":["# Load Model From Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21883,"status":"ok","timestamp":1665326442725,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"jvdaYoDtgQMT","outputId":"22a7c2ae-7d3e-4ed4-c89c-33ee602c20cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Initialize Paths of Models and Parameters"],"metadata":{"id":"5zoz2cQPKZtf"}},{"cell_type":"code","source":["path= \"/content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/\"\n","\n","model_list=[\"2908\",\"1000\"]"],"metadata":{"id":"MFKnZu9fKhqQ","executionInfo":{"status":"ok","timestamp":1665326442726,"user_tz":-360,"elapsed":8,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Install and Import Modules"],"metadata":{"id":"fLGZBCvrJC6X"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"_Fo-82DfGVBw","executionInfo":{"status":"ok","timestamp":1665326504381,"user_tz":-360,"elapsed":61661,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3924684a-2d33-4afc-f828-2d2d3deb0990"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 5.1 MB/s \n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 3.1 MB/s \n","\u001b[?25hCollecting pybind11>=2.2\n","  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3165333 sha256=2a6feb8d43874fcdc7d52d4c6682950a5fbcd14148647820d6977a531559e16a\n","  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","import pickle\n","from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","import pandas as pd\n","\n","!pip install -q \"tensorflow-text==2.8.*\"\n","import tensorflow as tf\n","import tensorflow_text as text\n","\n","!pip install fasttext\n","import fasttext.util"]},{"cell_type":"markdown","metadata":{"id":"PDoZgXN3uQJn"},"source":["# Download Sentences for Test"]},{"cell_type":"code","source":["!wget http://www.manythings.org/anki/hin-eng.zip -O hin-eng.zip\n","!unzip hin-eng.zip"],"metadata":{"id":"O8PEeQ4LY6Xg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665326505333,"user_tz":-360,"elapsed":963,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"e22e62f1-6321-4ff9-d80c-81417b5d935e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-09 14:41:43--  http://www.manythings.org/anki/hin-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 131711 (129K) [application/zip]\n","Saving to: ‘hin-eng.zip’\n","\n","hin-eng.zip         100%[===================>] 128.62K  --.-KB/s    in 0.06s   \n","\n","2022-10-09 14:41:44 (2.20 MB/s) - ‘hin-eng.zip’ saved [131711/131711]\n","\n","Archive:  hin-eng.zip\n","  inflating: hin.txt                 \n","  inflating: _about.txt              \n"]}]},{"cell_type":"markdown","source":["# Def. Get Summary Statistics for every model "],"metadata":{"id":"sAM8mvxHJPj3"}},{"cell_type":"code","source":["def get_model_statistics_summary(model_path,path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary, encoderPath, decoderPath ):\n","\n"," \n","    reconstructed_model = keras.models.load_model(model_path)\n","    plot_model(reconstructed_model, to_file='modelsummary.png', show_shapes=True, show_layer_names=True)\n","    reconstructed_model.summary()\n","\n","\n","    ## Load Dictionaries and Parameters \n","    path_encoder_parameters= path_encoder_parameters\n","    path_encoder_dictionary= path_encoder_dictionary\n","    path_decoder_parameters= path_decoder_parameters\n","    path_decoder_dictionary= path_decoder_dictionary\n","\n","    # loading\n","    with open(path_encoder_parameters, 'rb') as handle:\n","        encoder_parameters = pickle.load(handle)\n","\n","    # loading\n","    with open(path_encoder_dictionary, 'rb') as handle:\n","        encoder_dictionary = pickle.load(handle)\n","\n","    # loading\n","    with open(path_decoder_parameters, 'rb') as handle:\n","        decoder_parameters= pickle.load(handle)\n","\n","    # loading\n","    with open(path_decoder_dictionary, 'rb') as handle:\n","        decoder_dictionary = pickle.load(handle)    \n","\n","    print(encoder_parameters)\n","    # encoder_dictionary\n","    print(decoder_parameters)\n","    # decoder_dictionary\n","\n","    encoder_inputs = reconstructed_model.input[0]  # input_1\n","    encoder_outputs, state_h_enc, state_c_enc = reconstructed_model.layers[4].output  # lstm_1\n","    encoder_states = [state_h_enc, state_c_enc]\n","    encoder_model = keras.Model(encoder_inputs, encoder_states)\n","    latent_dim = 256  # Note: may be need to save in drive as well\n","\n","\n","    num_decoder_tokens =decoder_parameters['num_decoder_tokens']\n","    max_output_length= decoder_parameters['max_decoder_seq_length']\n","    max_input_length= encoder_parameters['max_encoder_seq_length']\n","\n","    encoder_word_dict=encoder_dictionary\n","    decoder_word_dict= decoder_dictionary\n","\n","\n","    decoder_inputs = Input(shape=( max_output_length , ))\n","    decoder_embedding = Embedding( num_decoder_tokens, 256 , mask_zero=True) (decoder_inputs)\n","\n","    decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","    decoder_dense = Dense( num_decoder_tokens , activation=tf.keras.activations.softmax ) \n","\n","\n","    def str_to_tokens( sentence : str ):\n","        words = sentence.lower().split()\n","        tokens_list = list()\n","        for word in words:\n","                # print(\"word \", word, eng_word_dict.get(word,1) )\n","                my_word=  encoder_word_dict.get(word,1)\n","                tokens_list.append(my_word) \n","    \n","        return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n","\n","\n","    def make_inference_models():\n","        \n","            encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","            \n","            decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","            decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","            \n","            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","            \n","            decoder_outputs, state_h, state_c = decoder_lstm(\n","                decoder_embedding , initial_state=decoder_states_inputs)\n","            decoder_states = [state_h, state_c]\n","            decoder_outputs = decoder_dense(decoder_outputs)\n","            decoder_model = tf.keras.models.Model(\n","                [decoder_inputs] + decoder_states_inputs,\n","                [decoder_outputs] + decoder_states)\n","            \n","            return encoder_model , decoder_model\n","\n","\n","    enc_model , dec_model = make_inference_models()\n","\n","\n","    # Test Previous Model\n","\n","\n","    encoderPath= encoderPath\n","    decoderPath= decoderPath\n","\n","    # loading\n","\n","    enc_model =  load_model(encoderPath)\n","    dec_model  =  load_model(decoderPath)\n","\n","    def translate_sentence(sentence):\n","        for epoch in range(1 ):\n","            states_values = enc_model.predict( str_to_tokens(sentence ) )\n","            empty_target_seq = np.zeros( ( 1 , 1 ) )\n","            empty_target_seq[0, 0] = decoder_word_dict['start']\n","            stop_condition = False\n","            decoded_translation = ''\n","            while not stop_condition :\n","                dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","                sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","                sampled_word = None\n","                for word , index in decoder_word_dict.items() :\n","                    if sampled_word_index == index :\n","                        decoded_translation += ' {}'.format( word )\n","                        sampled_word = word\n","                \n","                if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","                    stop_condition = True\n","                    \n","                empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","                empty_target_seq[ 0 , 0 ] = sampled_word_index\n","                states_values = [ h , c ] \n","\n","            print(\"Decoded Traslation \", decoded_translation )\n","        return  decoded_translation\n","\n","\n","    ## Get sentences to test the model\n","\n","    lines = pd.read_table( 'hin.txt' , names=[ 'eng' , 'hindi' ] )\n","    lines.reset_index( level=0 , inplace=True )\n","    lines.rename( columns={ 'index' : 'eng' , 'eng' : 'hindi' , 'hindi' : 'c' } , inplace=True )\n","    lines = lines.drop( 'c' , 1 )  \n","\n","    sample_sentences= lines[-10:]\n","    sample_sentences\n","\n","    # Reference Token \n","\n","    reference_tokens=[]\n","\n","    for line in sample_sentences['eng']:\n","        print( line.split() ) \n","        reference_tokens.append( line.split() )\n","\n","    df = pd.DataFrame(      columns=['reference', 'candidate', 'bleu_score'],  )\n","\n","    df[\"reference\"]= reference_tokens\n","\n","\n","\n","    # Candidate Tokens \n","    candidate_tokens=[]\n","\n","\n","    for line in sample_sentences['hindi']:\n","    \n","        result= translate_sentence(line)\n","        temp =result.split()\n","        temp= temp[:-1]\n","        candidate_tokens.append(temp)\n","        \n","\n","    df[\"candidate\"]= candidate_tokens\n","\n","\n","    ## Calculate BLEU score\n","\n","    scores=[]\n","    for reference, candidate in zip(df['reference'], df['candidate']):\n","    \n","        result= sentence_bleu([reference], candidate)\n","        scores.append(result)\n","    \n","    df[\"bleu_score\"]= scores    ## BLEU score calculated\n","\n","\n","    ## Calcualte ROUGE score\n","    scores=[]\n","    for reference, candidate in zip(df['reference'], df['candidate']):\n","        temp =['captain', 'of', 'the', 'delta', 'flight']\n","        references =tf.ragged.constant([reference])\n","        hypotheses= tf.ragged.constant([candidate])\n","\n","        result= text.metrics.rouge_l(hypotheses, references)\n","        \n","        result_str= \" F-measure: \"+str(result.f_measure.numpy()[0]) +\"  Precision: \"+str(result.p_measure.numpy()[0])+\"  Recall: \"+str(result.r_measure.numpy()[0])\n","        column=[\"f_measure\", \"p_measure\", \"r_measure\"]\n","        data= [[result.f_measure.numpy()[0] ,result.p_measure.numpy()[0] , result.r_measure.numpy()[0] ]]\n","        metric= pd.DataFrame(data=data, columns=column)\n","        resultObj= {\"f_measure\": result.f_measure.numpy()[0] , \"p_measure\": result.p_measure.numpy()[0],  \"r_measure\":result.r_measure.numpy()[0] }  \n","        scores.append(resultObj)\n","    \n","    \n","\n","    df[\"rouge_score\"]= scores  ## ROUGE score calculated\n","\n","\n","\n","    rouge_metric= pd.DataFrame.from_records(df['rouge_score'])\n","\n","    average_f_measure = rouge_metric['f_measure'].mean()\n","    average_p_measure = rouge_metric['p_measure'].mean()\n","    average_r_measure = rouge_metric['r_measure'].mean()\n","    # average_cosine= df['cosine_similarity'].mean()\n","    average_bleu= df['bleu_score'].mean()\n","\n","    ## return BLEU and ROUGE score to the list \n","    return [average_f_measure, average_p_measure,average_r_measure, average_bleu]\n","\n","        \n","\n","    "],"metadata":{"id":"r9dvKzzc4pdQ","executionInfo":{"status":"ok","timestamp":1665326505334,"user_tz":-360,"elapsed":6,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Get Summary Statistics Table"],"metadata":{"id":"YBiGboDFKJyy"}},{"cell_type":"code","source":["\n","stat=[]\n","for item in model_list:\n","\n","    model_path= path+item+\"/model.h5\" \n","    path_encoder_parameters= path+item+\"/parameters/encoder_parameters.pickle\" \n","    path_encoder_dictionary= path+item+\"/dictionaries/encoder_dictionary.pickle\" \n","    path_decoder_parameters= path+item+\"/parameters/decoder_parameters.pickle\" \n","    path_decoder_dictionary= path+item+\"/dictionaries/decoder_dictionary.pickle\" \n","    encoderPath= path+item+\"/enc_model.h5\" \n","    decoderPath= path+item+\"/dec_model.h5\" \n","    print(model_path, path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary,encoderPath,decoderPath)\n","    result= get_model_statistics_summary(model_path, path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary,encoderPath,decoderPath)\n","    stat.append(result)\n","\n","table =pd.DataFrame(columns=[\"average_f_measure\", \"average_p_measure\",\"average_r_measure\",  \"average_bleu\"], data=stat)\n","table"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"USuXFNZ5AS_l","executionInfo":{"status":"ok","timestamp":1665326571864,"user_tz":-360,"elapsed":16266,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"outputId":"1e09ad98-7177-4bf4-b330-a307652c2b12"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/model.h5 /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/parameters/encoder_parameters.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/dictionaries/encoder_dictionary.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/parameters/decoder_parameters.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/dictionaries/decoder_dictionary.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/enc_model.h5 /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/2908/dec_model.h5\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 21)]         0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 20)]         0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 21, 256)      770816      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 20, 256)      612864      ['input_2[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 256),        525312      ['embedding[0][0]']              \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, 20, 256),    525312      ['embedding_1[0][0]',            \n","                                 (None, 256),                     'lstm[0][1]',                   \n","                                 (None, 256)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, 20, 2394)     615258      ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,049,562\n","Trainable params: 3,049,562\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 21, 'num_encoder_tokens': 3011}\n","{'max_decoder_seq_length': 20, 'num_decoder_tokens': 2394}\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:135: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"]},{"output_type":"stream","name":"stdout","text":["['Mother', 'Teresa', 'was', 'a', 'Catholic', 'nun', 'who', 'lived', 'and', 'worked', 'in', 'Calcutta,', 'India.']\n","['George', 'Washington', 'was', 'the', 'first', 'president', 'of', 'the', 'Unites', 'States', 'of', 'America.']\n","['From', 'my', 'point', 'of', 'view,', 'Australia', 'is', 'one', 'of', 'the', 'best', 'countries', 'in', 'the', 'world.']\n","['In', '1951,', 'Sister', 'Teresa', 'was', 'sent', 'to', 'Calcutta,', 'then', 'the', 'largest', 'city', 'in', 'India.']\n","['Mother', 'Teresa', 'used', 'the', 'prize', 'money', 'for', 'her', 'work', 'in', 'India', 'and', 'around', 'the', 'world.']\n","['If', 'you', 'go', 'to', 'that', 'supermarket,', 'you', 'can', 'buy', 'most', 'things', 'you', 'use', 'in', 'your', 'daily', 'life.']\n","['The', 'passengers', 'who', 'were', 'injured', 'in', 'the', 'accident', 'were', 'taken', 'to', 'the', 'nearest', 'hospital.']\n","['Democracy', 'is', 'the', 'worst', 'form', 'of', 'government,', 'except', 'all', 'the', 'others', 'that', 'have', 'been', 'tried.']\n","['If', 'my', 'boy', 'had', 'not', 'been', 'killed', 'in', 'the', 'traffic', 'accident,', 'he', 'would', 'be', 'a', 'college', 'student', 'now.']\n","['When', 'I', 'was', 'a', 'kid,', 'touching', 'bugs', \"didn't\", 'bother', 'me', 'a', 'bit.', 'Now', 'I', 'can', 'hardly', 'stand', 'looking', 'at', 'pictures', 'of', 'them.']\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Model was constructed with shape (None, 20) for input KerasTensor(type_spec=TensorSpec(shape=(None, 20), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"output_type":"stream","name":"stdout","text":["Decoded Traslation   a man was thrown for a long were wrong end\n","Decoded Traslation   could you a doctor in end\n","Decoded Traslation   my father has just cleared the room of the table end\n","Decoded Traslation   the man woke up for a strange man end\n","Decoded Traslation   whoever wins the rule to the rumor say that we have a house end\n","Decoded Traslation   it was not to do that he went to the job end\n","Decoded Traslation   did the police arrest his job end\n","Decoded Traslation   the doctor advised that he went to the job to the party end\n","Decoded Traslation   my father is afraid of her grandmother end\n","Decoded Traslation   i don't think if i had a fight with my mother time end\n","/content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/model.h5 /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/parameters/encoder_parameters.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/dictionaries/encoder_dictionary.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/parameters/decoder_parameters.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/dictionaries/decoder_dictionary.pickle /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/enc_model.h5 /content/drive/My Drive/Machine Learning/GitHub Projects/machine-translation/models/1000/dec_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 12)]         0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 9)]          0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 12, 256)      309760      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 9, 256)       236032      ['input_2[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 256),        525312      ['embedding[0][0]']              \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, 9, 256),     525312      ['embedding_1[0][0]',            \n","                                 (None, 256),                     'lstm[0][1]',                   \n","                                 (None, 256)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, 9, 922)       236954      ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,833,370\n","Trainable params: 1,833,370\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 12, 'num_encoder_tokens': 1210}\n","{'max_decoder_seq_length': 9, 'num_decoder_tokens': 922}\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stdout","text":["['Mother', 'Teresa', 'was', 'a', 'Catholic', 'nun', 'who', 'lived', 'and', 'worked', 'in', 'Calcutta,', 'India.']\n","['George', 'Washington', 'was', 'the', 'first', 'president', 'of', 'the', 'Unites', 'States', 'of', 'America.']\n","['From', 'my', 'point', 'of', 'view,', 'Australia', 'is', 'one', 'of', 'the', 'best', 'countries', 'in', 'the', 'world.']\n","['In', '1951,', 'Sister', 'Teresa', 'was', 'sent', 'to', 'Calcutta,', 'then', 'the', 'largest', 'city', 'in', 'India.']\n","['Mother', 'Teresa', 'used', 'the', 'prize', 'money', 'for', 'her', 'work', 'in', 'India', 'and', 'around', 'the', 'world.']\n","['If', 'you', 'go', 'to', 'that', 'supermarket,', 'you', 'can', 'buy', 'most', 'things', 'you', 'use', 'in', 'your', 'daily', 'life.']\n","['The', 'passengers', 'who', 'were', 'injured', 'in', 'the', 'accident', 'were', 'taken', 'to', 'the', 'nearest', 'hospital.']\n","['Democracy', 'is', 'the', 'worst', 'form', 'of', 'government,', 'except', 'all', 'the', 'others', 'that', 'have', 'been', 'tried.']\n","['If', 'my', 'boy', 'had', 'not', 'been', 'killed', 'in', 'the', 'traffic', 'accident,', 'he', 'would', 'be', 'a', 'college', 'student', 'now.']\n","['When', 'I', 'was', 'a', 'kid,', 'touching', 'bugs', \"didn't\", 'bother', 'me', 'a', 'bit.', 'Now', 'I', 'can', 'hardly', 'stand', 'looking', 'at', 'pictures', 'of', 'them.']\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Model was constructed with shape (None, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"output_type":"stream","name":"stdout","text":["Decoded Traslation   she isn't my cousin end\n","Decoded Traslation   she is my wealthy woman end\n","Decoded Traslation   my family loved my woman end\n","Decoded Traslation   she is his man but the job end\n","Decoded Traslation   that guy annoys me end\n","Decoded Traslation   he is not my little toy end\n","Decoded Traslation   she is poor but the sharp end\n","Decoded Traslation   she isn't my cousin end\n","Decoded Traslation   he tends to be very beer end\n","Decoded Traslation   she isn't my cousin end\n"]},{"output_type":"execute_result","data":{"text/plain":["   average_f_measure  average_p_measure  average_r_measure   average_bleu\n","0           0.118541           0.154241           0.098203  4.154123e-156\n","1           0.046190           0.084286           0.032063  1.071106e-232"],"text/html":["\n","  <div id=\"df-5b166885-facb-4bcd-b9e0-5707e703d85f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>average_f_measure</th>\n","      <th>average_p_measure</th>\n","      <th>average_r_measure</th>\n","      <th>average_bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.118541</td>\n","      <td>0.154241</td>\n","      <td>0.098203</td>\n","      <td>4.154123e-156</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.046190</td>\n","      <td>0.084286</td>\n","      <td>0.032063</td>\n","      <td>1.071106e-232</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b166885-facb-4bcd-b9e0-5707e703d85f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5b166885-facb-4bcd-b9e0-5707e703d85f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5b166885-facb-4bcd-b9e0-5707e703d85f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyPvYxlTuCkxyIUYQrknpQaa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}