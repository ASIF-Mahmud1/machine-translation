{"cells":[{"cell_type":"markdown","metadata":{"id":"B453NNRC0joS"},"source":["### Load Model From Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28332,"status":"ok","timestamp":1665306135759,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"jvdaYoDtgQMT","outputId":"6da8f8e2-ddba-4151-b358-edb220959dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_Fo-82DfGVBw","executionInfo":{"status":"ok","timestamp":1665306137678,"user_tz":-360,"elapsed":1923,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","import pickle\n","from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","import pandas as pd"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6378,"status":"ok","timestamp":1665306144050,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"PRmk_1DQG1K7","outputId":"a3ae048e-9cb3-4c40-98e0-dd4acabba391"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 6.8 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q \"tensorflow-text==2.8.*\"\n","import tensorflow as tf\n","import tensorflow_text as text"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37379,"status":"ok","timestamp":1665306181420,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"f-ewkr7UHDZb","outputId":"393fc9fb-f089-4771-88b8-d57043374108"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 4.8 MB/s \n","\u001b[?25hCollecting pybind11>=2.2\n","  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3165401 sha256=978c4f544e7b00b801ec8a282dbac89fb489d154ca2219ef605ea3b106570a20\n","  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"]}],"source":["!pip install fasttext\n","import fasttext.util"]},{"cell_type":"markdown","metadata":{"id":"PDoZgXN3uQJn"},"source":["#Path of Saved Models"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":854,"status":"ok","timestamp":1665306182255,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"LRp6PY8huVGl","outputId":"f7aec18f-9e20-486e-9cc6-f32895e4e737"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n","/content/drive/MyDrive/Machine Learning/GitHub Projects/machine-translation\n","gitCommands.ipynb  models  notes.txt  README.md  test  train\n","/content/drive/MyDrive/Machine Learning/GitHub Projects/machine-translation/models\n","1000  2908  _about.txt\thin-eng.zip  hin.txt  modelsummary.png\n"]}],"source":["## Move to root directory\n","!ls\n","%cd drive/MyDrive/Machine\\ Learning/GitHub\\ Projects/machine-translation\n","!ls\n","\n","## Move to models directory\n","%cd models/\n","!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Hiz0d8v9fTjS","executionInfo":{"status":"ok","timestamp":1665306182256,"user_tz":-360,"elapsed":4,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["##Copy of Previous Cell"],"metadata":{"id":"c8vcvCyNPBdr"}},{"cell_type":"code","source":["# %cd ..\n","# %cd ..\n","# %cd ..\n","# %cd ..\n","# %cd ..\n","# %cd ..\n","# %cd ..\n","# !ls ## Need to change in the future: currently text files are downloaded on my Drive\n","\n","# !ls\n","# %cd content \n","# !ls\n","# ## Move to root directory\n","# !ls\n","# %cd drive/MyDrive/Machine\\ Learning/GitHub\\ Projects/machine-translation\n","# !ls\n","\n","# ## Move to models directory\n","# %cd models/\n","# !ls"],"metadata":{"id":"MA52obXJX8Wu","executionInfo":{"status":"ok","timestamp":1665306190105,"user_tz":-360,"elapsed":8,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!wget http://www.manythings.org/anki/hin-eng.zip -O hin-eng.zip\n","!unzip hin-eng.zip"],"metadata":{"id":"O8PEeQ4LY6Xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_statistics_summary= []"],"metadata":{"id":"VgvzL7Vbl8Tn","executionInfo":{"status":"ok","timestamp":1665306205895,"user_tz":-360,"elapsed":10,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_model_statistics_summary(model_path,path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary, encoderPath, decoderPath ):\n","\n"," \n","    reconstructed_model = keras.models.load_model(model_path)\n","    plot_model(reconstructed_model, to_file='modelsummary.png', show_shapes=True, show_layer_names=True)\n","    reconstructed_model.summary()\n","\n","\n","    ## Load Dictionaries and Parameters \n","    path_encoder_parameters= path_encoder_parameters\n","    path_encoder_dictionary= path_encoder_dictionary\n","    path_decoder_parameters= path_decoder_parameters\n","    path_decoder_dictionary= path_decoder_dictionary\n","\n","    # loading\n","    with open(path_encoder_parameters, 'rb') as handle:\n","        encoder_parameters = pickle.load(handle)\n","\n","    # loading\n","    with open(path_encoder_dictionary, 'rb') as handle:\n","        encoder_dictionary = pickle.load(handle)\n","\n","    # loading\n","    with open(path_decoder_parameters, 'rb') as handle:\n","        decoder_parameters= pickle.load(handle)\n","\n","    # loading\n","    with open(path_decoder_dictionary, 'rb') as handle:\n","        decoder_dictionary = pickle.load(handle)    \n","\n","    print(encoder_parameters)\n","    # encoder_dictionary\n","    print(decoder_parameters)\n","    # decoder_dictionary\n","\n","    encoder_inputs = reconstructed_model.input[0]  # input_1\n","    encoder_outputs, state_h_enc, state_c_enc = reconstructed_model.layers[4].output  # lstm_1\n","    encoder_states = [state_h_enc, state_c_enc]\n","    encoder_model = keras.Model(encoder_inputs, encoder_states)\n","    latent_dim = 256  # Note: may be need to save in drive as well\n","\n","\n","    num_decoder_tokens =decoder_parameters['num_decoder_tokens']\n","    max_output_length= decoder_parameters['max_decoder_seq_length']\n","    max_input_length= encoder_parameters['max_encoder_seq_length']\n","\n","    encoder_word_dict=encoder_dictionary\n","    decoder_word_dict= decoder_dictionary\n","\n","\n","    decoder_inputs = Input(shape=( max_output_length , ))\n","    decoder_embedding = Embedding( num_decoder_tokens, 256 , mask_zero=True) (decoder_inputs)\n","\n","    decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","    decoder_dense = Dense( num_decoder_tokens , activation=tf.keras.activations.softmax ) \n","\n","\n","    def str_to_tokens( sentence : str ):\n","        words = sentence.lower().split()\n","        tokens_list = list()\n","        for word in words:\n","                # print(\"word \", word, eng_word_dict.get(word,1) )\n","                my_word=  encoder_word_dict.get(word,1)\n","                tokens_list.append(my_word) \n","    \n","        return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n","\n","\n","    def make_inference_models():\n","        \n","            encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","            \n","            decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","            decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","            \n","            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","            \n","            decoder_outputs, state_h, state_c = decoder_lstm(\n","                decoder_embedding , initial_state=decoder_states_inputs)\n","            decoder_states = [state_h, state_c]\n","            decoder_outputs = decoder_dense(decoder_outputs)\n","            decoder_model = tf.keras.models.Model(\n","                [decoder_inputs] + decoder_states_inputs,\n","                [decoder_outputs] + decoder_states)\n","            \n","            return encoder_model , decoder_model\n","\n","\n","    enc_model , dec_model = make_inference_models()\n","\n","\n","    # Test Previous Model\n","\n","\n","    encoderPath= encoderPath\n","    decoderPath= decoderPath\n","\n","    # loading\n","\n","    enc_model =  load_model(encoderPath)\n","    dec_model  =  load_model(decoderPath)\n","\n","    def translate_sentence(sentence):\n","        for epoch in range(1 ):\n","            states_values = enc_model.predict( str_to_tokens(sentence ) )\n","            empty_target_seq = np.zeros( ( 1 , 1 ) )\n","            empty_target_seq[0, 0] = decoder_word_dict['start']\n","            stop_condition = False\n","            decoded_translation = ''\n","            while not stop_condition :\n","                dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","                sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","                sampled_word = None\n","                for word , index in decoder_word_dict.items() :\n","                    if sampled_word_index == index :\n","                        decoded_translation += ' {}'.format( word )\n","                        sampled_word = word\n","                \n","                if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","                    stop_condition = True\n","                    \n","                empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","                empty_target_seq[ 0 , 0 ] = sampled_word_index\n","                states_values = [ h , c ] \n","\n","            print(\"Decoded Traslation \", decoded_translation )\n","        return  decoded_translation\n","\n","\n","    ## Get sentences to test the model\n","\n","    lines = pd.read_table( 'hin.txt' , names=[ 'eng' , 'hindi' ] )\n","    lines.reset_index( level=0 , inplace=True )\n","    lines.rename( columns={ 'index' : 'eng' , 'eng' : 'hindi' , 'hindi' : 'c' } , inplace=True )\n","    lines = lines.drop( 'c' , 1 )  \n","\n","    sample_sentences= lines[-10:]\n","    sample_sentences\n","\n","    # Reference Token \n","\n","    reference_tokens=[]\n","\n","    for line in sample_sentences['eng']:\n","        print( line.split() ) \n","        reference_tokens.append( line.split() )\n","\n","    df = pd.DataFrame(      columns=['reference', 'candidate', 'bleu_score'],  )\n","\n","    df[\"reference\"]= reference_tokens\n","\n","\n","\n","    # Candidate Tokens \n","    candidate_tokens=[]\n","\n","\n","    for line in sample_sentences['hindi']:\n","    \n","        result= translate_sentence(line)\n","        temp =result.split()\n","        temp= temp[:-1]\n","        candidate_tokens.append(temp)\n","        \n","\n","    df[\"candidate\"]= candidate_tokens\n","\n","\n","    ## Calculate BLEU score\n","\n","    scores=[]\n","    for reference, candidate in zip(df['reference'], df['candidate']):\n","    \n","        result= sentence_bleu([reference], candidate)\n","        scores.append(result)\n","    \n","    df[\"bleu_score\"]= scores    ## BLEU score calculated\n","\n","\n","    ## Calcualte ROUGE score\n","    scores=[]\n","    for reference, candidate in zip(df['reference'], df['candidate']):\n","        temp =['captain', 'of', 'the', 'delta', 'flight']\n","        references =tf.ragged.constant([reference])\n","        hypotheses= tf.ragged.constant([candidate])\n","\n","        result= text.metrics.rouge_l(hypotheses, references)\n","        \n","        result_str= \" F-measure: \"+str(result.f_measure.numpy()[0]) +\"  Precision: \"+str(result.p_measure.numpy()[0])+\"  Recall: \"+str(result.r_measure.numpy()[0])\n","        column=[\"f_measure\", \"p_measure\", \"r_measure\"]\n","        data= [[result.f_measure.numpy()[0] ,result.p_measure.numpy()[0] , result.r_measure.numpy()[0] ]]\n","        metric= pd.DataFrame(data=data, columns=column)\n","        resultObj= {\"f_measure\": result.f_measure.numpy()[0] , \"p_measure\": result.p_measure.numpy()[0],  \"r_measure\":result.r_measure.numpy()[0] }  \n","        scores.append(resultObj)\n","    \n","    \n","\n","    df[\"rouge_score\"]= scores  ## ROUGE score calculated\n","\n","\n","\n","    rouge_metric= pd.DataFrame.from_records(df['rouge_score'])\n","\n","    average_f_measure = rouge_metric['f_measure'].mean()\n","    average_p_measure = rouge_metric['p_measure'].mean()\n","    average_r_measure = rouge_metric['r_measure'].mean()\n","    # average_cosine= df['cosine_similarity'].mean()\n","    average_bleu= df['bleu_score'].mean()\n","\n","    ## Append BLEU and ROUGE score to the list \"model_statistics_summary\"\n","    return [average_f_measure, average_p_measure,average_r_measure, average_bleu]\n","\n","        \n","\n","    "],"metadata":{"id":"r9dvKzzc4pdQ","executionInfo":{"status":"ok","timestamp":1665308100278,"user_tz":-360,"elapsed":489,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["model_list=[\"2908\",\"1000\"]\n","stat=[]\n","for item in model_list:\n","\n","    model_path= item+\"/model.h5\" ## \"2908/model.h5\"\n","    path_encoder_parameters= item+\"/parameters/encoder_parameters.pickle\" ## '2908/parameters/encoder_parameters.pickle'\n","    path_encoder_dictionary= item+\"/dictionaries/encoder_dictionary.pickle\" ## '2908/dictionaries/encoder_dictionary.pickle'\n","    path_decoder_parameters= item+\"/parameters/decoder_parameters.pickle\" ## '2908/dictionaries/encoder_dictionary.pickle'\n","    path_decoder_dictionary= item+\"/dictionaries/decoder_dictionary.pickle\" ## '2908/dictionaries/decoder_dictionary.pickle'\n","    encoderPath= item+\"/enc_model.h5\" ## \"2908/enc_model.h5\" \n","    decoderPath= item+\"/dec_model.h5\" ##  \"2908/dec_model.h5\"\n","    print(model_path, path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary,encoderPath,decoderPath)\n","    result= get_model_statistics_summary(model_path, path_encoder_parameters,path_encoder_dictionary,path_decoder_parameters,path_decoder_dictionary,encoderPath,decoderPath)\n","    stat.append(result)\n","    \n","table =pd.DataFrame(columns=[\"average_f_measure\", \"average_p_measure\",\"average_r_measure\",  \"average_bleu\"], data=stat)\n","table"],"metadata":{"id":"USuXFNZ5AS_l"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyNIcHOyxMFapf2FswwfA7ub"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}