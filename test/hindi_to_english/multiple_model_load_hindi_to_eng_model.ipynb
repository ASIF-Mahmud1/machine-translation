{"cells":[{"cell_type":"markdown","metadata":{"id":"B453NNRC0joS"},"source":["### Load Model From Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3914,"status":"ok","timestamp":1665245331833,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"jvdaYoDtgQMT","outputId":"66b41179-b99e-4c2f-9281-b05fdb46bb01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_Fo-82DfGVBw","executionInfo":{"status":"ok","timestamp":1665245325839,"user_tz":-360,"elapsed":4834,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","import pickle\n","from tensorflow.keras.layers import Input, Embedding,Dense,  LSTM\n","from tensorflow.keras import layers , activations , models , preprocessing , utils\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7550,"status":"ok","timestamp":1665245339374,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"PRmk_1DQG1K7","outputId":"c4ea7f73-ca60-4ead-a1cd-69fbd8dcc436"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 5.3 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q \"tensorflow-text==2.8.*\"\n","import tensorflow as tf\n","import tensorflow_text as text"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61166,"status":"ok","timestamp":1665245400535,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"f-ewkr7UHDZb","outputId":"b18ff879-61c9-4247-86af-e28803a569f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 3.9 MB/s \n","\u001b[?25hCollecting pybind11>=2.2\n","  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3165348 sha256=2381b29fc7e5573879bcc64de62b2b7f25e5d8dc5c3266335b4ee167722a6f8f\n","  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"]}],"source":["!pip install fasttext\n","import fasttext.util"]},{"cell_type":"markdown","metadata":{"id":"PDoZgXN3uQJn"},"source":["#Path of Saved Models"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":904,"status":"ok","timestamp":1665245401412,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"},"user_tz":-360},"id":"LRp6PY8huVGl","outputId":"a9a34873-0921-4075-89f6-1a52f2c8d3fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n","/content/drive/MyDrive/Machine Learning/GitHub Projects/machine-translation\n","gitCommands.ipynb  models  notes.txt  README.md  test  train\n","/content/drive/MyDrive/Machine Learning/GitHub Projects/machine-translation/models\n","1000  2908  _about.txt\thin-eng.zip  hin.txt  modelsummary.png\n"]}],"source":["## Move to root directory\n","!ls\n","%cd drive/MyDrive/Machine\\ Learning/GitHub\\ Projects/machine-translation\n","!ls\n","\n","## Move to models directory\n","%cd models/\n","!ls"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Hiz0d8v9fTjS","executionInfo":{"status":"ok","timestamp":1665245401412,"user_tz":-360,"elapsed":4,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dllAfUOefe8m","executionInfo":{"status":"ok","timestamp":1665245408428,"user_tz":-360,"elapsed":7019,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1a3fb9af-a9de-4b39-e99c-953772700370"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 21)]         0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 20)]         0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 21, 256)      770816      ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 20, 256)      612864      ['input_2[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 256),        525312      ['embedding[0][0]']              \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, 20, 256),    525312      ['embedding_1[0][0]',            \n","                                 (None, 256),                     'lstm[0][1]',                   \n","                                 (None, 256)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, 20, 2394)     615258      ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,049,562\n","Trainable params: 3,049,562\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","{'max_encoder_seq_length': 21, 'num_encoder_tokens': 3011}\n","{'max_decoder_seq_length': 20, 'num_decoder_tokens': 2394}\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:Model was constructed with shape (None, 20) for input KerasTensor(type_spec=TensorSpec(shape=(None, 20), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"output_type":"stream","name":"stdout","text":["Decoded Traslation   i'm not a doctor end\n"]}],"source":["model_2908=\"2908/model.h5\"\n","model_1000=\"1000/model.h5\"\n","model_2909=\"\"\n","reconstructed_model = keras.models.load_model(model_2908)\n","plot_model(reconstructed_model, to_file='modelsummary.png', show_shapes=True, show_layer_names=True)\n","reconstructed_model.summary()\n","\n","\n","## Load Dictionaries and Parameters \n","path_encoder_parameters='2908/parameters/encoder_parameters.pickle'\n","path_encoder_dictionary='2908/dictionaries/encoder_dictionary.pickle'\n","path_decoder_parameters='2908/parameters/decoder_parameters.pickle'\n","path_decoder_dictionary='2908/dictionaries/decoder_dictionary.pickle'\n","\n","# loading\n","with open(path_encoder_parameters, 'rb') as handle:\n","    encoder_parameters = pickle.load(handle)\n","\n","# loading\n","with open(path_encoder_dictionary, 'rb') as handle:\n","    encoder_dictionary = pickle.load(handle)\n","\n","# loading\n","with open(path_decoder_parameters, 'rb') as handle:\n","    decoder_parameters= pickle.load(handle)\n","\n","# loading\n","with open(path_decoder_dictionary, 'rb') as handle:\n","    decoder_dictionary = pickle.load(handle)    \n","\n","print(encoder_parameters)\n","# encoder_dictionary\n","print(decoder_parameters)\n","# decoder_dictionary\n","\n","encoder_inputs = reconstructed_model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = reconstructed_model.layers[4].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","latent_dim = 256  # Note: may be need to save in drive as well\n","\n","\n","num_decoder_tokens =decoder_parameters['num_decoder_tokens']\n","max_output_length= decoder_parameters['max_decoder_seq_length']\n","max_input_length= encoder_parameters['max_encoder_seq_length']\n","\n","encoder_word_dict=encoder_dictionary\n","decoder_word_dict= decoder_dictionary\n","\n","\n","decoder_inputs = Input(shape=( max_output_length , ))\n","decoder_embedding = Embedding( num_decoder_tokens, 256 , mask_zero=True) (decoder_inputs)\n","\n","decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","decoder_dense = Dense( num_decoder_tokens , activation=tf.keras.activations.softmax ) \n","\n","\n","def str_to_tokens( sentence : str ):\n","    words = sentence.lower().split()\n","    tokens_list = list()\n","    for word in words:\n","          # print(\"word \", word, eng_word_dict.get(word,1) )\n","          my_word=  encoder_word_dict.get(word,1)\n","          tokens_list.append(my_word) \n","  \n","    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n","\n","\n","def make_inference_models():\n","    \n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","    \n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","    \n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    \n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","    \n","    return encoder_model , decoder_model\n","\n","\n","enc_model , dec_model = make_inference_models()\n","\n","\n","# Test Previous Model\n","\n","\n","encoderPath=\"2908/enc_model.h5\" \n","decoderPath=\"2908/dec_model.h5\"\n","\n","# loading\n","\n","enc_model =  load_model(encoderPath)\n","dec_model  =  load_model(decoderPath)\n","\n","for epoch in range(1 ):\n","    states_values = enc_model.predict( str_to_tokens(\"मुझे खाने से प्यार है\" ) )\n","    empty_target_seq = np.zeros( ( 1 , 1 ) )\n","    empty_target_seq[0, 0] = decoder_word_dict['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","    while not stop_condition :\n","        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","        sampled_word = None\n","        for word , index in decoder_word_dict.items() :\n","            if sampled_word_index == index :\n","                decoded_translation += ' {}'.format( word )\n","                sampled_word = word\n","        \n","        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","            stop_condition = True\n","            \n","        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","        empty_target_seq[ 0 , 0 ] = sampled_word_index\n","        states_values = [ h , c ] \n","\n","    print(\"Decoded Traslation \", decoded_translation )\n","    # print(f\"{bcolors.OKGREEN}Decoded Traslation: { decoded_translation}{bcolors.ENDC}\")"]},{"cell_type":"markdown","source":["##Copy of Previous Cell"],"metadata":{"id":"c8vcvCyNPBdr"}},{"cell_type":"code","source":["%cd ..\n","%cd ..\n","%cd ..\n","%cd ..\n","%cd ..\n","%cd ..\n","%cd ..\n","!ls ## Need to change in the future: currently text files are downloaded on my Drive\n","\n","!ls\n","%cd content \n","!ls\n","## Move to root directory\n","!ls\n","%cd drive/MyDrive/Machine\\ Learning/GitHub\\ Projects/machine-translation\n","!ls\n","\n","## Move to models directory\n","%cd models/\n","!ls"],"metadata":{"id":"MA52obXJX8Wu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget http://www.manythings.org/anki/hin-eng.zip -O hin-eng.zip\n","!unzip hin-eng.zip"],"metadata":{"id":"O8PEeQ4LY6Xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model_2908=\"2908/model.h5\"\n","model_1000=\"1000/model.h5\"\n","model_2909=\"\"\n","reconstructed_model = keras.models.load_model(model_2908)\n","plot_model(reconstructed_model, to_file='modelsummary.png', show_shapes=True, show_layer_names=True)\n","reconstructed_model.summary()\n","\n","\n","## Load Dictionaries and Parameters \n","path_encoder_parameters='2908/parameters/encoder_parameters.pickle'\n","path_encoder_dictionary='2908/dictionaries/encoder_dictionary.pickle'\n","path_decoder_parameters='2908/parameters/decoder_parameters.pickle'\n","path_decoder_dictionary='2908/dictionaries/decoder_dictionary.pickle'\n","\n","# loading\n","with open(path_encoder_parameters, 'rb') as handle:\n","    encoder_parameters = pickle.load(handle)\n","\n","# loading\n","with open(path_encoder_dictionary, 'rb') as handle:\n","    encoder_dictionary = pickle.load(handle)\n","\n","# loading\n","with open(path_decoder_parameters, 'rb') as handle:\n","    decoder_parameters= pickle.load(handle)\n","\n","# loading\n","with open(path_decoder_dictionary, 'rb') as handle:\n","    decoder_dictionary = pickle.load(handle)    \n","\n","print(encoder_parameters)\n","# encoder_dictionary\n","print(decoder_parameters)\n","# decoder_dictionary\n","\n","encoder_inputs = reconstructed_model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = reconstructed_model.layers[4].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","latent_dim = 256  # Note: may be need to save in drive as well\n","\n","\n","num_decoder_tokens =decoder_parameters['num_decoder_tokens']\n","max_output_length= decoder_parameters['max_decoder_seq_length']\n","max_input_length= encoder_parameters['max_encoder_seq_length']\n","\n","encoder_word_dict=encoder_dictionary\n","decoder_word_dict= decoder_dictionary\n","\n","\n","decoder_inputs = Input(shape=( max_output_length , ))\n","decoder_embedding = Embedding( num_decoder_tokens, 256 , mask_zero=True) (decoder_inputs)\n","\n","decoder_lstm = LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n","decoder_dense = Dense( num_decoder_tokens , activation=tf.keras.activations.softmax ) \n","\n","\n","def str_to_tokens( sentence : str ):\n","    words = sentence.lower().split()\n","    tokens_list = list()\n","    for word in words:\n","          # print(\"word \", word, eng_word_dict.get(word,1) )\n","          my_word=  encoder_word_dict.get(word,1)\n","          tokens_list.append(my_word) \n","  \n","    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n","\n","\n","def make_inference_models():\n","    \n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","    \n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n","    \n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    \n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","    \n","    return encoder_model , decoder_model\n","\n","\n","enc_model , dec_model = make_inference_models()\n","\n","\n","# Test Previous Model\n","\n","\n","encoderPath=\"2908/enc_model.h5\" \n","decoderPath=\"2908/dec_model.h5\"\n","\n","# loading\n","\n","enc_model =  load_model(encoderPath)\n","dec_model  =  load_model(decoderPath)\n","\n","def translate_sentence(sentence):\n","  for epoch in range(1 ):\n","    states_values = enc_model.predict( str_to_tokens(sentence ) )\n","    empty_target_seq = np.zeros( ( 1 , 1 ) )\n","    empty_target_seq[0, 0] = decoder_word_dict['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","    while not stop_condition :\n","        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","        sampled_word = None\n","        for word , index in decoder_word_dict.items() :\n","            if sampled_word_index == index :\n","                decoded_translation += ' {}'.format( word )\n","                sampled_word = word\n","        \n","        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n","            stop_condition = True\n","            \n","        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n","        empty_target_seq[ 0 , 0 ] = sampled_word_index\n","        states_values = [ h , c ] \n","\n","    print(\"Decoded Traslation \", decoded_translation )\n","  return  decoded_translation\n","\n","\n","\n","lines = pd.read_table( 'hin.txt' , names=[ 'eng' , 'hindi' ] )\n","lines.reset_index( level=0 , inplace=True )\n","lines.rename( columns={ 'index' : 'eng' , 'eng' : 'hindi' , 'hindi' : 'c' } , inplace=True )\n","lines = lines.drop( 'c' , 1 )  \n","\n","sample_sentences= lines[-10:]\n","sample_sentences\n","\n","# Reference Token \n","\n","reference_tokens=[]\n","\n","for line in sample_sentences['eng']:\n","   print( line.split() ) \n","   reference_tokens.append( line.split() )\n","\n","df = pd.DataFrame(      columns=['reference', 'candidate', 'bleu_score'],  )\n","\n","df[\"reference\"]= reference_tokens\n","\n","df\n","\n","# Candidate Tokens \n","candidate_tokens=[]\n","\n","\n","for line in sample_sentences['hindi']:\n","   \n","   result= translate_sentence(line)\n","   temp =result.split()\n","   temp= temp[:-1]\n","   candidate_tokens.append(temp)\n","   \n","\n","df[\"candidate\"]= candidate_tokens\n","\n","df\n","\n","scores=[]\n","for reference, candidate in zip(df['reference'], df['candidate']):\n","  \n","   result= sentence_bleu([reference], candidate)\n","   scores.append(result)\n","  #  print( result)\n"," \n","\n","df[\"bleu_score\"]= scores\n","\n","df\n","\n"],"metadata":{"id":"3FSNEUb9PGMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EFqh9lDvQYeW","executionInfo":{"status":"ok","timestamp":1665245481450,"user_tz":-360,"elapsed":7,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ejOg2-kFQaRT","executionInfo":{"status":"ok","timestamp":1665245481450,"user_tz":-360,"elapsed":6,"user":{"displayName":"'ASIFMAHMUD' IUB","userId":"12339450445858385211"}}},"execution_count":9,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyPsevSU9KqCyEha8A5ZBNPL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}