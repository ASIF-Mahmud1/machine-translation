{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSTALL THESE LIBRARES\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE=15000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/learn/Desktop/Projects/machine-translation/machine-learning/train/lstm', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python39.zip', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python3.9', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python3.9/lib-dynload', '', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python3.9/site-packages']\n",
      "['/Users/learn/Desktop/Projects/machine-translation/machine-learning/train/lstm', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python39.zip', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python3.9', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python3.9/lib-dynload', '', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/venv/lib/python3.9/site-packages', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/helper', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/utils', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/test/helper', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/utils', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/data', '/Users/learn/Desktop/Projects/machine-translation/machine-learning/train/lstm/create-model/lstm']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "from string import punctuation\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def printmd(string):\n",
    "    # Print with Markdowns    \n",
    "    display(Markdown(string))\n",
    "\n",
    "    \n",
    "def handle_helper_functions():\n",
    "    print(sys.path)\n",
    "    directory_path = os.path.abspath(os.path.join('../../helper'))\n",
    "    if directory_path not in sys.path:\n",
    "        sys.path.append(directory_path)    \n",
    "\n",
    "    translation_path=os.path.abspath(os.path.join('../../utils')) \n",
    "    if translation_path not in sys.path:\n",
    "        sys.path.append(translation_path)   \n",
    "    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/machine-learning/test/helper')                                       \n",
    "    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/machine-learning/utils')\n",
    "    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/machine-learning/data')\n",
    "\n",
    "\n",
    "    lstm_helper_path = os.path.abspath(os.path.join('./create-model/lstm/'))\n",
    "    if lstm_helper_path not in sys.path:\n",
    "        sys.path.append(lstm_helper_path)    \n",
    "\n",
    "    print(sys.path)\n",
    "    \n",
    "\n",
    "handle_helper_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n",
      "Reusing dataset parquet (/Users/learn/.cache/huggingface/datasets/parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8342309201641148b097c9b3d9cca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from iit_dataset import createDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator():\n",
    "    def __init__(self, training_size=10000) -> None:\n",
    "        self.model=None\n",
    "        self.training_size=training_size\n",
    "        self.idx_src=0\n",
    "        self.idx_tar = 1\n",
    "        self.source_str, self.target_str = \"Hindi\", \"English\"\n",
    "        self.tar_tokenizer=None #\n",
    "        self.tar_vocab_size=None #\n",
    "        self.src_tokenizer=None #\n",
    "        self.tar_length=None #\n",
    "        self.src_vocab_size=None #\n",
    "        self.src_length=None #\n",
    "        self.trainX=None\n",
    "        self.trainY=None\n",
    "        pass\n",
    "\n",
    "    def _get_training_data(self):\n",
    "        pool_oftexts, pairs =createDataset(data_size=self.training_size, type=\"train\")\n",
    "        dataset= pool_oftexts\n",
    "        return dataset\n",
    "    \n",
    "    def clean(self,string):\n",
    "        string = string.replace(\"\\u202f\",\" \")\n",
    "        string = string.lower()\n",
    "        for p in punctuation + \"«»\" + \"0123456789\":\n",
    "            string = string.replace(p,\" \")  \n",
    "        string = re.sub('\\s+',' ', string)\n",
    "        string = string.strip()\n",
    "            \n",
    "        return string\n",
    "    \n",
    "    def _generate_train_test_split(self):\n",
    "        ''' \n",
    "        GET TRAINING DATA AND SPLIT THEM TO TRAIN AND TEST SET\n",
    "\n",
    "        RETURN dataset, train, test\n",
    "        '''\n",
    "        dataset= self._get_training_data()\n",
    "        total_sentences= len(dataset)\n",
    "        test_proportion = 0.1\n",
    "        train_test_threshold = int( (1-test_proportion) * total_sentences)\n",
    "\n",
    "        dataset[\"eng\"] = dataset[\"eng\"].apply(lambda x: self.clean(x))\n",
    "        dataset[\"hindi\"] = dataset[\"hindi\"].apply(lambda x: self.clean(x))\n",
    "\n",
    "        dataset = dataset.values\n",
    "        dataset = dataset[:total_sentences]\n",
    "\n",
    "        train, test = dataset[:train_test_threshold], dataset[train_test_threshold:]\n",
    "\n",
    "        return dataset, train, test\n",
    "    def create_tokenizer(self,lines):\n",
    "        # fit a tokenizer\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        return tokenizer\n",
    " \n",
    "\n",
    "    def encode_sequences(self,tokenizer, length, lines):\n",
    "        # encode and pad sequences\n",
    "        X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n",
    "        X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n",
    "        return X\n",
    "    \n",
    "    def encode_output(self,sequences, vocab_size):\n",
    "        # one hot encode target sequence\n",
    "        ylist = list()\n",
    "        for sequence in sequences:\n",
    "            encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "            ylist.append(encoded)\n",
    "        y = np.array(ylist)\n",
    "        y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "        return y\n",
    " \n",
    "\n",
    "    def _convert_sentence_to_vectors(self):\n",
    "        ''' \n",
    "        GENERATE TRAIN TEST SPLIT\n",
    "        CREATE TARGET TOKENIZER\n",
    "        CREATE SOURCE TOKENIZER\n",
    "        RETURN VECTORS FOR TRAINING\n",
    "        '''\n",
    "        dataset, train, test= self. _generate_train_test_split()\n",
    "        # Prepare target tokenizer\n",
    "        tar_tokenizer = self.create_tokenizer(dataset[:, self.idx_tar]) #save\n",
    "        tar_vocab_size = len(tar_tokenizer.word_index) + 1  #save\n",
    "        tar_length = 15  #save\n",
    "        printmd(f'\\nTarget ({self.target_str}) Vocabulary Size: {tar_vocab_size}')\n",
    "        printmd(f'Target ({self.target_str}) Max Length: {tar_length}')\n",
    "\n",
    "        # Prepare source tokenizer\n",
    "        src_tokenizer = self.create_tokenizer(dataset[:, self.idx_src])  #save\n",
    "        src_vocab_size = len(src_tokenizer.word_index) + 1  #save\n",
    "        src_length = 15  #save\n",
    "        #### SAVE\n",
    "        self.tar_tokenizer=tar_tokenizer\n",
    "        self.tar_vocab_size=tar_vocab_size\n",
    "        self.src_tokenizer=src_tokenizer\n",
    "        self.tar_length=tar_length\n",
    "        self.src_vocab_size=src_vocab_size\n",
    "        self.src_length=src_length\n",
    "        ### SAVE\n",
    "        printmd(f'\\nSource ({self.source_str}) Vocabulary Size: {src_vocab_size}')\n",
    "        printmd(f'Source ({self.source_str}) Max Length: {src_length}\\n')\n",
    "        # PREPARING TRAINING DATA\n",
    "        trainX = self.encode_sequences(src_tokenizer, src_length, train[:, self.idx_src])\n",
    "        trainY = self.encode_sequences(tar_tokenizer, tar_length, train[:, self.idx_tar])\n",
    "        trainY = self.encode_output(trainY, tar_vocab_size)\n",
    "        self.trainX=trainX\n",
    "        self.trainY=trainY\n",
    "        pass\n",
    "    \n",
    "    def create_model(self, src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "        # Create the model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "        model.add(LSTM(n_units))\n",
    "        model.add(RepeatVector(tar_timesteps))\n",
    "        model.add(LSTM(n_units, return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        self._convert_sentence_to_vectors()\n",
    "        # Create model\n",
    "        model=self.model\n",
    "        model = self.create_model(self.src_vocab_size, self.tar_vocab_size, self.src_length, self.tar_length, 256)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        #######################\n",
    "        # display(\"TRAIN X: \",self.trainX)\n",
    "        # display(\"TRAIN Y \",self.trainY)\n",
    "        # display(model)\n",
    "        #######################\n",
    "        history = model.fit(self.trainX, \n",
    "               self.trainY, \n",
    "                epochs=200 ,\n",
    "                batch_size=64, \n",
    "                validation_split=0.1, \n",
    "                verbose=1,\n",
    "                callbacks=[\n",
    "                                EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                patience=10,\n",
    "                                restore_best_weights=True\n",
    "                            )\n",
    "                    ])\n",
    "\n",
    "\n",
    "        model.summary()\n",
    "        # save model in computer '/Users/learn/Desktop/Projects/machine-translation/machine-learning/data'\n",
    "        # model.save(filepath='../temp_model/')\n",
    "        self.save_models_and_parameters(total_sentences=TRAINING_SIZE, model= model,src_tokenizer=self.src_tokenizer, tar_tokenizer=self.tar_tokenizer, src_length=self.src_length, tar_length=self.tar_length, src_vocab_size=self.src_vocab_size,tar_vocab_size=self.tar_vocab_size )\n",
    "        pd.DataFrame(history.history).plot()\n",
    "        plt.title(\"Loss\")\n",
    "        plt.show()\n",
    "        pass\n",
    "\n",
    "    def save_models_and_parameters( self, total_sentences ,model ,src_tokenizer , tar_tokenizer, src_length, tar_length, src_vocab_size,tar_vocab_size ):\n",
    "        ''' \n",
    "        SAVE MODEL AND OTHER PARAMETERS IN DESIGNATED FOLDER\n",
    "        '''\n",
    "        model_name =str(total_sentences)\n",
    "        # path=\"../../model/lstm/\"+model_name+\"/\"\n",
    "        path='../temp_model/'+model_name+\"/\"\n",
    "        \n",
    "        src_parameters={\n",
    "            'src_length': src_length,\n",
    "            'src_vocab_size': src_vocab_size,\n",
    "        }\n",
    "        src_tokenizer= src_tokenizer\n",
    "\n",
    "        target_parameters={\n",
    "            'target_length': tar_length,\n",
    "            'target_vocab_size': tar_vocab_size,\n",
    "        }\n",
    "        target_tokenizer= tar_tokenizer\n",
    "\n",
    "        model.save(path+'lstm_model' ) \n",
    "        with open(path+'src_parameters.pickle', 'wb') as handle:\n",
    "            pickle.dump(src_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path+'src_tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(src_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path+'target_parameters.pickle', 'wb') as handle:\n",
    "            pickle.dump(target_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path+'target_tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(target_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pass\n",
    "\n",
    "    def load_model(self,model_path ):\n",
    "        model, src_tokenizer, target_tokenizer, src_parameters, target_parameters= self._load_models_and_parameters(model_path)\n",
    "        print(model, src_tokenizer, target_tokenizer, src_parameters, target_parameters)\n",
    "       \n",
    "        self.model=model\n",
    "        \n",
    "        self.src_tokenizer=src_tokenizer\n",
    "        \n",
    "        self.tar_tokenizer=target_tokenizer\n",
    "\n",
    "        self.src_length=src_parameters[\"src_length\"]\n",
    "        self.src_vocab_size=src_parameters[\"src_vocab_size\"]\n",
    "\n",
    "        self.tar_length=target_parameters[\"target_length\"]\n",
    "        self.tar_vocab_size=target_parameters[\"target_vocab_size\"]\n",
    "        pass\n",
    "\n",
    "    def _load_models_and_parameters(self, model_size):\n",
    "        path=model_size+'/'\n",
    "\n",
    "        model = keras.models.load_model(path+'lstm_model')\n",
    "        with open(path+ \"src_parameters.pickle\", 'rb') as handle:\n",
    "            src_parameters = pickle.load(handle)\n",
    "\n",
    "        with open(path+ \"src_tokenizer.pickle\", 'rb') as handle:\n",
    "            src_tokenizer = pickle.load(handle)\n",
    "\n",
    "        with open(path+ \"target_parameters.pickle\", 'rb') as handle:\n",
    "            target_parameters = pickle.load(handle)\n",
    "\n",
    "        with open(path+ \"target_tokenizer.pickle\", 'rb') as handle:\n",
    "            target_tokenizer = pickle.load(handle)\n",
    "        return model, src_tokenizer, target_tokenizer, src_parameters, target_parameters\n",
    "    \n",
    "    def word_for_id(self,integer, tokenizer):\n",
    "        # map an integer to a word\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == integer:\n",
    "                return word\n",
    "        return None\n",
    " \n",
    "    def predict_seq(self, model, tokenizer, source):\n",
    "        # generate target from a source sequence\n",
    "        prediction = model.predict(source, verbose=0)[0]\n",
    "        integers = [np.argmax(vector) for vector in prediction]\n",
    "        target = list()\n",
    "        for i in integers:\n",
    "            word = self.word_for_id(i, tokenizer)\n",
    "            if word is None:\n",
    "                break\n",
    "            target.append(word)\n",
    "        return ' '.join(target)\n",
    "    \n",
    "    def compare_prediction(self,model, tar_tokenizer, sources, raw_dataset, limit=20):\n",
    "        # evaluate a model\n",
    "        actual, predicted  = [], []\n",
    "        src = f'{self.source_str.upper()} (SOURCE)'\n",
    "        tgt = f'{self.target_str.upper()} (TARGET)'\n",
    "        pred = f'AUTOMATIC TRANSLATION IN {self.target_str.upper()}'\n",
    "        print(f'{src:30} {tgt:25} {pred}\\n')\n",
    "        \n",
    "        for i, source in enumerate(sources): # translate encoded source text\n",
    "            source = source.reshape((1, source.shape[0]))\n",
    "            translation = self.predict_seq(model, tar_tokenizer, source)\n",
    "            raw_src,raw_target = raw_dataset[i]\n",
    "            print(\"##############################################################################\")\n",
    "            print(f' {i+1}. {raw_src:30} || {raw_target:25} || {translation}')\n",
    "            ## STORE PREDICTIONS\n",
    "            ############################################\n",
    "            actual.append(raw_target.split())\n",
    "            predicted.append(translation.split())\n",
    "            ############################################\n",
    "            if i >= limit: # Display some of the result\n",
    "                break\n",
    "        return actual, predicted\n",
    "    \n",
    "    def genereate_test_results(self):\n",
    "        data_size=1000\n",
    "\n",
    "        pool_oftexts, pairs =createDataset(data_size=data_size, type=\"train\")\n",
    "        dataset= pool_oftexts\n",
    "        dataset = dataset.values\n",
    "        test=dataset\n",
    "        testX = self.encode_sequences(self.src_tokenizer, self.src_length, test[:, self.idx_src])\n",
    "        testY = self.encode_sequences(self.tar_tokenizer, self.tar_length, test[:, self.idx_tar])\n",
    "        testY = self.encode_output(testY, self.tar_vocab_size)\n",
    "        actual, predicted= self.compare_prediction(self.model, self.tar_tokenizer, testX, test)\n",
    "        return actual, predicted\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRAIN MODEL\n",
    "# translator= Translator(training_size=TRAINING_SIZE)\n",
    "# translator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:15:08.747576: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-08-16 22:15:08.747620: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-08-16 22:15:08.747630: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-08-16 22:15:08.749399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-16 22:15:08.752077: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-08-16 22:15:10.191789: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:10.204668: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:10.503082: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:11.904100: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:11.988924: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:12.014240: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:12.432548: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:12.453647: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:12.740255: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-08-16 22:15:12.751857: W tensorflow/core/common_runtime/graph_constructor.cc:834] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.engine.sequential.Sequential object at 0x107ce3fa0> <keras.src.preprocessing.text.Tokenizer object at 0x28162a7c0> <keras.src.preprocessing.text.Tokenizer object at 0x28162a7f0> {'src_length': 15, 'src_vocab_size': 2494} {'target_length': 15, 'target_vocab_size': 1996}\n",
      "English 1996\n",
      "HINDI (SOURCE)                 ENGLISH (TARGET)          AUTOMATIC TRANSLATION IN ENGLISH\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:15:24.509555: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-08-16 22:15:24.796856: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-08-16 22:15:25.003297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      " 1. अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें || give your application an accessibility workout || give your application an accessibility workout\n",
      "##############################################################################\n",
      " 2. एक्सेर्साइसर पहुंचनीयता अन्वेषक || accerciser accessibility explorer || accerciser accessibility explorer\n",
      "##############################################################################\n",
      " 3. निचले पटल के लिए डिफोल्ट प्लगइन खाका || the default plugin layout for the bottom panel || the layout for layout bottom the bottom panel\n",
      "##############################################################################\n",
      " 4. ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका || the default plugin layout for the top panel || the default plugin layout for the top panel\n",
      "##############################################################################\n",
      " 5. उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है || a list of plugins that are disabled by default || a list of plugins that are disabled by default\n",
      "##############################################################################\n",
      " 6. अवधि को हाइलाइट रकें           || highlight duration        || highlight duration\n",
      "##############################################################################\n",
      " 7. पहुंचनीय आसंधि नोड को चुनते समय हाइलाइट बक्से की अवधि || the duration of the highlight box when selecting accessible nodes || the duration of the highlight box when selecting accessible nodes\n",
      "##############################################################################\n",
      " 8. सीमांत बोर्डर के रंग को हाइलाइट करें || highlight border color    || highlight border color\n",
      "##############################################################################\n",
      " 9. हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता।  || the color and opacity of the highlight border || the color and opacity of the highlight border\n",
      "##############################################################################\n",
      " 10. भराई के रंग को हाइलाइट करें    || highlight fill color      || highlight fill color\n",
      "##############################################################################\n",
      " 11. हाइलाइट किया गया भराई का रंग और पारदर्शिता।  || the color and opacity of the highlight fill || the color and opacity of the highlight fill\n",
      "##############################################################################\n",
      " 12. एपीआई विचरक                    || api browser               || api browser\n",
      "##############################################################################\n",
      " 13. इस समय जिसे प्राप्त किया गया हो उसकी विभिन्न विधियों मेथड में विचरण करें || browse the various methods of the current accessible || browse the various methods of the current accessible\n",
      "##############################################################################\n",
      " 14. निजी गुणों को छिपाएं           || hide private attributes   || hide private attributes\n",
      "##############################################################################\n",
      " 15. विधि                           || method                    || method\n",
      "##############################################################################\n",
      " 16. गुणधर्म                        || property                  || property\n",
      "##############################################################################\n",
      " 17. मान                            || value                     || value\n",
      "##############################################################################\n",
      " 18. आईपाइथन कन्सोल                 || ipython console           || ipython console\n",
      "##############################################################################\n",
      " 19. इस समय चुने गए एक्सेसेबेल से काम लेने के लिए अंतर्क्रियात्मक कन्सोल || interactive console for manipulating currently selected accessible || interactive console for manipulating currently selected accessible\n",
      "##############################################################################\n",
      " 20. घटना मानिटर                    || event monitor             || event monitor\n",
      "##############################################################################\n",
      " 21. घटनाओं को मानिटर करें  m       ||  monitor events           || monitor events\n",
      "[['give', 'your', 'application', 'an', 'accessibility', 'workout'], ['accerciser', 'accessibility', 'explorer'], ['the', 'default', 'plugin', 'layout', 'for', 'the', 'bottom', 'panel'], ['the', 'default', 'plugin', 'layout', 'for', 'the', 'top', 'panel'], ['a', 'list', 'of', 'plugins', 'that', 'are', 'disabled', 'by', 'default'], ['highlight', 'duration'], ['the', 'duration', 'of', 'the', 'highlight', 'box', 'when', 'selecting', 'accessible', 'nodes'], ['highlight', 'border', 'color'], ['the', 'color', 'and', 'opacity', 'of', 'the', 'highlight', 'border'], ['highlight', 'fill', 'color'], ['the', 'color', 'and', 'opacity', 'of', 'the', 'highlight', 'fill'], ['api', 'browser'], ['browse', 'the', 'various', 'methods', 'of', 'the', 'current', 'accessible'], ['hide', 'private', 'attributes'], ['method'], ['property'], ['value'], ['ipython', 'console'], ['interactive', 'console', 'for', 'manipulating', 'currently', 'selected', 'accessible'], ['event', 'monitor'], ['monitor', 'events']] [['give', 'your', 'application', 'an', 'accessibility', 'workout'], ['accerciser', 'accessibility', 'explorer'], ['the', 'layout', 'for', 'layout', 'bottom', 'the', 'bottom', 'panel'], ['the', 'default', 'plugin', 'layout', 'for', 'the', 'top', 'panel'], ['a', 'list', 'of', 'plugins', 'that', 'are', 'disabled', 'by', 'default'], ['highlight', 'duration'], ['the', 'duration', 'of', 'the', 'highlight', 'box', 'when', 'selecting', 'accessible', 'nodes'], ['highlight', 'border', 'color'], ['the', 'color', 'and', 'opacity', 'of', 'the', 'highlight', 'border'], ['highlight', 'fill', 'color'], ['the', 'color', 'and', 'opacity', 'of', 'the', 'highlight', 'fill'], ['api', 'browser'], ['browse', 'the', 'various', 'methods', 'of', 'the', 'current', 'accessible'], ['hide', 'private', 'attributes'], ['method'], ['property'], ['value'], ['ipython', 'console'], ['interactive', 'console', 'for', 'manipulating', 'currently', 'selected', 'accessible'], ['event', 'monitor'], ['monitor', 'events']]\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL AND PARAMETERS FROM DISK\n",
    "translator= Translator(training_size=TRAINING_SIZE)\n",
    "model_name='15000'\n",
    "model_path='../temp_model/'+model_name+\"/\"\n",
    "translator.load_model(model_path=model_path)\n",
    "print(translator.target_str,translator.tar_vocab_size)\n",
    "actual, predicted =translator.genereate_test_results()\n",
    "print(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" \".join(actual[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "def get_cosine_val(word_list_1:[],word_list_2:[]):\n",
    "    def get_cosine(vec1, vec2):\n",
    "    \n",
    "        # get intersecting keys\n",
    "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "        # multiply and sum weights\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "        # compute denominator\n",
    "        sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "        sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "        # return cosine score\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "    def text_to_vector(text):\n",
    "        words = WORD.findall(text)\n",
    "        return Counter(words)\n",
    "    \n",
    "    text1= \" \".join(word_list_1)\n",
    "    text1= text1.lower()\n",
    "    text2= \" \".join(word_list_2)\n",
    "    text2=text2.lower()\n",
    "    # turn text into vector counts\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "    # compute similarity\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    return cosine\n",
    "\n",
    "class PerformanceAnalyser():\n",
    "    def __init__(self,actual=[], predicted=[]) -> None:\n",
    "        self.actual=actual\n",
    "        self.predicted=predicted\n",
    "        pass\n",
    "\n",
    "    def bleu_score(self):\n",
    "        # Get the bleu score of a model\n",
    "        bleu_dic = {}\n",
    "        format_actual= [ [sent] for sent in self.actual ]\n",
    "        bleu_dic['bleu-1-grams'] = corpus_bleu(format_actual, self.predicted, weights=(1.0, 0, 0, 0))\n",
    "        bleu_dic['bleu-1-2-grams'] = corpus_bleu(format_actual, self.predicted, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_dic['bleu-1-3-grams'] = corpus_bleu(format_actual, self.predicted, weights=(0.3, 0.3, 0.3, 0))\n",
    "        bleu_dic['bleu-1-4-grams'] = corpus_bleu(format_actual, self.predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        return bleu_dic\n",
    "    \n",
    "    def cosine_score(self):\n",
    "        cosine_value_list=[]\n",
    "        actual= self.actual\n",
    "        predicted=self.predicted\n",
    "        for a, p in zip(actual, predicted):\n",
    "            score= get_cosine_val(a,p)\n",
    "            cosine_value_list.append(score)\n",
    "            \n",
    "        average_cosine= mean(cosine_value_list)\n",
    "        return average_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf= PerformanceAnalyser(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu-1-grams': 0.979381443298969,\n",
       " 'bleu-1-2-grams': 0.9447755337023734,\n",
       " 'bleu-1-3-grams': 0.9049337027020472,\n",
       " 'bleu-1-4-grams': 0.8421580153696709}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.bleu_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926263930823103"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.cosine_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-translation",
   "language": "python",
   "name": "machine-translation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
