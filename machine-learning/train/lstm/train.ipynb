{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSTALL THESE LIBRARES\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE=2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "from string import punctuation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def printmd(string):\n",
    "    # Print with Markdowns    \n",
    "    display(Markdown(string))\n",
    "\n",
    "    \n",
    "def handle_helper_functions():\n",
    "    print(sys.path)\n",
    "    directory_path = os.path.abspath(os.path.join('../../helper'))\n",
    "    if directory_path not in sys.path:\n",
    "        sys.path.append(directory_path)    \n",
    "\n",
    "    translation_path=os.path.abspath(os.path.join('../../utils')) \n",
    "    if translation_path not in sys.path:\n",
    "        sys.path.append(translation_path)   \n",
    "    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/machine-learning/test/helper')                                       \n",
    "    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/machine-learning/utils')\n",
    "    sys.path.append('/Users/learn/Desktop/Projects/machine-translation/machine-learning/data')\n",
    "\n",
    "\n",
    "    lstm_helper_path = os.path.abspath(os.path.join('./create-model/lstm/'))\n",
    "    if lstm_helper_path not in sys.path:\n",
    "        sys.path.append(lstm_helper_path)    \n",
    "\n",
    "    print(sys.path)\n",
    "    \n",
    "\n",
    "handle_helper_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iit_dataset import createDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator():\n",
    "    def __init__(self, training_size=10000) -> None:\n",
    "        self.model=None\n",
    "        self.training_size=training_size\n",
    "        self.idx_src=0\n",
    "        self.idx_tar = 1\n",
    "        self.source_str, self.target_str = \"Hindi\", \"English\"\n",
    "        self.tar_tokenizer=None\n",
    "        self.tar_vocab_size=None\n",
    "        self.src_tokenizer=None\n",
    "        self.tar_length=None\n",
    "        self.src_vocab_size=None\n",
    "        self.src_length=None\n",
    "        self.trainX=None\n",
    "        self.trainY=None\n",
    "        self._convert_sentence_to_vectors()\n",
    "        pass\n",
    "\n",
    "    def train_model(self):\n",
    "        ''' \n",
    "        GET TRAINING DATA FROM IIT DATASET\n",
    "        TURN THEM INTO VECTORS\n",
    "        CREATE ENCODER DICTIONARY \n",
    "        SAVE THEM INSIDE THE CLASS\n",
    "        '''\n",
    "        pass\n",
    "    def _get_training_data(self):\n",
    "        pool_oftexts, pairs =createDataset(data_size=self.training_size, type=\"train\")\n",
    "        dataset= pool_oftexts\n",
    "        return dataset\n",
    "    \n",
    "    def clean(self,string):\n",
    "        string = string.replace(\"\\u202f\",\" \")\n",
    "        string = string.lower()\n",
    "        for p in punctuation + \"«»\" + \"0123456789\":\n",
    "            string = string.replace(p,\" \")  \n",
    "        string = re.sub('\\s+',' ', string)\n",
    "        string = string.strip()\n",
    "            \n",
    "        return string\n",
    "    \n",
    "    def _generate_train_test_split(self):\n",
    "        ''' \n",
    "        GET TRAINING DATA AND SPLIT THEM TO TRAIN AND TEST SET\n",
    "\n",
    "        RETURN dataset, train, test\n",
    "        '''\n",
    "        dataset= self._get_training_data()\n",
    "        total_sentences= len(dataset)\n",
    "        test_proportion = 0.1\n",
    "        train_test_threshold = int( (1-test_proportion) * total_sentences)\n",
    "\n",
    "        dataset[\"eng\"] = dataset[\"eng\"].apply(lambda x: self.clean(x))\n",
    "        dataset[\"hindi\"] = dataset[\"hindi\"].apply(lambda x: self.clean(x))\n",
    "\n",
    "        dataset = dataset.values\n",
    "        dataset = dataset[:total_sentences]\n",
    "\n",
    "        train, test = dataset[:train_test_threshold], dataset[train_test_threshold:]\n",
    "\n",
    "        return dataset, train, test\n",
    "    def create_tokenizer(self,lines):\n",
    "        # fit a tokenizer\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        return tokenizer\n",
    " \n",
    "\n",
    "    def encode_sequences(self,tokenizer, length, lines):\n",
    "        # encode and pad sequences\n",
    "        X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n",
    "        X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n",
    "        return X\n",
    "    \n",
    "    def encode_output(self,sequences, vocab_size):\n",
    "        # one hot encode target sequence\n",
    "        ylist = list()\n",
    "        for sequence in sequences:\n",
    "            encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "            ylist.append(encoded)\n",
    "        y = np.array(ylist)\n",
    "        y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "        return y\n",
    " \n",
    "\n",
    "    def _convert_sentence_to_vectors(self):\n",
    "        ''' \n",
    "        GENERATE TRAIN TEST SPLIT\n",
    "        CREATE TARGET TOKENIZER\n",
    "        CREATE SOURCE TOKENIZER\n",
    "        RETURN VECTORS FOR TRAINING\n",
    "        '''\n",
    "        dataset, train, test= self. _generate_train_test_split()\n",
    "        # Prepare target tokenizer\n",
    "        tar_tokenizer = self.create_tokenizer(dataset[:, self.idx_tar]) #save\n",
    "        tar_vocab_size = len(tar_tokenizer.word_index) + 1  #save\n",
    "        tar_length = 15  #save\n",
    "        printmd(f'\\nTarget ({self.target_str}) Vocabulary Size: {tar_vocab_size}')\n",
    "        printmd(f'Target ({self.target_str}) Max Length: {tar_length}')\n",
    "\n",
    "        # Prepare source tokenizer\n",
    "        src_tokenizer = self.create_tokenizer(dataset[:, self.idx_src])  #save\n",
    "        src_vocab_size = len(src_tokenizer.word_index) + 1  #save\n",
    "        src_length = 15  #save\n",
    "        #### SAVE\n",
    "        self.tar_tokenizer=tar_tokenizer\n",
    "        self.tar_vocab_size=tar_vocab_size\n",
    "        self.src_tokenizer=src_tokenizer\n",
    "        self.tar_length=tar_length\n",
    "        self.src_vocab_size=src_vocab_size\n",
    "        self.src_length=src_length\n",
    "        ### SAVE\n",
    "        printmd(f'\\nSource ({self.source_str}) Vocabulary Size: {src_vocab_size}')\n",
    "        printmd(f'Source ({self.source_str}) Max Length: {src_length}\\n')\n",
    "        # PREPARING TRAINING DATA\n",
    "        trainX = self.encode_sequences(src_tokenizer, src_length, train[:, self.idx_src])\n",
    "        trainY = self.encode_sequences(tar_tokenizer, tar_length, train[:, self.idx_tar])\n",
    "        trainY = self.encode_output(trainY, tar_vocab_size)\n",
    "        self.trainX=trainX\n",
    "        self.trainY=trainY\n",
    "        pass\n",
    "    \n",
    "    def create_model(self, src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "        # Create the model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "        model.add(LSTM(n_units))\n",
    "        model.add(RepeatVector(tar_timesteps))\n",
    "        model.add(LSTM(n_units, return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        # Create model\n",
    "        model=self.model\n",
    "        model = self.create_model(self.src_vocab_size, self.tar_vocab_size, self.src_length, self.tar_length, 256)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        #######################\n",
    "        # display(\"TRAIN X: \",self.trainX)\n",
    "        # display(\"TRAIN Y \",self.trainY)\n",
    "        # display(model)\n",
    "        #######################\n",
    "        history = model.fit(self.trainX, \n",
    "               self.trainY, \n",
    "                epochs=1,#200 \n",
    "                batch_size=64, \n",
    "                validation_split=0.1, \n",
    "                verbose=1,\n",
    "                callbacks=[\n",
    "                                EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                patience=10,\n",
    "                                restore_best_weights=True\n",
    "                            )\n",
    "                    ])\n",
    "\n",
    "\n",
    "        model.summary()\n",
    "        # save model in computer '/Users/learn/Desktop/Projects/machine-translation/machine-learning/data'\n",
    "        # model.save(filepath='../temp_model/')\n",
    "        self.save_models_and_parameters(total_sentences=TRAINING_SIZE, model= model,src_tokenizer=self.src_tokenizer, tar_tokenizer=self.tar_tokenizer, src_length=self.src_length, tar_length=self.tar_length, src_vocab_size=self.src_vocab_size,tar_vocab_size=self.tar_vocab_size )\n",
    "        pd.DataFrame(history.history).plot()\n",
    "        plt.title(\"Loss\")\n",
    "        plt.show()\n",
    "        pass\n",
    "\n",
    "    def save_models_and_parameters( self, total_sentences ,model ,src_tokenizer , tar_tokenizer, src_length, tar_length, src_vocab_size,tar_vocab_size ):\n",
    "        ''' \n",
    "        SAVE MODEL AND OTHER PARAMETERS IN DESIGNATED FOLDER\n",
    "        '''\n",
    "        model_name =str(total_sentences)\n",
    "        # path=\"../../model/lstm/\"+model_name+\"/\"\n",
    "        path='../temp_model/'+model_name+\"/\"\n",
    "        \n",
    "        src_parameters={\n",
    "            'src_length': src_length,\n",
    "            'src_vocab_size': src_vocab_size,\n",
    "        }\n",
    "        src_tokenizer= src_tokenizer\n",
    "\n",
    "        target_parameters={\n",
    "            'target_length': tar_length,\n",
    "            'target_vocab_size': tar_vocab_size,\n",
    "        }\n",
    "        target_tokenizer= tar_tokenizer\n",
    "\n",
    "        model.save(path+'lstm_model' ) \n",
    "        with open(path+'src_parameters.pickle', 'wb') as handle:\n",
    "            pickle.dump(src_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path+'src_tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(src_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path+'target_parameters.pickle', 'wb') as handle:\n",
    "            pickle.dump(target_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(path+'target_tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(target_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator= Translator(training_size=TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! jupyter kernelspec uninstall silicon-kernel  -f \n",
    "! jupyter kernelspec list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-translation",
   "language": "python",
   "name": "machine-translation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
